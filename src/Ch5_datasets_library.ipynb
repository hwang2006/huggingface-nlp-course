{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f1e0ac-195a-4f78-ad72-68ff3092869c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "#!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93d3341-d8d5-4ea5-bae2-b1ca646aa3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!gzip -dv SQuAD_it*.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00cd23f-5e9c-4415-bfd0-9234bb6585d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71c4046-ee9d-4cc3-9c00-29354c6344bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f16026-294e-48d1-9046-4447ba4a7541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#squad_it_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7076d912-2cad-494e-aae4-9bcd9ec38a64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff7c6564-62cc-4722-a530-1663396c94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e229bc4f-ae75-4200-a4a3-c9b322789cab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'paragraphs'],\n",
      "        num_rows: 442\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'paragraphs'],\n",
      "        num_rows: 48\n",
      "    })\n",
      "})\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "24748496\n",
      "24748496\n"
     ]
    }
   ],
   "source": [
    "print(squad_it_dataset)\n",
    "print(type(squad_it_dataset))\n",
    "print(type(squad_it_dataset[\"train\"]))\n",
    "print(type(squad_it_dataset[\"test\"]))\n",
    "print(squad_it_dataset[\"train\"].dataset_size)\n",
    "print(squad_it_dataset[\"test\"].dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ed802b-91de-467e-a72e-28e177ba49dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access squad_it_dataset[train]: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -al squad_it_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed033f-10e7-4694-9f66-bfa5a69712e0",
   "metadata": {},
   "source": [
    "이번 섹션에서 예제로 UC Irvine Machine Learning Repository에서 호스팅되는 Drug Review Dataset을 사용합니다. 여기에는 다양한 약물에 대한 환자 리뷰, 치료 상태 및 환자 만족도에 대한 별 10개 등급(10-star rating)이 포함되어 있습니다.\n",
    "\n",
    "먼저 wget 및 unzip 명령으로 데이터를 다운로드하고 압축을 풀어봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15f32e80-474b-4c5f-870e-f8ec8a6dcf52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "#!unzip drugsCom_raw.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504bc802-3a75-4267-bce4-26ae886768c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
    "# \\t 는 Python에서 탭 문자를 나타냅니다.\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c4c62fe-9864-42b1-8ba9-9716273a1d91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
      "        num_rows: 161297\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
      "        num_rows: 53766\n",
      "    })\n",
      "})\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "116144739\n",
      "116144739\n"
     ]
    }
   ],
   "source": [
    "print(drug_dataset)\n",
    "print(type(drug_dataset))\n",
    "print(type(drug_dataset[\"train\"]))\n",
    "print(drug_dataset[\"train\"].dataset_size)\n",
    "print(drug_dataset[\"test\"].dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d91bc2c9-7684-4186-9f38-265ed70073b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 qualis in0183 84289175 Jul 24 22:37 drugsComTrain_raw.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls -al drugsComTrain_raw.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c7ce65-77fe-47ed-8473-dc45080f82b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 qualis in0183 28071166 Jul 24 22:37 drugsComTest_raw.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls -al drugsComTest_raw.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed416add-e5ad-43d0-ae80-0a770670b7da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161297\n"
     ]
    }
   ],
   "source": [
    "print(len(drug_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9598e8c0-9de6-4b40-85e6-72be19d9d6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for split in drug_dataset.keys():\n",
    "    print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62f6e539-9a52-43c4-8a82-40c14a554d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161297\n",
      "161297\n"
     ]
    }
   ],
   "source": [
    "print(len(drug_dataset[\"train\"].unique(\"Unnamed: 0\")))\n",
    "print(len(drug_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d42ae063-aac6-40a9-8e58-8542605de9da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53766\n",
      "53766\n"
     ]
    }
   ],
   "source": [
    "print(len(drug_dataset[\"test\"].unique(\"Unnamed: 0\")))\n",
    "print(len(drug_dataset[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d8c8c1d-fb8d-4ef5-b6e6-7c0bc0295be9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for split in drug_dataset.keys():\n",
    "    print(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b264268d-de63-4032-adce-01be2cf9d907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for split in drug_dataset.keys():\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e3a48bf-c3c5-43c8-b663-8b05f1c54256",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# 앞쪽의 샘플 몇개를 가져옵니다.\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f23acf8c-3f30-41ec-b8c5-92d89a1236f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e00f85e8-3905-45e5-884c-8f4b6d105a27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drug_dataset = drug_dataset.rename_column(\n",
    "#    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
    "#)\n",
    "\n",
    "drug_dataset = drug_dataset.rename_column(\"Unnamed: 0\", \"patient_id\")\n",
    "drug_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37da2dbc-7c9b-4f9e-85bc-97f64983cb24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lowercase_condition(example):\n",
    "    return {\"condition\": example[\"condition\"].lower()}\n",
    "\n",
    "#drug_dataset.map(lowercase_condition)\n",
    "# AttributeError: 'NoneType' object has no attribute 'lower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3410da19-470d-4e87-9cc7-c4f927fc2b78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x: x * x)(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fdd1222-02a2-4049-8479-66789228d3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda base, height: 0.5 * base * height)(4, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2064927e-ae4c-40e8-a298-817beba32e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77d70fc5b6b49b496cc4f083a538b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/161297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf89b93428494a41977d2b8b9e81b8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/53766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4942f23d-b9e9-4623-bd4f-25f7c67972b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 160398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d2fdee1-67df-461e-ac88-50ca8c5ff128",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a98db5c54f493a9d16237e7dd22e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4ba732cd1749dfaf4d4b8647bceab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "# 소문자화가 제대로 진행되었는지 확인.\n",
    "#drug_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cd3665b-4d3e-4eda-a4a4-a22d1d9e58e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 160398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b4f9d05-125e-46b4-949c-e966d99a912c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['left ventricular dysfunction', 'adhd', 'birth control']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset[\"train\"][\"condition\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c17dcc1c-6f83-44c7-b2f4-04b447701101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 160398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53471\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2ba44ee-fea5-4190-b0f6-5c4157d6932f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"\n",
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(drug_dataset[\"train\"][\"review\"][0])\n",
    "print(len(drug_dataset[\"train\"][\"review\"][0].split())) # default delimiter is space\n",
    "print(len(drug_dataset[\"train\"][\"review\"][0].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e8ae525-348f-4014-bb2e-13e38246caab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27dd5514-1a3a-4421-9734-feac2f1a9a79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c6951b903d411fbe0e12d0f9cc6804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db0c2786b454c9a8e2756ba11b6dd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'patient_id': 206461,\n",
       " 'drugName': 'Valsartan',\n",
       " 'condition': 'left ventricular dysfunction',\n",
       " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       " 'rating': 9.0,\n",
       " 'date': 'May 20, 2012',\n",
       " 'usefulCount': 27,\n",
       " 'review_length': 17}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "# 첫 학습 예제를 살펴봅니다.\n",
    "drug_dataset[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9224951b-9699-436a-90fc-5f687e0a8d86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': [111469, 13653, 53602],\n",
       " 'drugName': ['Ledipasvir / sofosbuvir',\n",
       "  'Amphetamine / dextroamphetamine',\n",
       "  'Alesse'],\n",
       " 'condition': ['hepatitis c', 'adhd', 'birth control'],\n",
       " 'review': ['\"Headache\"', '\"Great\"', '\"Awesome\"'],\n",
       " 'rating': [10.0, 10.0, 10.0],\n",
       " 'date': ['February 3, 2015', 'October 20, 2009', 'November 23, 2015'],\n",
       " 'usefulCount': [41, 3, 0],\n",
       " 'review_length': [1, 1, 1]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset[\"train\"].sort(\"review_length\")[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3702f396-25e5-4c67-a0d1-31bd320d9aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3574b8145ae04d47a5b2db9bf0d855bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4c1a1cf204407c861a9a8c98fd2434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 138514, 'test': 46108}\n"
     ]
    }
   ],
   "source": [
    "print(type(drug_dataset))\n",
    "\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c32fe2e8-6180-49e6-85c7-131e6cef837f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 138514\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b9f7b5c-e023-4da6-b1d5-611dc7d36c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a transformer called BERT\n",
      "! \" # $ % & '\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "text = \"I&#039;m a transformer called BERT\" # html markdown &#039; == single quote\n",
    "print(html.unescape(text))\n",
    "\n",
    "test = \"&#33; &#34; &#35; &#36; &#37; &#38; &#39;\"\n",
    "print(html.unescape(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "654e7635-3a63-41f5-afdd-09f1c0d38ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d369a40f8b8a412f9763014745fb5059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3095e64ce0340a3aa5365521d99dac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c0fed5b-b512-4711-a5a3-7352911f4e53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 138514\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c4285-37da-46b5-97d0-8025ceb9d31a",
   "metadata": {},
   "source": [
    "Dataset.map() 메서드의 batched 매개변수가 True로 설정되면, 호출되는 순간마다 여러 개의 예제로 구성된 하나의 배치(batch)가 한번에 map 함수에 입력됩니다. 배치 크기(batch size)는 별도로 설정이 가능하고 디폴트값은 1000입니다. 예를 들어, 모든 HTML 특수문자들을 이스케이프 해제(unescape)하기 위해서 위에서 실행한 map 함수는 실행 속도가 약간 느립니다(진행 표시줄에서 소요된 시간을 읽을 수 있습니다). 리스트 컴프리헨션(list comprehension, 내포)를 사용하여 동시에 여러 예제를 한번에 처리하여 속도를 높일 수 있습니다.\n",
    "\n",
    "batched=True가 지정되면 Dataset.map() 메서드의 매개변수로 전달되는 함수가 데이터셋의 필드가 포함된 하나의 딕셔너리를 입력받지만 이 딕셔너리 내부의 각 필드값은 이제 단일 값이 아니라 리스트(list of values)입니다. Dataset.map()의 반환 값은 동일해야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a59e405-3523-4528-b0ba-4438ab1066be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0231866458664214bf6bac0dcfda94d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0701473ecae469bb1ddaec1823e2a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 169 ms, total: 10.5 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%time drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f990f7fd-97da-4114-aae2-4a9e09fc987e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9466d1f6901d4dec81cd9a4551caa548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a74c35b6f042a28e4857bacda8791b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 293 ms, sys: 114 ms, total: 406 ms\n",
      "Wall time: 869 ms\n"
     ]
    }
   ],
   "source": [
    "%time drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a34f0bf-d765-4239-96b6-0419743d5697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4a6b5580ec4e4a9b5fb4ef826ba2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130d1aa5cade47a697a3a6244d446b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 342 ms, sys: 132 ms, total: 474 ms\n",
      "Wall time: 2.81 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a17d4805fd4208aaa905b19ee3eead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7799d5a37c884eb387181bfef3d5500f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 268 ms, sys: 73.9 ms, total: 342 ms\n",
      "Wall time: 497 ms\n"
     ]
    }
   ],
   "source": [
    "%time new_drug_dataset = drug_dataset.map(lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True)\n",
    "%time drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ac92503-2105-497f-9806-9c745f3c506a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 138514\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba055b48-08d4-4455-baba-4b2e0f080dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca49ffae-eb7f-4f93-9fdf-7208aef8a3af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ac5fd7ad804be397e6234c845ce304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b7f16edad34670a61bf290bbe83bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.8 s, sys: 314 ms, total: 1min\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "104ff50e-6aa1-4328-b0df-70a44e2739e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe72eb17928b4c77a76135ac3e346bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd862edc2894fd9ad897c3f35dae864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 347 ms, total: 1min 19s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%time tokenized_dataset = drug_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c057a08c-d797-4152-a652-34453f597e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c927fbf1-edec-4e5a-9aa5-cccf2d4f89ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa30f55f783146cab00611904585f8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b90cc64d34a42568802aae10974a367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 18s, sys: 874 ms, total: 3min 19s\n",
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "def slow_tokenize_function(examples):\n",
    "    return slow_tokenizer(examples[\"review\"], truncation=True)\n",
    "%time tokenized_dataset_slow = drug_dataset.map(slow_tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "705be994-a780-49f5-95ca-3396e4d7ddee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddc0caf5045402caf596b3cad715445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1b3193e26c437ea5379c62353321b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 55s, sys: 398 ms, total: 2min 55s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "def slow_tokenize_function(examples):\n",
    "    return slow_tokenizer(examples[\"review\"], truncation=True)\n",
    "%time tokenized_dataset_slow = drug_dataset.map(slow_tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931d09a0-4148-40c1-b962-6f2a9ccddf42",
   "metadata": {},
   "source": [
    "위 결과는 batched=True 옵션과 함께 빠른(fast) 토크나이저를 사용하는 것이 일괄 처리가 없는 느린 토크나이저보다 30배 더 빠르다는 것을 의미합니다. 이것은 정말 놀라운 일입니다! 이것이 AutoTokenizer를 사용할 때 빠른 토크나이저가 디폴트로 지정된 이유입니다. 그리고 왜 \"빠른(fast)\"이라는 수식어가 붙었는지에 대한 이유기기도 하지요. 이러한 속도 향상을 가져올 수 있는 이유는 하부에서 토큰화 코드가 코드 실행을 쉽게 병렬화할 수 있게 하는 언어인 Rust에서 실행되기 때문입니다.\n",
    "\n",
    "또한 병렬화(parallelization)는 배치 처리(batching)를 기반으로 빠른 토크나이저가 6배 이상의 속도 향상을 가져오는 이유이기도 합니다. 단일 토큰화 작업을 병렬화할 수는 없습니다. 그러나 많은 텍스트를 동시에 토큰화하려는 경우 각각의 텍스트를 담당하는 여러 프로세스로 실행을 분할할 수 있습니다.\n",
    "\n",
    "Dataset.map()에는 자체 병렬화 기능도 있습니다. Rust가 지원되지 않기 때문에 느린 토크나이저는 빠른 토크나이저를 따라잡을 수 없지만 여전히 도움이 될 수 있습니다(특히 빠른 버전이 없는 토크나이저를 사용하는 경우에는 더 그렇지요). 다중 처리를 활성화하려면 num_proc 인수를 사용하고 Dataset.map() 호출에 사용할 프로세스 수를 지정합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7978e2b6-db99-4677-b4ef-d4c7a6358f65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ec1e15425c4a5d9ec0de711482b0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21d21c792b9416caa3460a617801fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 600 ms, sys: 148 ms, total: 749 ms\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
    "\n",
    "\n",
    "def slow_tokenize_function(examples):\n",
    "    return slow_tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "%time tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91589953-75c4-457c-b5b9-a77f74f5e070",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bbcd5c592749eba2620bec90b5fb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8791cc2c5e4d27aa0852929ef89710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 665 ms, sys: 244 ms, total: 909 ms\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
    "\n",
    "\n",
    "def slow_tokenize_function(examples):\n",
    "    return slow_tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "%time tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee401033-9571-442e-bfb9-dabcd56135bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cb097b3a4a4f0bb1db92f3bfc6ffc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4aac5c2ea64a1bbf790c6db942c098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 659 ms, sys: 115 ms, total: 774 ms\n",
      "Wall time: 47.3 s\n"
     ]
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
    "\n",
    "\n",
    "def slow_tokenize_function(examples):\n",
    "    return slow_tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "%time tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb2910eb-2ed0-4cb9-af33-3aa3cb445c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf36ef55-a9be-4f18-b7d3-d60969bb926f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(drug_dataset[\"train\"][0][\"review\"].split()))\n",
    "drug_dataset[\"train\"][0][\"review\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76550c92-2c09-4203-a012-d74ee1cf1e92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 107, 1422, 1488, 1110, 9079, 1194, 1117, 2223, 1989, 1104, 1130, 19972, 11083, 119, 1284, 1245, 4264, 1165, 1119, 1310, 1142, 1314, 1989, 117, 1165, 1119, 1408, 1781, 1103, 2439, 13753, 1119, 1209, 1129, 1113, 119, 1370, 1160, 1552, 117, 1119, 1180, 6374, 1243, 1149, 1104, 1908, 117, 1108, 1304, 172, 14687, 1183, 117, 1105, 7362, 1111, 2212, 129, 2005, 1113, 170, 2797, 1313, 1121, 1278, 12020, 113, 1304, 5283, 1111, 1140, 119, 114, 146, 1270, 1117, 3995, 1113, 6356, 2106, 1105, 1131, 1163, 1106, 6166, 1122, 1149, 170, 1374, 1552, 119, 3969, 1293, 1119, 1225, 1120, 1278, 117, 1105, 1114, 2033, 1146, 1107, 1103, 2106, 119, 1109, 1314, 1160, 1552, 1138, 1151, 2463, 1714, 119, 1124, 1110, 150, 21986, 3048, 1167, 5340, 1895, 1190, 1518, 102], [101, 119, 1124, 1110, 1750, 6438, 113, 170, 1363, 1645, 114, 117, 1750, 172, 14687, 1183, 119, 1124, 1110, 11566, 1155, 1103, 1614, 1119, 1431, 119, 8007, 1117, 4658, 1110, 1618, 119, 1284, 1138, 1793, 1242, 1472, 23897, 1105, 1177, 1677, 1142, 1110, 1103, 1211, 3903, 119, 107, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "42c3925e-4782-4de2-b891-2b10eb8aa754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 107, 1422, 1488, 1110, 9079, 1194, 1117, 2223, 1989, 1104, 1130, 19972, 11083, 119, 1284, 1245, 4264, 1165, 1119, 1310, 1142, 1314, 1989, 117, 1165, 1119, 1408, 1781, 1103, 2439, 13753, 1119, 1209, 1129, 1113, 119, 1370, 1160, 1552, 117, 1119, 1180, 6374, 1243, 1149, 1104, 1908, 117, 1108, 1304, 172, 14687, 1183, 117, 1105, 7362, 1111, 2212, 129, 2005, 1113, 170, 2797, 1313, 1121, 1278, 12020, 113, 1304, 5283, 1111, 1140, 119, 114, 146, 1270, 1117, 3995, 1113, 6356, 2106, 1105, 1131, 1163, 1106, 6166, 1122, 1149, 170, 1374, 1552, 119, 3969, 1293, 1119, 1225, 1120, 1278, 117, 1105, 1114, 2033, 1146, 1107, 1103, 2106, 119, 1109, 1314, 1160, 1552, 1138, 1151, 2463, 1714, 119, 1124, 1110, 150, 21986, 3048, 1167, 5340, 1895, 1190, 1518, 102], [101, 119, 1124, 1110, 1750, 6438, 113, 170, 1363, 1645, 114, 117, 1750, 172, 14687, 1183, 119, 1124, 1110, 11566, 1155, 1103, 1614, 1119, 1431, 119, 8007, 1117, 4658, 1110, 1618, 119, 1284, 1138, 1793, 1242, 1472, 23897, 1105, 1177, 1677, 1142, 1110, 1103, 1211, 3903, 119, 107, 102], [101, 107, 146, 1215, 1106, 1321, 1330, 9619, 14255, 4487, 17046, 117, 1134, 1125, 1626, 21822, 5120, 117, 1105, 1108, 1304, 2816, 118, 1304, 1609, 6461, 117, 12477, 1775, 126, 1552, 117, 1185, 1168, 1334, 3154, 119, 1252, 1122, 4049, 21055, 176, 2556, 13040, 1673, 117, 1134, 1110, 1136, 1907, 1107, 1646, 117, 1177, 146, 6759, 1106, 149, 1183, 9730, 1233, 117, 1272, 1103, 13288, 1132, 1861, 119, 1332, 1139, 1168, 17029, 2207, 117, 146, 1408, 149, 1183, 9730, 1233, 2411, 117, 1113, 1139, 1148, 1285, 1104, 1669, 117, 1112, 1103, 7953, 1163, 119, 1262, 1103, 1669, 5695, 1111, 1160, 2277, 119, 1332, 1781, 1103, 1248, 5246, 118, 1269, 1160, 2277, 119, 1262, 1208, 117, 1114, 1503, 5246, 1614, 1400, 1256, 4146, 118, 1139, 1503, 1669, 5695, 102], [101, 1111, 1160, 2277, 1105, 1208, 1122, 112, 188, 1103, 1322, 1104, 1103, 1503, 1989, 118, 146, 1253, 1138, 3828, 3058, 12398, 119, 1109, 3112, 1334, 1110, 1115, 146, 1238, 112, 189, 1138, 1251, 1168, 1334, 3154, 119, 1109, 1911, 1104, 1217, 1669, 1714, 1108, 1177, 27386, 119, 119, 119, 2586, 2225, 119, 107, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 1, 1]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tokenize_and_split(drug_dataset[\"train\"][:2])\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ffacc5fa-5a65-48b7-bbce-4a088c0c52d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 49, 128, 55, 116, 128, 55, 108]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tokenize_and_split(drug_dataset[\"train\"][:5])\n",
    "[len(x) for x in result['input_ids']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6cc43785-fe63-4f27-9f0e-9a6a98bf0cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 138514\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 46108\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['patient_id',\n",
       " 'drugName',\n",
       " 'condition',\n",
       " 'review',\n",
       " 'rating',\n",
       " 'date',\n",
       " 'usefulCount',\n",
       " 'review_length']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(drug_dataset)\n",
    "drug_dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64369ce5-029f-4c5e-862e-1da4652c5824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b1674880514c528761d74d7df79a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 8 named input_ids expected length 1000 but got length 1463",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdrug_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_and_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/dataset_dict.py:868\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 868\u001b[0m     {\n\u001b[1;32m    869\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    870\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    871\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    872\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    873\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    874\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    875\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    876\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    877\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    878\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    879\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    880\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    881\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    882\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    883\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    884\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    885\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    886\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/dataset_dict.py:869\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    868\u001b[0m     {\n\u001b[0;32m--> 869\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3107\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:3501\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3499\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(batch))\n\u001b[1;32m   3500\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3501\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3502\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/arrow_writer.py:570\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    568\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[1;32m    569\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n\u001b[0;32m--> 570\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/pyarrow/table.pxi:3991\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/pyarrow/table.pxi:3271\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 8 named input_ids expected length 1000 but got length 1463"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa94c346-9caa-4384-ad6e-1f1cf9c5be44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f787e1c5114b99a058ca873bdb4fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8c23c3defc45d3b044c6bd393ecc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = drug_dataset.map(\n",
    "    tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "396e0c68-6b0f-4c43-955c-a76324eeba67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 206772\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 68876\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a549077-86df-4069-9f27-5524a00dccc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 49, 128]\n",
      "[[101, 107, 1422, 1488, 1110, 9079, 1194, 1117, 2223, 1989, 1104, 1130, 19972, 11083, 119, 1284, 1245, 4264, 1165, 1119, 1310, 1142, 1314, 1989, 117, 1165, 1119, 1408, 1781, 1103, 2439, 13753, 1119, 1209, 1129, 1113, 119, 1370, 1160, 1552, 117, 1119, 1180, 6374, 1243, 1149, 1104, 1908, 117, 1108, 1304, 172, 14687, 1183, 117, 1105, 7362, 1111, 2212, 129, 2005, 1113, 170, 2797, 1313, 1121, 1278, 12020, 113, 1304, 5283, 1111, 1140, 119, 114, 146, 1270, 1117, 3995, 1113, 6356, 2106, 1105, 1131, 1163, 1106, 6166, 1122, 1149, 170, 1374, 1552, 119, 3969, 1293, 1119, 1225, 1120, 1278, 117, 1105, 1114, 2033, 1146, 1107, 1103, 2106, 119, 1109, 1314, 1160, 1552, 1138, 1151, 2463, 1714, 119, 1124, 1110, 150, 21986, 3048, 1167, 5340, 1895, 1190, 1518, 102], [101, 119, 1124, 1110, 1750, 6438, 113, 170, 1363, 1645, 114, 117, 1750, 172, 14687, 1183, 119, 1124, 1110, 11566, 1155, 1103, 1614, 1119, 1431, 119, 8007, 1117, 4658, 1110, 1618, 119, 1284, 1138, 1793, 1242, 1472, 23897, 1105, 1177, 1677, 1142, 1110, 1103, 1211, 3903, 119, 107, 102], [101, 107, 146, 1215, 1106, 1321, 1330, 9619, 14255, 4487, 17046, 117, 1134, 1125, 1626, 21822, 5120, 117, 1105, 1108, 1304, 2816, 118, 1304, 1609, 6461, 117, 12477, 1775, 126, 1552, 117, 1185, 1168, 1334, 3154, 119, 1252, 1122, 4049, 21055, 176, 2556, 13040, 1673, 117, 1134, 1110, 1136, 1907, 1107, 1646, 117, 1177, 146, 6759, 1106, 149, 1183, 9730, 1233, 117, 1272, 1103, 13288, 1132, 1861, 119, 1332, 1139, 1168, 17029, 2207, 117, 146, 1408, 149, 1183, 9730, 1233, 2411, 117, 1113, 1139, 1148, 1285, 1104, 1669, 117, 1112, 1103, 7953, 1163, 119, 1262, 1103, 1669, 5695, 1111, 1160, 2277, 119, 1332, 1781, 1103, 1248, 5246, 118, 1269, 1160, 2277, 119, 1262, 1208, 117, 1114, 1503, 5246, 1614, 1400, 1256, 4146, 118, 1139, 1503, 1669, 5695, 102]]\n"
     ]
    }
   ],
   "source": [
    "print([len(x) for x in tokenized_dataset[\"train\"][\"input_ids\"][:3]])\n",
    "print(tokenized_dataset[\"train\"][\"input_ids\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16739cda-a509-4d09-885d-16f86bd48a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 2, 3, 3, 4, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "print([x for x in tokenized_dataset['train']['overflow_to_sample_mapping'][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30d49320-26be-46d4-ad7a-3103e6f495ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_split(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    # 신규 인덱스와 이전 인덱스와의 매핑 추출\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    #print(sample_map)\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3432f4fc-dc44-4750-8058-80831aa5082b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6d1b7874ca47a39d3ca853815a1ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35278e014fcf4d3c8e0e337e8b33c7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 206772\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 68876\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
    "#tokenized_dataset = drug_dataset.map(tokenize_and_split)\n",
    "tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5981e40f-e69f-45ad-a204-6da7bf714677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       " '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       " '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it\\'s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn\\'t have any other side effects. The idea of being period free was so tempting... Alas.\"',\n",
       " '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it\\'s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn\\'t have any other side effects. The idea of being period free was so tempting... Alas.\"',\n",
       " '\"This is my first time using any form of birth control. I\\'m glad I went with the patch, I have been on it for 8 months. At first It decreased my libido but that subsided. The only downside is that it made my periods longer (5-6 days to be exact) I used to only have periods for 3-4 days max also made my cramps intense for the first two days of my period, I never had cramps before using birth control. Other than that in happy with the patch\"']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']['review'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2db3a88e-aafc-44bc-b589-303865d9d13c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[141, 141, 134, 134, 89]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']['review_length'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9c978e0-32e0-4267-bd9e-aa458dcfe879",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[95260, 95260, 92703, 92703, 138000]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']['patient_id'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39534056-e3fc-4213-9155-e5d4a378885c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adhd', 'adhd', 'birth control', 'birth control', 'birth control']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']['condition'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "887966f6-1505-429e-994f-258cdfd54410",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 49, 128, 55, 116]\n",
      "[[101, 107, 1422, 1488, 1110, 9079, 1194, 1117, 2223, 1989, 1104, 1130, 19972, 11083, 119, 1284, 1245, 4264, 1165, 1119, 1310, 1142, 1314, 1989, 117, 1165, 1119, 1408, 1781, 1103, 2439, 13753, 1119, 1209, 1129, 1113, 119, 1370, 1160, 1552, 117, 1119, 1180, 6374, 1243, 1149, 1104, 1908, 117, 1108, 1304, 172, 14687, 1183, 117, 1105, 7362, 1111, 2212, 129, 2005, 1113, 170, 2797, 1313, 1121, 1278, 12020, 113, 1304, 5283, 1111, 1140, 119, 114, 146, 1270, 1117, 3995, 1113, 6356, 2106, 1105, 1131, 1163, 1106, 6166, 1122, 1149, 170, 1374, 1552, 119, 3969, 1293, 1119, 1225, 1120, 1278, 117, 1105, 1114, 2033, 1146, 1107, 1103, 2106, 119, 1109, 1314, 1160, 1552, 1138, 1151, 2463, 1714, 119, 1124, 1110, 150, 21986, 3048, 1167, 5340, 1895, 1190, 1518, 102], [101, 119, 1124, 1110, 1750, 6438, 113, 170, 1363, 1645, 114, 117, 1750, 172, 14687, 1183, 119, 1124, 1110, 11566, 1155, 1103, 1614, 1119, 1431, 119, 8007, 1117, 4658, 1110, 1618, 119, 1284, 1138, 1793, 1242, 1472, 23897, 1105, 1177, 1677, 1142, 1110, 1103, 1211, 3903, 119, 107, 102], [101, 107, 146, 1215, 1106, 1321, 1330, 9619, 14255, 4487, 17046, 117, 1134, 1125, 1626, 21822, 5120, 117, 1105, 1108, 1304, 2816, 118, 1304, 1609, 6461, 117, 12477, 1775, 126, 1552, 117, 1185, 1168, 1334, 3154, 119, 1252, 1122, 4049, 21055, 176, 2556, 13040, 1673, 117, 1134, 1110, 1136, 1907, 1107, 1646, 117, 1177, 146, 6759, 1106, 149, 1183, 9730, 1233, 117, 1272, 1103, 13288, 1132, 1861, 119, 1332, 1139, 1168, 17029, 2207, 117, 146, 1408, 149, 1183, 9730, 1233, 2411, 117, 1113, 1139, 1148, 1285, 1104, 1669, 117, 1112, 1103, 7953, 1163, 119, 1262, 1103, 1669, 5695, 1111, 1160, 2277, 119, 1332, 1781, 1103, 1248, 5246, 118, 1269, 1160, 2277, 119, 1262, 1208, 117, 1114, 1503, 5246, 1614, 1400, 1256, 4146, 118, 1139, 1503, 1669, 5695, 102], [101, 1111, 1160, 2277, 1105, 1208, 1122, 112, 188, 1103, 1322, 1104, 1103, 1503, 1989, 118, 146, 1253, 1138, 3828, 3058, 12398, 119, 1109, 3112, 1334, 1110, 1115, 146, 1238, 112, 189, 1138, 1251, 1168, 1334, 3154, 119, 1109, 1911, 1104, 1217, 1669, 1714, 1108, 1177, 27386, 119, 119, 119, 2586, 2225, 119, 107, 102], [101, 107, 1188, 1110, 1139, 1148, 1159, 1606, 1251, 1532, 1104, 3485, 1654, 119, 146, 112, 182, 5171, 146, 1355, 1114, 1103, 10085, 117, 146, 1138, 1151, 1113, 1122, 1111, 129, 1808, 119, 1335, 1148, 1135, 10558, 1139, 181, 21883, 2572, 1133, 1115, 4841, 25984, 119, 1109, 1178, 1205, 5570, 1110, 1115, 1122, 1189, 1139, 6461, 2039, 113, 126, 118, 127, 1552, 1106, 1129, 6129, 114, 146, 1215, 1106, 1178, 1138, 6461, 1111, 124, 118, 125, 1552, 12477, 1775, 1145, 1189, 1139, 172, 4515, 3491, 5827, 1111, 1103, 1148, 1160, 1552, 1104, 1139, 1669, 117, 146, 1309, 1125, 172, 4515, 3491, 1196, 1606, 3485, 1654, 119, 2189, 1190, 1115, 1107, 2816, 1114, 1103, 10085, 107, 102]]\n"
     ]
    }
   ],
   "source": [
    "print([len(x) for x in tokenized_dataset[\"train\"][\"input_ids\"][:5]])\n",
    "print(tokenized_dataset['train']['input_ids'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8b3de-0af9-4ffa-bab0-d9631ae88732",
   "metadata": {},
   "source": [
    "#### Datasets\u001d",
    "과 DataFrames 간의 상호 변환\n",
    "다양한 서드 파티 라이브러리들 간의 변환을 가능하게 하기 위해 🤗Datasets는 Dataset.set_format() 함수를 제공합니다. 이 기능은 데이터셋의 출력 형식(output format)만 변경하므로 기본 데이터 포멧(Apache Arrow)에 영향을 주지 않고 다른 형식으로 쉽게 전환할 수 있습니다. 우리 데이터셋을 Pandas로 변환해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a73cce8-465d-43a7-b7a0-bb816400ca56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 138514\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b2af4c8-f5ac-48fe-bb6f-222cefb6dd99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drug_dataset.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e38259a7-5e21-4cd4-a591-a0290700613b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>adhd</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id    drugName      condition  \\\n",
       "0       95260  Guanfacine           adhd   \n",
       "1       92703      Lybrel  birth control   \n",
       "2      138000  Ortho Evra  birth control   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"My son is halfway through his fourth week of ...     8.0   \n",
       "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "2  \"This is my first time using any form of birth...     8.0   \n",
       "\n",
       "                date  usefulCount  review_length  \n",
       "0     April 27, 2010          192            141  \n",
       "1  December 14, 2009           17            134  \n",
       "2   November 3, 2015           10             89  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset[\"train\"][:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c13233e7-9da8-4227-8aec-b1e0d09b2765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = drug_dataset[\"train\"][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66ffbb1c-bccd-429b-bc7d-96fb14b01f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>adhd</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>opiate dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155963</td>\n",
       "      <td>Cialis</td>\n",
       "      <td>benign prostatic hyperplasia</td>\n",
       "      <td>\"2nd day on 5mg started to work with rock hard...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>November 28, 2015</td>\n",
       "      <td>43</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id                  drugName                     condition  \\\n",
       "0       95260                Guanfacine                          adhd   \n",
       "1       92703                    Lybrel                 birth control   \n",
       "2      138000                Ortho Evra                 birth control   \n",
       "3       35696  Buprenorphine / naloxone             opiate dependence   \n",
       "4      155963                    Cialis  benign prostatic hyperplasia   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"My son is halfway through his fourth week of ...     8.0   \n",
       "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "2  \"This is my first time using any form of birth...     8.0   \n",
       "3  \"Suboxone has completely turned my life around...     9.0   \n",
       "4  \"2nd day on 5mg started to work with rock hard...     2.0   \n",
       "\n",
       "                date  usefulCount  review_length  \n",
       "0     April 27, 2010          192            141  \n",
       "1  December 14, 2009           17            134  \n",
       "2   November 3, 2015           10             89  \n",
       "3  November 27, 2016           37            124  \n",
       "4  November 28, 2015           43             68  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "39b191bc-9658-472e-9814-b87d9857d6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(drug_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f74fb0e5-550a-4a99-84db-e068f0a4b316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "35b6fdcd-0630-4d1e-a857-49ee515a99e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>birth control</th>\n",
       "      <td>27655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depression</th>\n",
       "      <td>8023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acne</th>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anxiety</th>\n",
       "      <td>4991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pain</th>\n",
       "      <td>4744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parkinsonian trem</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rabies prophylaxis</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cerebral spasticity</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myelofibrosis</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutropenia</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>819 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count\n",
       "condition                 \n",
       "birth control        27655\n",
       "depression            8023\n",
       "acne                  5209\n",
       "anxiety               4991\n",
       "pain                  4744\n",
       "...                    ...\n",
       "parkinsonian trem        1\n",
       "rabies prophylaxis       1\n",
       "cerebral spasticity      1\n",
       "myelofibrosis            1\n",
       "neutropenia              1\n",
       "\n",
       "[819 rows x 1 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"condition\"].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dc3a8732-3a2c-43df-a4cc-334a9158dc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>birth control</td>\n",
       "      <td>27655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>depression</td>\n",
       "      <td>8023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acne</td>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>4991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pain</td>\n",
       "      <td>4744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>parkinsonian trem</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>rabies prophylaxis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>cerebral spasticity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>myelofibrosis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>neutropenia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>819 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               condition  count\n",
       "0          birth control  27655\n",
       "1             depression   8023\n",
       "2                   acne   5209\n",
       "3                anxiety   4991\n",
       "4                   pain   4744\n",
       "..                   ...    ...\n",
       "814    parkinsonian trem      1\n",
       "815   rabies prophylaxis      1\n",
       "816  cerebral spasticity      1\n",
       "817        myelofibrosis      1\n",
       "818          neutropenia      1\n",
       "\n",
       "[819 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"condition\"].value_counts().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f771b7f-62a7-460b-8397-a2a294e51ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>birth control</td>\n",
       "      <td>27655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>depression</td>\n",
       "      <td>8023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acne</td>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>4991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pain</td>\n",
       "      <td>4744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>parkinsonian trem</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>rabies prophylaxis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>cerebral spasticity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>myelofibrosis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>neutropenia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>819 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               frequency  count\n",
       "0          birth control  27655\n",
       "1             depression   8023\n",
       "2                   acne   5209\n",
       "3                anxiety   4991\n",
       "4                   pain   4744\n",
       "..                   ...    ...\n",
       "814    parkinsonian trem      1\n",
       "815   rabies prophylaxis      1\n",
       "816  cerebral spasticity      1\n",
       "817        myelofibrosis      1\n",
       "818          neutropenia      1\n",
       "\n",
       "[819 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"condition\"].value_counts().to_frame().reset_index().rename(columns = {\"condition\" : \"frequency\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33a76701-abc3-42ba-a126-71b47639527a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>birth control</td>\n",
       "      <td>27655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>depression</td>\n",
       "      <td>8023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acne</td>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>4991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pain</td>\n",
       "      <td>4744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       frequency  count\n",
       "0  birth control  27655\n",
       "1     depression   8023\n",
       "2           acne   5209\n",
       "3        anxiety   4991\n",
       "4           pain   4744"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = (\n",
    "    train_df[\"condition\"]\n",
    "    .value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n",
    ")\n",
    "frequencies.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "204f054d-a5d9-4ebc-bbad-90b86c59b13d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['frequency', 'count'],\n",
       "    num_rows: 819\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "freq_dataset = Dataset.from_pandas(frequencies)\n",
    "freq_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d3cc7c4f-df78-4d3a-bc21-b377ad514a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency': ['birth control', 'depression', 'acne', 'anxiety', 'pain'],\n",
       " 'count': [27655, 8023, 5209, 4991, 4744]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca3166a1-8860-4e4e-89b1-997e8570e880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drug_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5b0f4e1e-daa6-4d2f-9829-1772f1e2abcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 138514\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fece04ce-ead3-4565-89b6-ed97e7ea7f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4877857d-dbb7-4410-98a0-76492b39f113",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f5a684e5-1e03-4ead-9dc1-0f110423fd8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기본 \"test\" 분할을 \"validation\"으로 변경함.\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# 'DatasetDict'에 \"test\" 집합을 추가.\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa971b3f-b1b2-4e59-9316-639ea1c35dec",
   "metadata": {},
   "source": [
    "데이터셋 저장\n",
    "🤗Datasets는 다운로드한 모든 데이터셋과 이에 대해 수행된 작업을 임시저장하지만 데이터셋을 디스크에 저장하고 싶을 때가 있습니다 (예: 캐시가 삭제된 경우). 아래 표에서 볼 수 있듯이 🤗Datasets는 데이터셋을 다양한 형식으로 저장하는 세 가지 주요 기능을 제공합니다.\n",
    "\n",
    "|Data format|Function|\n",
    "|-----|-----|\n",
    "|Arrow|Dataset.save_to_disk()|\n",
    "|CSV|Dataset.to_csv()|\n",
    "|JSON|Dataset.to_json()|\n",
    "\n",
    "예를 들어, 정리된 데이터셋을 Arrow 형식으로 저장해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "20ef93ff-982e-4f4e-bde6-8d9047d17820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732ed276145249f08bf3dd678c8d975b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/110811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8233a70f864401e96b49bdb46a3f290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/27703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50716702b42040e79b334d2bfcf70d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drug_dataset_clean.save_to_disk(\"drug-reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "20f7080c-16ce-439f-a52f-33dbb3580a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"drug-reviews\")\n",
    "drug_dataset_reloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d236dfa8-97e3-4481-ac05-c551771e3be4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b99866de5774cb7881e43a5cf565ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/111 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bb985bac324ea1a490c9c730ff431e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/28 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5292e99727a4d0ea6726d93a66ac28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/47 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"drug-reviews-{split}.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81628299-41b6-41f5-992d-9bcd520e9ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"patient_id\":89879,\"drugName\":\"Cyclosporine\",\"condition\":\"keratoconjunctivitis sicca\",\"review\":\"\\\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I've had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I've talked with my doctor about this and he said it is normal but should go away after some time, but it hasn't. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I've been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I'm ready to move on.\\\"\",\"rating\":2.0,\"date\":\"April 20, 2013\",\"usefulCount\":69,\"review_length\":147}\n",
      "{\"patient_id\":143975,\"drugName\":\"Etonogestrel\",\"condition\":\"birth control\",\"review\":\"\\\"My experience has been somewhat mixed. I have been using Implanon now for nearly 14 months and have decided to get it removed because I bleed every day, all day. I would occasionally stain my underwear and my sheets. It didn't start out that way, for the first month I didn't bleed at all, then I had an epic two week period, then everything was irregular, which has basically been my new norm, sadly. \\r\\r\\n\\r\\r\\nI decided to get rid of the Implanon because of the endless bleeding. I should mention that the bleeding was usually pretty light, a bit more than spotting but not quite a period either. Other than the endless bleeding, I've been pretty side effect free, except for minor acne.\\r\\r\\n\\r\\r\\nI didn't get pregnant on it, so yeah! But...blood, a lot of blood.\\\"\",\"rating\":7.0,\"date\":\"August 7, 2016\",\"usefulCount\":4,\"review_length\":136}\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 drug-reviews-train.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e1a03a3d-de20-4280-90bb-cd0030d7e6da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a224ce2d2a5458f9faad611dd3303f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8d23be5b8640528ed497ea27363035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50ec5ef65e44b50832e16d053e10dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\": \"drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696833a-e51f-4693-badf-085830e04a6b",
   "metadata": {},
   "source": [
    "### 3. 🤗Datasets가 빅데이터 문제를 해결한다!\n",
    "특히 BERT 또는 GPT-2와 같은 트랜스포머(transformers)를 밑바닥부터 사전 학습(pretraining)할 계획이라면, 기가바이트 규모의 데이터셋으로 작업하는 것이 드문 일이 아닙니다. 이런 경우에는 데이터를 로드하는 것조차 어려울 수 있습니다. 예를 들어, GPT-2를 사전 학습(pretraining)하는 데 사용되는 WebText 코퍼스는 8백만 개 이상의 문서와 40GB의 텍스트로 구성됩니다. 이 데이터셋을 노트북의 RAM에 로드하면 노트북이 심장마비를 일으킬 수 있습니다!\n",
    "\n",
    "다행히도, 🤗Datasets는 이러한 한계를 극복하도록 설계되었습니다. 데이터셋을 메모리에 **매핑된(memory-mapped) 파일(datasets.arrow_dataset.Dataset)** 로 처리하여 메모리 관리 문제를 해결하고, 말뭉치의 각 항목들을 스트리밍하여 하드 디스크 제한에서 우리를 해방시킬 수 있습니다.\n",
    "\n",
    "이 섹션에서는 Pile로 알려진 825GB 규모의 거대한 말뭉치를 이용하여, 위에서 설명한 🤗Datasets의 기능들을 살펴보겠습니다. 시작해 봅시다!\n",
    "\n",
    "Huggingface Big data? 🤗 Datasets to the rescue:\n",
    "https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt\n",
    "\n",
    "The Pile GitHub site: https://github.com/EleutherAI/the-pile\n",
    "\n",
    "The PubMed Abstract GitHub site: https://github.com/thoppe/The-Pile-PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1173718e-6cae-46b0-8f6c-5a40bf04a25a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting zstandard\n",
      "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard\n",
      "Successfully installed zstandard-0.22.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install zstandard\n",
    "# 만일 설치가 반영이 제대로 안된다면, 다음 명령어로 설치하고 커널을 다시 실행해야 합니다.\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install zstandard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1060b61b-6f2d-4755-82af-8e7b1b374cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 아래 코드는 실행하는데 몇분이 소요됩니다.\n",
    "#data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
    "#data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
    "#pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
    "#pubmed_dataset = load_dataset(\"pubmed\")\n",
    "#pubmed = load_dataset('pubmed', streaming=True)\n",
    "#https://github.com/Shaier/download_pubmed\n",
    "#pubmed_dataset = load_dataset(\"Shaier/pubmed\", split=\"train\")\n",
    "#pubmed_dataset = load_dataset(\"Shaier/pubmed\", download_mode=\"force_redownload\")\n",
    "#pubmed_dataset = load_dataset(\"json\", data_files=\"The-Pile-PubMed/data/PUBMED_title_abstracts_2020_baseline.jsonl\")\n",
    "#pubmed_dataset = load_dataset(\"json\", data_files=\"../genai.bak/The-Pile-PubMed/data/PUBMED_title_abstracts_2020_baseline.jsonl\")\n",
    "#pubmed_dataset_streamed = load_dataset(\"json\", data_files=\"pubmed.train.json\", split=\"train\", streaming=True)\n",
    "#pubmed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6f247e-6992-491c-9e55-9b18076e538d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pubmed_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fef07a-72fc-4019-8928-87aa987c3c56",
   "metadata": {},
   "source": [
    "#### The PUBMED Dataset Creation and Upload it to Hugging Face \n",
    "The PUBMED Dataset was created based on based on the The [PubMed Abstract GitHub Site](https://github.com/thoppe/The-Pile-PubMed) and uploaded on the HuggingFace:\n",
    "\n",
    "- The PUBMED dataset reproduced\n",
    "```\n",
    "$git clone https://github.com/thoppe/The-Pile-PubMed.git\n",
    "$cd The-Pile-PubMed/\n",
    "$python P0_download_listing.py\n",
    "$python P1_download_baseline.py\n",
    "$python P2_parse.py\n",
    "$python P3_build_final_LM_dataset.py\n",
    "```\n",
    "\n",
    "- The PUBMED dataset uploaded to Hugging Face\n",
    "```\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "from huggingface_hub import create_repo\n",
    "repo_url = create_repo(\"PUBMED_title_abstracts_2020_baseline\", repo_type=\"dataset\")\n",
    "repo_url\n",
    "\n",
    "from huggingface_hub import Repository\n",
    "repo = Repository(local_dir=\"PUBMED_title_abstracts_2020_baseline\", clone_from=repo_url)\n",
    "!cp data/PUBMED_title_abstracts_2020_baseline.jsonl.zst PUBMED_title_abstracts_2020_baseline/\n",
    "\n",
    "repo.lfs_track(\"*.jsonl.zst\")\n",
    "repo.push_to_hub()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f22ada65-b0dc-46e5-af4f-22d3fc138e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66448807c2946149db3c8b6f6a34d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1301415b9d1409cb9d5e59620632090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069b29e850784707904747b502677a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['meta', 'text'],\n",
       "        num_rows: 17722096\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "pubmed_dataset = load_dataset(\"qualis2006/PUBMED_title_abstracts_2020_baseline\")\n",
    "pubmed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05f9621-f4e9-496c-825f-a6a45b686a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b933d189e74ef3a73a05e3615e98d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1183071a6f16445793229ed9e3f3b364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['meta', 'text'],\n",
       "    num_rows: 17722096\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "pubmed_train_dataset = load_dataset(\"hwang2006/PUBMED_title_abstracts_2020_baseline\", split=\"train\")\n",
    "pubmed_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798e6daf-4e77-4cb6-8950-971c1dd24331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "24453015916\n"
     ]
    }
   ],
   "source": [
    "print(type(pubmed_train_dataset))\n",
    "print(pubmed_train_dataset.dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d48861-e033-4f64-a01d-8300c5ea2329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access The-Pile-PubMed/data/PUBMED_title_abstracts_2020_baseline.jsonl: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -al The-Pile-PubMed/data/PUBMED_title_abstracts_2020_baseline.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c27678c0-d5e9-4ee5-9507-dba0ee1dd8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'pmid': 1673585, 'language': 'eng'},\n",
       " 'text': 'Cardiac beta-adrenoceptor regulation and the effects of partial agonism.\\nThe in vivo effects of xamoterol on the regulation of rat cardiac beta adrenoceptors were investigated. Rats were implanted subcutaneously with osmotic minipumps and exposed to the following treatment regimens: (1) subcutaneous infusion of saline (control), isoprenaline or xamoterol for 6 days, (2) subcutaneous infusion of isoprenaline with co-administration of xamoterol for various periods up to 96 hours, and (3) subcutaneous infusion of xamoterol for up to 96 hours after previous treatment with isoprenaline for 72 hours. Xamoterol did not induce beta-adrenoceptor down-regulation after short-term (72-hour) or long-term (6-day) infusions. When coadministered with isoprenaline xamoterol did not affect the rate or extent of down-regulation induced by isoprenaline alone. In addition, recovery of beta adrenoceptors down-regulated by isoprenaline treatment was not influenced by xamoterol treatment. In all studies, doses of xamoterol were equivalent to those producing full functional responses to the drug in vivo.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbc7ed-6aad-427a-956d-e933f275327f",
   "metadata": {},
   "source": [
    "다음과 같이, 현재 프로세스의 메모리 사용량을 확인할 수 있는 Process 클래스를 제공합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d984645-1bdc-4fed-ab6d-d37c56ec480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 489.14 MB\n",
      "RAM used: 489.140625 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Process.memory_info는 바이트 단위로 표시되므로 이를 메가바이트로 변환합니다.\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024)} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c7ce6-0299-4951-ac34-0be118bc6978",
   "metadata": {},
   "source": [
    "여기에서 rss 속성은 프로세스가 RAM에서 차지하는 메모리 비율인 resident set size(상주 세트 크기)를 나타냅니다. 측정 과정에서 Python 인터프리터와 우리가 로드한 라이브러리가 사용하는 메모리양도 포함되므로 데이터셋을 로드하는데 사용되는 실제 메모리 양은 약간 더 적습니다. 비교를 위해 dataset_size 속성을 사용하여 데이터셋이 디스크에서 어느 정도 크기인지 봅시다. 결과는 이전과 같이 바이트로 표시되므로 수동으로 기가바이트로 변환해야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9542642c-579d-417f-970b-642efe0c664f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in dataset : 24453015916\n",
      "Dataset size (cache file) : 22.77 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of files in dataset : {pubmed_dataset['train'].dataset_size}\")\n",
    "size_gb = pubmed_dataset[\"train\"].dataset_size / (1024 ** 3)\n",
    "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c051ad8-7e0d-4f1f-906f-5a9416b22060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in dataset : 24453015916\n",
      "Dataset size (cache file) : 22.77 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of files in dataset : {pubmed_train_dataset.dataset_size}\")\n",
    "size_gb = pubmed_train_dataset.dataset_size / (1024 ** 3)\n",
    "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422bc55-f6b0-4cd3-8567-421174364192",
   "metadata": {},
   "source": [
    "좋습니다. 거의 20GB 크기에도 불구하고 훨씬 적은 RAM으로 데이터셋을 로드하고 액세스할 수 있습니다!\n",
    "\n",
    "Pandas에 익숙하다면 일반적으로 데이터셋 크기보다 5~10배 많은 RAM이 필요하다는 Wes Kinney의 유명한 경험 법칙(rule of thumb) 때문에 이 결과가 놀라울 수 있습니다. 그렇다면 🤗Datasets는 메모리 관리 문제를 어떻게 해결할까요? 🤗Datasets는 각 데이터셋을 메모리 매핑된 파일(memory-mapped file)로 처리합니다. 이 파일은 RAM과 파일 시스템 스토리지 간의 매핑을 제공하여 라이브러리가 데이터셋을 메모리에 완전히 로드할 필요없이 각 요소에 액세스하고 작동할 수 있도록 해줍니다.\n",
    "\n",
    "메모리 매핑된 파일(memory-mapped file)은 여러 프로세스에서 공유될 수도 있으므로 데이터셋을 이동하거나 복사할 필요없이 Dataset.map()과 같은 메서드를 병렬화할 수 있습니다. 내부적으로, 이러한 기능은 모두 Apache Arrow 메모리 형식 및 pyarrow 라이브러리에 의해 구현되어 데이터 로드 및 처리를 매우 빠르게 할수 있습니다. Apache Arrow 및 Pandas와의 비교에 대한 자세한 내용은 [Dejan Simic의 블로그](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a) 게시물을 확인하세요. 실제로 작동하는지 확인하기 위해 PubMed Abstracts 데이터셋의 모든 요소를 반복하여 실행 속도 테스트를 해보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e078f6d1-cc02-4190-a815-22662b998878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "code_snippet = \"\"\"batch_size = 1000\n",
    "\n",
    "for idx in range(0, len(pubmed_train_dataset), batch_size):\n",
    "    _ = pubmed_train_dataset[idx:idx + batch_size]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c723d944-dbcf-478a-ad6b-5675651bf933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterated over 17722096 examples (about 22.8 GB in 91.4s, i.e. 0.249 GB/s\n"
     ]
    }
   ],
   "source": [
    "time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n",
    "print(\n",
    "    f\"Iterated over {len(pubmed_train_dataset)} examples (about {size_gb:.1f} GB in \"\n",
    "    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6e56537-767f-408f-be5c-675ce5addf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 0 ns, total: 1min 16s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 1000\n",
    "\n",
    "for idx in range(0, len(pubmed_train_dataset), batch_size):\n",
    "    _ = pubmed_train_dataset[idx:idx + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6cd342-94fd-420e-a0cb-bc495cfe1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#batch_size = 1000\n",
    "\n",
    "#for idx in range(0, len(pubmed_train_dataset), batch_size):\n",
    "#    _ = pubmed_train_dataset[idx:idx + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd23fb91-ef3d-45f0-91bf-ab15aedc36c6",
   "metadata": {},
   "source": [
    "여기서 우리는 Python의 timeit 모듈을 사용하여 code_snippet\u001d",
    "의 실행 시간을 측정했습니다. 일반적으로 수십 GB/s에서 수 GB/s의 속도로 데이터셋을 반복할 수 있습니다. 이것은 대부분의 응용 프로그램에서 훌륭하게 작동하지만 때로는 노트북의 하드 드라이브에 저장하기에도 너무 큰 데이터셋으로 작업해야 하는 상황이 있을 수 있습니다. 예를 들어, Pile 전체를 다운로드하려고 하면 825GB의 여유 디스크 공간이 필요합니다! 이러한 경우를 처리하기 위해 🤗Datasets는 **전체 데이터셋을 다운로드할 필요없이 즉시 요소를 다운로드하고 액세스할 수 있는 스트리밍 기능**을 제공합니다. 이것이 어떻게 작동하는지 살펴보겠습니다.\n",
    "\n",
    "#### 스트리밍 데이터셋 (Streaming datasets)\n",
    "데이터셋 스트리밍을 활성화하려면 streaming=True 인수를 load_dataset() 함수에 전달하기만 하면 됩니다. 예를 들어, 스트리밍 모드에서 PubMed Abstracts 데이터셋을 다시 로드해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4072b9f8-e3de-4b44-9604-19c8c8e18b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_files = \"The-Pile-PubMed/data/PUBMED_title_abstracts_2020_baseline.jsonl\"\n",
    "data_files = \"PUBMED_title_abstracts_2020_baseline/PUBMED_title_abstracts_2020_baseline.jsonl\"\n",
    "pubmed_dataset_streamed = load_dataset(\n",
    "    \"json\", data_files=data_files, split=\"train\", streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68ad3301-9a30-43ae-a950-25b0ebdbb7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    n_shards: 1\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_dataset_streamed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ca09f-68d2-4b91-b07c-d630e3e789b3",
   "metadata": {},
   "source": [
    "우리에게 이미 친숙한 Dataset 대신 streaming=True로 반환된 객체는 IterableDataset입니다. 이름에서 알 수 있듯이 IterableDataset의 요소에 액세스하려면 반복해야 합니다. 다음과 같이 스트리밍된 데이터셋의 첫 번째 요소에 액세스할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e4d118d-148a-4d47-bbab-0d0b4ce19616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'pmid': 1673585, 'language': 'eng'},\n",
       " 'text': 'Cardiac beta-adrenoceptor regulation and the effects of partial agonism.\\nThe in vivo effects of xamoterol on the regulation of rat cardiac beta adrenoceptors were investigated. Rats were implanted subcutaneously with osmotic minipumps and exposed to the following treatment regimens: (1) subcutaneous infusion of saline (control), isoprenaline or xamoterol for 6 days, (2) subcutaneous infusion of isoprenaline with co-administration of xamoterol for various periods up to 96 hours, and (3) subcutaneous infusion of xamoterol for up to 96 hours after previous treatment with isoprenaline for 72 hours. Xamoterol did not induce beta-adrenoceptor down-regulation after short-term (72-hour) or long-term (6-day) infusions. When coadministered with isoprenaline xamoterol did not affect the rate or extent of down-regulation induced by isoprenaline alone. In addition, recovery of beta adrenoceptors down-regulated by isoprenaline treatment was not influenced by xamoterol treatment. In all studies, doses of xamoterol were equivalent to those producing full functional responses to the drug in vivo.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(pubmed_dataset_streamed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0ca5a-0818-4d6f-823a-614be1c82252",
   "metadata": {},
   "source": [
    "스트리밍된 데이터셋의 요소는 IterableDataset.map()을 사용하여 바로 처리할 수 있으며, 이는 학습 과정에서 입력을 토큰화해야 하는 경우 유용합니다. 이 작업은 3장에서 데이터셋을 토큰화하는데 사용한 방법과 동일하지만 출력이 하나씩 반환된다는 점만 다릅니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea676cf5-c618-44b6-9054-44f4bc5da0d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d2e571c54741748077abde48c5d8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c658d263eb6e40aab6c998e4787e0536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9f1e8f3eed4f03bbe6b02fabf83a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe56e23591a84891b853a210a7d7781f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99 µs, sys: 9 µs, total: 108 µs\n",
      "Wall time: 113 µs\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "%time tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x[\"text\"]))\n",
    "#next(iter(tokenized_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3f397a7-6abf-49fe-bab1-4047b6ff320a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 101 µs, sys: 0 ns, total: 101 µs\n",
      "Wall time: 105 µs\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "%time tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f2940-94d2-4229-8322-d1041e4edfc3",
   "metadata": {},
   "source": [
    "스트리밍으로 토큰화 속도를 높이려면 이전 섹션에서 본 것처럼 batched=True를 전달하면 됩니다. 그러면 배치(batch)별로 예제를 처리합니다. 기본 배치 크기는 1,000이며 batch_size 인수로 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06ebf123-46ec-4cb3-bc3e-45232def89c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta': {'pmid': 1673585, 'language': 'eng'}, 'text': 'Cardiac beta-adrenoceptor regulation and the effects of partial agonism.\\nThe in vivo effects of xamoterol on the regulation of rat cardiac beta adrenoceptors were investigated. Rats were implanted subcutaneously with osmotic minipumps and exposed to the following treatment regimens: (1) subcutaneous infusion of saline (control), isoprenaline or xamoterol for 6 days, (2) subcutaneous infusion of isoprenaline with co-administration of xamoterol for various periods up to 96 hours, and (3) subcutaneous infusion of xamoterol for up to 96 hours after previous treatment with isoprenaline for 72 hours. Xamoterol did not induce beta-adrenoceptor down-regulation after short-term (72-hour) or long-term (6-day) infusions. When coadministered with isoprenaline xamoterol did not affect the rate or extent of down-regulation induced by isoprenaline alone. In addition, recovery of beta adrenoceptors down-regulated by isoprenaline treatment was not influenced by xamoterol treatment. In all studies, doses of xamoterol were equivalent to those producing full functional responses to the drug in vivo.', 'input_ids': [101, 15050, 8247, 1011, 4748, 7389, 10085, 23606, 2953, 7816, 1998, 1996, 3896, 1997, 7704, 3283, 28113, 1012, 1996, 1999, 24269, 3896, 1997, 1060, 22591, 27833, 2006, 1996, 7816, 1997, 9350, 15050, 8247, 4748, 7389, 10085, 23606, 5668, 2020, 10847, 1012, 11432, 2020, 27159, 2098, 4942, 12690, 17191, 2135, 2007, 9808, 18938, 2594, 7163, 14289, 25370, 1998, 6086, 2000, 1996, 2206, 3949, 6939, 3619, 1024, 1006, 1015, 1007, 4942, 12690, 17191, 1999, 20523, 1997, 28413, 1006, 2491, 1007, 1010, 11163, 28139, 12032, 3170, 2030, 1060, 22591, 27833, 2005, 1020, 2420, 1010, 1006, 1016, 1007, 4942, 12690, 17191, 1999, 20523, 1997, 11163, 28139, 12032, 3170, 2007, 2522, 1011, 3447, 1997, 1060, 22591, 27833, 2005, 2536, 6993, 2039, 2000, 5986, 2847, 1010, 1998, 1006, 1017, 1007, 4942, 12690, 17191, 1999, 20523, 1997, 1060, 22591, 27833, 2005, 2039, 2000, 5986, 2847, 2044, 3025, 3949, 2007, 11163, 28139, 12032, 3170, 2005, 5824, 2847, 1012, 1060, 22591, 27833, 2106, 2025, 19653, 8247, 1011, 4748, 7389, 10085, 23606, 2953, 2091, 1011, 7816, 2044, 2460, 1011, 2744, 1006, 5824, 1011, 3178, 1007, 2030, 2146, 1011, 2744, 1006, 1020, 1011, 2154, 1007, 1999, 20523, 2015, 1012, 2043, 28155, 22117, 5498, 24167, 2007, 11163, 28139, 12032, 3170, 1060, 22591, 27833, 2106, 2025, 7461, 1996, 3446, 2030, 6698, 1997, 2091, 1011, 7816, 10572, 2011, 11163, 28139, 12032, 3170, 2894, 1012, 1999, 2804, 1010, 7233, 1997, 8247, 4748, 7389, 10085, 23606, 5668, 2091, 1011, 12222, 2011, 11163, 28139, 12032, 3170, 3949, 2001, 2025, 5105, 2011, 1060, 22591, 27833, 3949, 1012, 1999, 2035, 2913, 1010, 21656, 1997, 1060, 22591, 27833, 2020, 5662, 2000, 2216, 5155, 2440, 8360, 10960, 2000, 1996, 4319, 1999, 24269, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(tokenized_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10fac322-acd8-4de8-9c3b-7741b06bb7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    n_shards: 1\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb93eadb-64ec-4a50-9fa3-dfb3a2275baa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta': {'pmid': 1673585, 'language': 'eng'}, 'text': 'Cardiac beta-adrenoceptor regulation and the effects of partial agonism.\\nThe in vivo effects of xamoterol on the regulation of rat cardiac beta adrenoceptors were investigated. Rats were implanted subcutaneously with osmotic minipumps and exposed to the following treatment regimens: (1) subcutaneous infusion of saline (control), isoprenaline or xamoterol for 6 days, (2) subcutaneous infusion of isoprenaline with co-administration of xamoterol for various periods up to 96 hours, and (3) subcutaneous infusion of xamoterol for up to 96 hours after previous treatment with isoprenaline for 72 hours. Xamoterol did not induce beta-adrenoceptor down-regulation after short-term (72-hour) or long-term (6-day) infusions. When coadministered with isoprenaline xamoterol did not affect the rate or extent of down-regulation induced by isoprenaline alone. In addition, recovery of beta adrenoceptors down-regulated by isoprenaline treatment was not influenced by xamoterol treatment. In all studies, doses of xamoterol were equivalent to those producing full functional responses to the drug in vivo.', 'input_ids': [101, 15050, 8247, 1011, 4748, 7389, 10085, 23606, 2953, 7816, 1998, 1996, 3896, 1997, 7704, 3283, 28113, 1012, 1996, 1999, 24269, 3896, 1997, 1060, 22591, 27833, 2006, 1996, 7816, 1997, 9350, 15050, 8247, 4748, 7389, 10085, 23606, 5668, 2020, 10847, 1012, 11432, 2020, 27159, 2098, 4942, 12690, 17191, 2135, 2007, 9808, 18938, 2594, 7163, 14289, 25370, 1998, 6086, 2000, 1996, 2206, 3949, 6939, 3619, 1024, 1006, 1015, 1007, 4942, 12690, 17191, 1999, 20523, 1997, 28413, 1006, 2491, 1007, 1010, 11163, 28139, 12032, 3170, 2030, 1060, 22591, 27833, 2005, 1020, 2420, 1010, 1006, 1016, 1007, 4942, 12690, 17191, 1999, 20523, 1997, 11163, 28139, 12032, 3170, 2007, 2522, 1011, 3447, 1997, 1060, 22591, 27833, 2005, 2536, 6993, 2039, 2000, 5986, 2847, 1010, 1998, 1006, 1017, 1007, 4942, 12690, 17191, 1999, 20523, 1997, 1060, 22591, 27833, 2005, 2039, 2000, 5986, 2847, 2044, 3025, 3949, 2007, 11163, 28139, 12032, 3170, 2005, 5824, 2847, 1012, 1060, 22591, 27833, 2106, 2025, 19653, 8247, 1011, 4748, 7389, 10085, 23606, 2953, 2091, 1011, 7816, 2044, 2460, 1011, 2744, 1006, 5824, 1011, 3178, 1007, 2030, 2146, 1011, 2744, 1006, 1020, 1011, 2154, 1007, 1999, 20523, 2015, 1012, 2043, 28155, 22117, 5498, 24167, 2007, 11163, 28139, 12032, 3170, 1060, 22591, 27833, 2106, 2025, 7461, 1996, 3446, 2030, 6698, 1997, 2091, 1011, 7816, 10572, 2011, 11163, 28139, 12032, 3170, 2894, 1012, 1999, 2804, 1010, 7233, 1997, 8247, 4748, 7389, 10085, 23606, 5668, 2091, 1011, 12222, 2011, 11163, 28139, 12032, 3170, 3949, 2001, 2025, 5105, 2011, 1060, 22591, 27833, 3949, 1012, 1999, 2035, 2913, 1010, 21656, 1997, 1060, 22591, 27833, 2020, 5662, 2000, 2216, 5155, 2440, 8360, 10960, 2000, 1996, 4319, 1999, 24269, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(tokenized_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbc652-1d66-4e5b-ad63-25808a38020d",
   "metadata": {},
   "source": [
    "스트리밍으로 토큰화 속도를 높이려면 이전 섹션에서 본 것처럼 batched=True를 전달하면 됩니다. 그러면 배치(batch)별로 예제를 처리합니다. 기본 배치 크기는 1,000이며 batch_size 인수로 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "effb0439-4cfb-4979-82c4-e1eda2d14326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta': {'pmid': 1675166, 'language': 'ita'}, 'text': '[Benzodiazepine withdrawal syndrome].\\nBenzodiazepines (BDZ) are widely prescribed in clinical practice for many pathological conditions, because of their anxiolytic, sedative, myorelaxant and anticonvulsant properties. The effectiveness, specificity and rapidity of action, the few side effects and the virtual absence of toxicity, have contributed to the widespread use of these compounds. In the last decade, however, the attitude towards BDZ has greatly changed, due to growing awareness and concern about dependence liability, withdrawal phenomena, and long-term side effects. Withdrawal symptoms have been singled out and specified in the contest of a well-defined syndrome with foreseeable onset, duration and remission. Psychic and physical symptoms and disorders of sensory perception can be observed. These manifestations can be suppressed by resuming treatment. The symptomatic and developmental aspects of BDZ withdrawal syndrome are discussed, according to the available literature, with particular reference to clinical features of patients suffering from anxiety and mood disorders.'}\n"
     ]
    }
   ],
   "source": [
    "shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10000, seed=42)\n",
    "print(next(iter(shuffled_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686eca2-fe18-4b6c-a4d8-c45d67749601",
   "metadata": {},
   "source": [
    "위 예에서는 버퍼의 처음 10,000개 예제에서 임의로 예제를 선택했습니다. 일단 예제에 접근하면 버퍼의 해당 지점이 말뭉치의 다음 예제로 채워집니다 (즉, 위의 경우 10,001번째 예제). Dataset.select()와 유사한 방식으로 작동하는 IterableDataset.take() 및 IterableDataset.skip() 함수를 사용하여 스트리밍된 데이터셋에서 요소를 선택할 수도 있습니다. 예를 들어, PubMed Abstracts 데이터셋에서 처음 5개의 예제를 선택하기 위해 다음을 수행할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15ef00ca-1df3-44ab-af64-f9441a7b7276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'meta': {'pmid': 1673585, 'language': 'eng'},\n",
       "  'text': 'Cardiac beta-adrenoceptor regulation and the effects of partial agonism.\\nThe in vivo effects of xamoterol on the regulation of rat cardiac beta adrenoceptors were investigated. Rats were implanted subcutaneously with osmotic minipumps and exposed to the following treatment regimens: (1) subcutaneous infusion of saline (control), isoprenaline or xamoterol for 6 days, (2) subcutaneous infusion of isoprenaline with co-administration of xamoterol for various periods up to 96 hours, and (3) subcutaneous infusion of xamoterol for up to 96 hours after previous treatment with isoprenaline for 72 hours. Xamoterol did not induce beta-adrenoceptor down-regulation after short-term (72-hour) or long-term (6-day) infusions. When coadministered with isoprenaline xamoterol did not affect the rate or extent of down-regulation induced by isoprenaline alone. In addition, recovery of beta adrenoceptors down-regulated by isoprenaline treatment was not influenced by xamoterol treatment. In all studies, doses of xamoterol were equivalent to those producing full functional responses to the drug in vivo.'},\n",
       " {'meta': {'pmid': 1673586, 'language': 'eng'},\n",
       "  'text': 'Interrupting the adaptive changes in congestive heart failure.\\nCirculating plasma concentrations of norepinephrine, renin, angiotensin and vasopressin are increased in congestive heart failure. By increasing ventricular afterload, heart failure is further worsened, which in turn--in a vicious cycle--stimulates neurohumoral vasoconstrictor mechanisms. Furthermore, because of the compensatory but excessive stimulation of the sympathomimetic system, a down-regulation and desensitization particularly of the myocardial beta 1 receptors and depletion of myocardial catecholamine occurs in chronic heart failure. These defects may be restored toward normal by interventions that attenuate the activity of the sympathetic nervous system. A direct approach to modify the excessive vasoconstriction is to administer systemic vasodilator drugs, but despite favorable short-term effects, tolerance developed to most of these drugs during long-term treatment. One reason for the loss of effectiveness is the reflex activation of the sympathetic system, which increases vasoconstrictor hormone concentrations. Activation of the renin-angiotensin system can be modified effectively by angiotensin-converting enzyme inhibitors that have shown favorable responses in patients with chronic heart failure. Beta-blocking agents interfere with endogenous sympathetic activation and have produced beneficial effects in patients with congestive cardiomyopathy. Long-term treatment is associated with up-regulation of the number of beta receptors and an improved responsiveness to catecholamines. Owing to the negative inotropic effects of beta-blocking agents, some of the patients with severe heart failure deteriorated hemodynamically and clinically. Theoretically, it should be advantageous to have a substance that combines protection against excessive beta stimulation with a mild inotropic support to prevent cardiac decompensation. This may be achieved by a selective beta 1-partial agonist like xamoterol.'},\n",
       " {'meta': {'pmid': 1673587, 'language': 'eng'},\n",
       "  'text': 'Acute and chronic hemodynamic effects of xamoterol in mild to moderate congestive heart failure.\\nXamoterol, a new beta 1 partial agonist, has the potential to modulate cardiac response to variations in sympathetic tone in patients with heart failure. Its properties should result in beta-receptor stimulatory effects at low levels of sympathetic tone and beta-receptor protective effects at higher levels of sympathetic tone. The acute effects of intravenous (i.v.) xamoterol on hemodynamics at rest and during exercise were studied in 30 patients with mild to moderate heart failure (13 patients in New York Heart Association class II; 17 in class III) due to ischemic (n = 24) or cardiomyopathic (n = 6) heart disease. Cardiac index, stroke volume and stroke work index at rest were significantly improved after i.v. administration of xamoterol and consistent with net agonist effects. During exercise, heart rate and double product were significantly reduced (net antagonist effects), but with preservation of the expected increases in cardiac index and systolic blood pressure. These hemodynamic findings confirm the ability of xamoterol to modulate cardiac response to variations in sympathetic tone. Tachyphylaxis and arrhythmogenicity limit the chronic use of drugs with full beta-agonist properties as positive inotropes in heart failure. The patients were therefore entered into a 6-month double-blind, placebo-controlled, crossover study of chronic oral xamoterol therapy, 200 mg twice daily, and the hemodynamic responses to i.v. xamoterol were repeated at the end of the trial. No impairment in either resting or exercise effects was observed, indicative of a maintained response and absence of tachyphylaxis after chronic therapy. Furthermore, 24-hour ambulatory electrocardiographic monitoring showed no change in ventricular arrhythmias during oral treatment.(ABSTRACT TRUNCATED AT 250 WORDS)'},\n",
       " {'meta': {'pmid': 1673588, 'language': 'eng'},\n",
       "  'text': 'Local cardiac responses--alternative methods of control.\\nMuch attention has been paid to the influence of the beta-adrenoceptor system on cardiac function in heart failure. Full agonists and partial agonists acting on cardiac beta 1 receptors have been widely investigated, as has the density of these receptors in the failing heart. However, other cardiac control mechanisms may play important roles in the normal heart as well as in heart failure. The Frank-Starling mechanism of enhanced cardiac contraction produced by mechanical stretching of the ventricular myofibrils is well known. When treating patients with heart failure with diuretics, vasodilators and other drugs that influence preload, it is important to consider their overall effects in relation to the Starling curves. Atrial stretching also produces compensatory responses which are currently being intensively studied. Reflex release of atrial natriuretic factor after stimulation of atrial receptors has important physiologic effects in heart failure. The atria, but not the ventricles, are innervated by the vagus; the influence of the parasympathetic nervous system on the heart and circulation is often overlooked. The initial increase in heart rate during exercise is primarily due to withdrawal of vagal influence. Besides acetylcholine, the parasympathetic transmitter, many other local hormones may affect cardiac function; these include prostaglandins, 5-hydroxytryptamine and histamine. Although the activity of the sympathetic nervous system is mediated primarily through beta 1 adrenoceptors, both beta 2 and alpha receptors are also found in the heart. Myocardial alpha 1 receptors, which mediate a positive inotropic effect, have been identified, and prejunctional alpha 2 receptors may mediate inhibition of norepinephrine release from sympathetic nerves.(ABSTRACT TRUNCATED AT 250 WORDS)'},\n",
       " {'meta': {'pmid': 1673589, 'language': 'eng'},\n",
       "  'text': 'Dietary alpha-linolenic acid is as effective as oleic acid and linoleic acid in lowering blood cholesterol in normolipidemic men.\\nThe effect of dietary oleic acid (OA), linoleic acid (LA), and linolenic acid (LNA) on plasma lipid metabolism was studied in eight normolipidemic men. A mixed-fat diet composed of conventional foods was fed during 6-d pre- and post-experimental periods. The same basic diet but with 75% of the fat (26% of total energy) provided by sunflower and olive; canola; soybean; and sunflower, olive, and flax oils was fed during four 18-d experimental periods. Mean plasma total cholesterol (-18%), low-density-lipoprotein-cholesterol, (-22%) and very-low-density-lipoprotein-cholesterol (-41%) concentrations were significantly (P less than 0.004) lower after the experimental diets than after the mixed-fat diet. Mean serum apolipoprotein B (-19%) and apolipoprotein A-I (-9%) concentrations were also significantly (P less than 0.0007) lower after the experimental diets. The experimental diets were equally effective in lowering total and lipoprotein cholesterol and apolipoprotein concentrations in plasma, indicating that dietary OA, LA, and LNA were equally hypocholesterolemic.'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_head = pubmed_dataset_streamed.take(5)\n",
    "list(dataset_head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9638f9-77a3-4ade-80b9-a26c950a82d7",
   "metadata": {},
   "source": [
    "위 예에서는 버퍼의 처음 10,000개 예제에서 임의로 예제를 선택했습니다. 일단 예제에 접근하면 버퍼의 해당 지점이 말뭉치의 다음 예제로 채워집니다 (즉, 위의 경우 10,001번째 예제). Dataset.select()와 유사한 방식으로 작동하는 IterableDataset.take() 및 IterableDataset.skip() 함수를 사용하여 스트리밍된 데이터셋에서 요소를 선택할 수도 있습니다. 예를 들어, PubMed Abstracts 데이터셋에서 처음 5개의 예제를 선택하기 위해 다음을 수행할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "458deddb-3a95-4148-a895-10a529d0302d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 처음 1000개의 예제를 스킵하고 나머지를 학습 집합으로 포함시킨다.\n",
    "train_dataset = shuffled_dataset.skip(1000)\n",
    "# 처음 1000개의 예제를 검증 집합으로 구성한다.\n",
    "validation_dataset = shuffled_dataset.take(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2849138f-0bfb-4c18-98c7-41ab54705eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(list(validation_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119368f2-5ec5-49ca-8dce-8e2a22a29954",
   "metadata": {},
   "source": [
    "여러 데이터셋을 결합하여 단일 코퍼스를 생성하는 하나의 일반적인 응용방법을 설명함으로써 데이터셋 스트리밍에 대한 설명을 마무리하겠습니다. 🤗Datasets는 IterableDataset 객체 리스트를 단일 IterableDataset으로 변환하는 interleave_datasets() 함수를 제공합니다. 여기서 새 데이터셋의 요소는 소스 예제를 번갈아 가며 얻습니다. 이 기능은 대규모 데이터셋을 결합하려고 할 때 특히 유용합니다. 예를 들어 미국 법원(US courts)의 법적 의견(legal opinions)이 담긴 51GB 데이터셋인 Pile의 FreeLaw 하위세트(subsets)들을 스트리밍해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3dca74f2-2fe0-410a-b71c-207ae9f9d05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'pmid': 1673585, 'language': 'eng'},\n",
       " 'text': 'Cardiac beta-adrenoceptor regulation and the effects of partial agonism.\\nThe in vivo effects of xamoterol on the regulation of rat cardiac beta adrenoceptors were investigated. Rats were implanted subcutaneously with osmotic minipumps and exposed to the following treatment regimens: (1) subcutaneous infusion of saline (control), isoprenaline or xamoterol for 6 days, (2) subcutaneous infusion of isoprenaline with co-administration of xamoterol for various periods up to 96 hours, and (3) subcutaneous infusion of xamoterol for up to 96 hours after previous treatment with isoprenaline for 72 hours. Xamoterol did not induce beta-adrenoceptor down-regulation after short-term (72-hour) or long-term (6-day) infusions. When coadministered with isoprenaline xamoterol did not affect the rate or extent of down-regulation induced by isoprenaline alone. In addition, recovery of beta adrenoceptors down-regulated by isoprenaline treatment was not influenced by xamoterol treatment. In all studies, doses of xamoterol were equivalent to those producing full functional responses to the drug in vivo.'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(pubmed_dataset_streamed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a890029-3f6c-4a6c-8fc2-3b53efa5da6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'Unnamed: 0': 206461, 'drugName': 'Valsartan', 'condition': 'Left Ventricular Dysfunction', 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"', 'rating': 9.0, 'date': 'May 20, 2012', 'usefulCount': 27}\n",
      "1 {'Unnamed: 0': 95260, 'drugName': 'Guanfacine', 'condition': 'ADHD', 'review': '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"', 'rating': 8.0, 'date': 'April 27, 2010', 'usefulCount': 192}\n",
      "2 {'Unnamed: 0': 92703, 'drugName': 'Lybrel', 'condition': 'Birth Control', 'review': '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it&#039;s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn&#039;t have any other side effects. The idea of being period free was so tempting... Alas.\"', 'rating': 5.0, 'date': 'December 14, 2009', 'usefulCount': 17}\n",
      "3 {'Unnamed: 0': 138000, 'drugName': 'Ortho Evra', 'condition': 'Birth Control', 'review': '\"This is my first time using any form of birth control. I&#039;m glad I went with the patch, I have been on it for 8 months. At first It decreased my libido but that subsided. The only downside is that it made my periods longer (5-6 days to be exact) I used to only have periods for 3-4 days max also made my cramps intense for the first two days of my period, I never had cramps before using birth control. Other than that in happy with the patch\"', 'rating': 8.0, 'date': 'November 3, 2015', 'usefulCount': 10}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = \"drugsComTrain_raw.tsv\"\n",
    "# \\t 는 Python에서 탭 문자를 나타냅니다.\n",
    "drug_dataset_streamed = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", split=\"train\", streaming=True)\n",
    "for i, data in enumerate(drug_dataset_streamed):\n",
    "    print(i, data)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43b884fc-7b3e-4484-90cc-b7e3eb9b43b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'patient_id': 89879, 'drugName': 'Cyclosporine', 'condition': 'keratoconjunctivitis sicca', 'review': '\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I\\'ve had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I\\'ve talked with my doctor about this and he said it is normal but should go away after some time, but it hasn\\'t. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I\\'ve been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I\\'m ready to move on.\"', 'rating': 2.0, 'date': 'April 20, 2013', 'usefulCount': 69, 'review_length': 147}\n",
      "1 {'patient_id': 143975, 'drugName': 'Etonogestrel', 'condition': 'birth control', 'review': '\"My experience has been somewhat mixed. I have been using Implanon now for nearly 14 months and have decided to get it removed because I bleed every day, all day. I would occasionally stain my underwear and my sheets. It didn\\'t start out that way, for the first month I didn\\'t bleed at all, then I had an epic two week period, then everything was irregular, which has basically been my new norm, sadly. \\r\\r\\n\\r\\r\\nI decided to get rid of the Implanon because of the endless bleeding. I should mention that the bleeding was usually pretty light, a bit more than spotting but not quite a period either. Other than the endless bleeding, I\\'ve been pretty side effect free, except for minor acne.\\r\\r\\n\\r\\r\\nI didn\\'t get pregnant on it, so yeah! But...blood, a lot of blood.\"', 'rating': 7.0, 'date': 'August 7, 2016', 'usefulCount': 4, 'review_length': 136}\n",
      "2 {'patient_id': 106473, 'drugName': 'Implanon', 'condition': 'birth control', 'review': '\"This is my second Implanon would not recommend at all....first one was okay for the first 2 years until I started bleeding which never stopped when I woke in the morning I would stand up and be absolutely covered in blood it was that bad.....I lost 2 stone in weight my mood swings were all over the place and it affected my relationship. I was advised to have that one removed and another one inserted which I did which I\\'m into the second year now and it\\'s starting all over again! The only good thing about this is the reliability that your not going to get pregnant IF there\\'s ever a chance that you can have a quick fumble in between periods!! It gets me so down I can be off in the morning and on again at night.....so frustrating\"', 'rating': 1.0, 'date': 'May 11, 2016', 'usefulCount': 6, 'review_length': 140}\n",
      "3 {'patient_id': 184526, 'drugName': 'Hydroxyzine', 'condition': 'anxiety', 'review': '\"I recommend taking as prescribed, and the bottle usually says \"take X amount every X hours\". I think that having a steady stream of any medication is the only way to have it work. This medication\\'s only side effect I have found is almost exactly like when one takes Benadryl, and only when you are first starting out on it. I am used to it now after 3 weeks and the only effect I notice is that my hands no longer shake and my mind is at ease. Much better and smoother than other meds like Xanax or Tramadol or Klonopin, etc., etc. \"', 'rating': 10.0, 'date': 'March 19, 2012', 'usefulCount': 124, 'review_length': 104}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = \"drug-reviews-train.jsonl\"\n",
    "# \\t 는 Python에서 탭 문자를 나타냅니다.\n",
    "drug_dataset_streamed = load_dataset(\"json\", data_files=data_files, split=\"train\", streaming=True)\n",
    "#drug_dataset_streamed = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "for i, data in enumerate(drug_dataset_streamed):\n",
    "    print(i, data)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58197666-8e0f-4139-9a99-b475b416daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'patient_id': 89879, 'drugName': 'Cyclosporine', 'condition': 'keratoconjunctivitis sicca', 'review': '\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I\\'ve had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I\\'ve talked with my doctor about this and he said it is normal but should go away after some time, but it hasn\\'t. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I\\'ve been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I\\'m ready to move on.\"', 'rating': 2.0, 'date': 'April 20, 2013', 'usefulCount': 69, 'review_length': 147}\n",
      "1 {'patient_id': 143975, 'drugName': 'Etonogestrel', 'condition': 'birth control', 'review': '\"My experience has been somewhat mixed. I have been using Implanon now for nearly 14 months and have decided to get it removed because I bleed every day, all day. I would occasionally stain my underwear and my sheets. It didn\\'t start out that way, for the first month I didn\\'t bleed at all, then I had an epic two week period, then everything was irregular, which has basically been my new norm, sadly. \\r\\r\\n\\r\\r\\nI decided to get rid of the Implanon because of the endless bleeding. I should mention that the bleeding was usually pretty light, a bit more than spotting but not quite a period either. Other than the endless bleeding, I\\'ve been pretty side effect free, except for minor acne.\\r\\r\\n\\r\\r\\nI didn\\'t get pregnant on it, so yeah! But...blood, a lot of blood.\"', 'rating': 7.0, 'date': 'August 7, 2016', 'usefulCount': 4, 'review_length': 136}\n",
      "2 {'patient_id': 106473, 'drugName': 'Implanon', 'condition': 'birth control', 'review': '\"This is my second Implanon would not recommend at all....first one was okay for the first 2 years until I started bleeding which never stopped when I woke in the morning I would stand up and be absolutely covered in blood it was that bad.....I lost 2 stone in weight my mood swings were all over the place and it affected my relationship. I was advised to have that one removed and another one inserted which I did which I\\'m into the second year now and it\\'s starting all over again! The only good thing about this is the reliability that your not going to get pregnant IF there\\'s ever a chance that you can have a quick fumble in between periods!! It gets me so down I can be off in the morning and on again at night.....so frustrating\"', 'rating': 1.0, 'date': 'May 11, 2016', 'usefulCount': 6, 'review_length': 140}\n",
      "3 {'patient_id': 184526, 'drugName': 'Hydroxyzine', 'condition': 'anxiety', 'review': '\"I recommend taking as prescribed, and the bottle usually says \"take X amount every X hours\". I think that having a steady stream of any medication is the only way to have it work. This medication\\'s only side effect I have found is almost exactly like when one takes Benadryl, and only when you are first starting out on it. I am used to it now after 3 weeks and the only effect I notice is that my hands no longer shake and my mind is at ease. Much better and smoother than other meds like Xanax or Tramadol or Klonopin, etc., etc. \"', 'rating': 10.0, 'date': 'March 19, 2012', 'usefulCount': 124, 'review_length': 104}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = \"drug-reviews-train.jsonl\"\n",
    "# \\t 는 Python에서 탭 문자를 나타냅니다.\n",
    "drug_dataset_streamed = load_dataset(\"json\", data_files=data_files, split=\"train\", streaming=True)\n",
    "for i, data in enumerate(drug_dataset_streamed):\n",
    "    print(i, data)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1072d043-f0ac-4428-9aac-8cfaa5beb435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00.jsonl.zst',\n",
       " '01.jsonl.zst',\n",
       " '02.jsonl.zst',\n",
       " '03.jsonl.zst',\n",
       " '04.jsonl.zst',\n",
       " '05.jsonl.zst',\n",
       " '06.jsonl.zst',\n",
       " '07.jsonl.zst',\n",
       " '08.jsonl.zst',\n",
       " '09.jsonl.zst',\n",
       " '10.jsonl.zst',\n",
       " '11.jsonl.zst',\n",
       " '12.jsonl.zst',\n",
       " '13.jsonl.zst',\n",
       " '14.jsonl.zst',\n",
       " '15.jsonl.zst',\n",
       " '16.jsonl.zst',\n",
       " '17.jsonl.zst',\n",
       " '18.jsonl.zst',\n",
       " '19.jsonl.zst',\n",
       " '20.jsonl.zst',\n",
       " '21.jsonl.zst',\n",
       " '22.jsonl.zst',\n",
       " '23.jsonl.zst',\n",
       " '24.jsonl.zst',\n",
       " '25.jsonl.zst',\n",
       " '26.jsonl.zst',\n",
       " '27.jsonl.zst',\n",
       " '28.jsonl.zst',\n",
       " '29.jsonl.zst']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{idx:02d}.jsonl.zst\" for idx in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ab9673d-ec76-4b73-a272-11d01bfe5682",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    n_shards: 1\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_dataset_streamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a45a9e19-916c-47f0-88ec-f7184cf8aad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    n_shards: 1\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset_streamed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f8ba7-a48b-4eb8-8734-6568c1ea2ab5",
   "metadata": {},
   "source": [
    "이 데이터셋은 대부분의 랩톱에서 RAM에 부담을 주기에 충분하지만 어려움 없이 로드하고 액세스할 수 있습니다! 이제 Drug Dataset 및 PubMed Abstracts 데이터셋의 예제를 interleave_datasets() 함수와 결합해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa90bc10-b160-4763-8c2d-a50f40dfd38e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'patient_id': 89879,\n",
       "  'drugName': 'Cyclosporine',\n",
       "  'condition': 'keratoconjunctivitis sicca',\n",
       "  'review': '\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I\\'ve had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I\\'ve talked with my doctor about this and he said it is normal but should go away after some time, but it hasn\\'t. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I\\'ve been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I\\'m ready to move on.\"',\n",
       "  'rating': 2.0,\n",
       "  'date': 'April 20, 2013',\n",
       "  'usefulCount': 69,\n",
       "  'review_length': 147,\n",
       "  'meta': None,\n",
       "  'text': None},\n",
       " {'patient_id': None,\n",
       "  'drugName': None,\n",
       "  'condition': None,\n",
       "  'review': None,\n",
       "  'rating': None,\n",
       "  'date': None,\n",
       "  'usefulCount': None,\n",
       "  'review_length': None,\n",
       "  'meta': {'language': 'eng', 'pmid': 1673585},\n",
       "  'text': 'Cardiac beta-adrenoceptor regulation and the effects of partial agonism.\\nThe in vivo effects of xamoterol on the regulation of rat cardiac beta adrenoceptors were investigated. Rats were implanted subcutaneously with osmotic minipumps and exposed to the following treatment regimens: (1) subcutaneous infusion of saline (control), isoprenaline or xamoterol for 6 days, (2) subcutaneous infusion of isoprenaline with co-administration of xamoterol for various periods up to 96 hours, and (3) subcutaneous infusion of xamoterol for up to 96 hours after previous treatment with isoprenaline for 72 hours. Xamoterol did not induce beta-adrenoceptor down-regulation after short-term (72-hour) or long-term (6-day) infusions. When coadministered with isoprenaline xamoterol did not affect the rate or extent of down-regulation induced by isoprenaline alone. In addition, recovery of beta adrenoceptors down-regulated by isoprenaline treatment was not influenced by xamoterol treatment. In all studies, doses of xamoterol were equivalent to those producing full functional responses to the drug in vivo.'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from datasets import interleave_datasets\n",
    "\n",
    "combined_dataset = interleave_datasets([drug_dataset_streamed, pubmed_dataset_streamed])\n",
    "list(islice(combined_dataset, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a19a624-4c7a-4e11-85a0-16f1f5e0113d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': 89879,\n",
       " 'drugName': 'Cyclosporine',\n",
       " 'condition': 'keratoconjunctivitis sicca',\n",
       " 'review': '\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I\\'ve had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I\\'ve talked with my doctor about this and he said it is normal but should go away after some time, but it hasn\\'t. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I\\'ve been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I\\'m ready to move on.\"',\n",
       " 'rating': 2.0,\n",
       " 'date': 'April 20, 2013',\n",
       " 'usefulCount': 69,\n",
       " 'review_length': 147,\n",
       " 'meta': None,\n",
       " 'text': None}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(combined_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1807802c-e7f2-42c3-b4bf-fc4ce96f0eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': 89879,\n",
       " 'drugName': 'Cyclosporine',\n",
       " 'condition': 'keratoconjunctivitis sicca',\n",
       " 'review': '\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I\\'ve had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I\\'ve talked with my doctor about this and he said it is normal but should go away after some time, but it hasn\\'t. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I\\'ve been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I\\'m ready to move on.\"',\n",
       " 'rating': 2.0,\n",
       " 'date': 'April 20, 2013',\n",
       " 'usefulCount': 69,\n",
       " 'review_length': 147,\n",
       " 'meta': None,\n",
       " 'text': None}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(combined_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25fcaf9d-d5c9-4d90-aa77-ba8010cb6a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.iterable_dataset.IterableDataset"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e3ba011-fd4f-4695-a75a-b979ee2553ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'patient_id': 89879, 'drugName': 'Cyclosporine', 'condition': 'keratoconjunctivitis sicca', 'review': '\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I\\'ve had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I\\'ve talked with my doctor about this and he said it is normal but should go away after some time, but it hasn\\'t. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I\\'ve been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I\\'m ready to move on.\"', 'rating': 2.0, 'date': 'April 20, 2013', 'usefulCount': 69, 'review_length': 147, 'meta': None, 'text': None}\n",
      "1 {'patient_id': None, 'drugName': None, 'condition': None, 'review': None, 'rating': None, 'date': None, 'usefulCount': None, 'review_length': None, 'meta': {'language': 'eng', 'pmid': 1673585}, 'text': 'Cardiac beta-adrenoceptor regulation and the effects of partial agonism.\\nThe in vivo effects of xamoterol on the regulation of rat cardiac beta adrenoceptors were investigated. Rats were implanted subcutaneously with osmotic minipumps and exposed to the following treatment regimens: (1) subcutaneous infusion of saline (control), isoprenaline or xamoterol for 6 days, (2) subcutaneous infusion of isoprenaline with co-administration of xamoterol for various periods up to 96 hours, and (3) subcutaneous infusion of xamoterol for up to 96 hours after previous treatment with isoprenaline for 72 hours. Xamoterol did not induce beta-adrenoceptor down-regulation after short-term (72-hour) or long-term (6-day) infusions. When coadministered with isoprenaline xamoterol did not affect the rate or extent of down-regulation induced by isoprenaline alone. In addition, recovery of beta adrenoceptors down-regulated by isoprenaline treatment was not influenced by xamoterol treatment. In all studies, doses of xamoterol were equivalent to those producing full functional responses to the drug in vivo.'}\n",
      "2 {'patient_id': 143975, 'drugName': 'Etonogestrel', 'condition': 'birth control', 'review': '\"My experience has been somewhat mixed. I have been using Implanon now for nearly 14 months and have decided to get it removed because I bleed every day, all day. I would occasionally stain my underwear and my sheets. It didn\\'t start out that way, for the first month I didn\\'t bleed at all, then I had an epic two week period, then everything was irregular, which has basically been my new norm, sadly. \\r\\r\\n\\r\\r\\nI decided to get rid of the Implanon because of the endless bleeding. I should mention that the bleeding was usually pretty light, a bit more than spotting but not quite a period either. Other than the endless bleeding, I\\'ve been pretty side effect free, except for minor acne.\\r\\r\\n\\r\\r\\nI didn\\'t get pregnant on it, so yeah! But...blood, a lot of blood.\"', 'rating': 7.0, 'date': 'August 7, 2016', 'usefulCount': 4, 'review_length': 136, 'meta': None, 'text': None}\n",
      "3 {'patient_id': None, 'drugName': None, 'condition': None, 'review': None, 'rating': None, 'date': None, 'usefulCount': None, 'review_length': None, 'meta': {'language': 'eng', 'pmid': 1673586}, 'text': 'Interrupting the adaptive changes in congestive heart failure.\\nCirculating plasma concentrations of norepinephrine, renin, angiotensin and vasopressin are increased in congestive heart failure. By increasing ventricular afterload, heart failure is further worsened, which in turn--in a vicious cycle--stimulates neurohumoral vasoconstrictor mechanisms. Furthermore, because of the compensatory but excessive stimulation of the sympathomimetic system, a down-regulation and desensitization particularly of the myocardial beta 1 receptors and depletion of myocardial catecholamine occurs in chronic heart failure. These defects may be restored toward normal by interventions that attenuate the activity of the sympathetic nervous system. A direct approach to modify the excessive vasoconstriction is to administer systemic vasodilator drugs, but despite favorable short-term effects, tolerance developed to most of these drugs during long-term treatment. One reason for the loss of effectiveness is the reflex activation of the sympathetic system, which increases vasoconstrictor hormone concentrations. Activation of the renin-angiotensin system can be modified effectively by angiotensin-converting enzyme inhibitors that have shown favorable responses in patients with chronic heart failure. Beta-blocking agents interfere with endogenous sympathetic activation and have produced beneficial effects in patients with congestive cardiomyopathy. Long-term treatment is associated with up-regulation of the number of beta receptors and an improved responsiveness to catecholamines. Owing to the negative inotropic effects of beta-blocking agents, some of the patients with severe heart failure deteriorated hemodynamically and clinically. Theoretically, it should be advantageous to have a substance that combines protection against excessive beta stimulation with a mild inotropic support to prevent cardiac decompensation. This may be achieved by a selective beta 1-partial agonist like xamoterol.'}\n",
      "4 {'patient_id': 106473, 'drugName': 'Implanon', 'condition': 'birth control', 'review': '\"This is my second Implanon would not recommend at all....first one was okay for the first 2 years until I started bleeding which never stopped when I woke in the morning I would stand up and be absolutely covered in blood it was that bad.....I lost 2 stone in weight my mood swings were all over the place and it affected my relationship. I was advised to have that one removed and another one inserted which I did which I\\'m into the second year now and it\\'s starting all over again! The only good thing about this is the reliability that your not going to get pregnant IF there\\'s ever a chance that you can have a quick fumble in between periods!! It gets me so down I can be off in the morning and on again at night.....so frustrating\"', 'rating': 1.0, 'date': 'May 11, 2016', 'usefulCount': 6, 'review_length': 140, 'meta': None, 'text': None}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(combined_dataset):\n",
    "    print(i, data)\n",
    "    if i > 3: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c087c37-985e-4184-8e95-d06f2a61deb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'patient_id': 89879,\n",
       "  'drugName': 'Cyclosporine',\n",
       "  'condition': 'keratoconjunctivitis sicca',\n",
       "  'review': '\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I\\'ve had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I\\'ve talked with my doctor about this and he said it is normal but should go away after some time, but it hasn\\'t. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I\\'ve been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I\\'m ready to move on.\"',\n",
       "  'rating': 2.0,\n",
       "  'date': 'April 20, 2013',\n",
       "  'usefulCount': 69,\n",
       "  'review_length': 147,\n",
       "  'meta': None,\n",
       "  'text': None}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(combined_dataset, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc3282e2-d794-44e5-8d45-8635d7a756fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'patient_id': 89879,\n",
       "  'drugName': 'Cyclosporine',\n",
       "  'condition': 'keratoconjunctivitis sicca',\n",
       "  'review': '\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I\\'ve had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I\\'ve talked with my doctor about this and he said it is normal but should go away after some time, but it hasn\\'t. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I\\'ve been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I\\'m ready to move on.\"',\n",
       "  'rating': 2.0,\n",
       "  'date': 'April 20, 2013',\n",
       "  'usefulCount': 69,\n",
       "  'review_length': 147,\n",
       "  'meta': None,\n",
       "  'text': None},\n",
       " {'patient_id': None,\n",
       "  'drugName': None,\n",
       "  'condition': None,\n",
       "  'review': None,\n",
       "  'rating': None,\n",
       "  'date': None,\n",
       "  'usefulCount': None,\n",
       "  'review_length': None,\n",
       "  'meta': {'language': 'eng', 'pmid': 1673585},\n",
       "  'text': 'Cardiac beta-adrenoceptor regulation and the effects of partial agonism.\\nThe in vivo effects of xamoterol on the regulation of rat cardiac beta adrenoceptors were investigated. Rats were implanted subcutaneously with osmotic minipumps and exposed to the following treatment regimens: (1) subcutaneous infusion of saline (control), isoprenaline or xamoterol for 6 days, (2) subcutaneous infusion of isoprenaline with co-administration of xamoterol for various periods up to 96 hours, and (3) subcutaneous infusion of xamoterol for up to 96 hours after previous treatment with isoprenaline for 72 hours. Xamoterol did not induce beta-adrenoceptor down-regulation after short-term (72-hour) or long-term (6-day) infusions. When coadministered with isoprenaline xamoterol did not affect the rate or extent of down-regulation induced by isoprenaline alone. In addition, recovery of beta adrenoceptors down-regulated by isoprenaline treatment was not influenced by xamoterol treatment. In all studies, doses of xamoterol were equivalent to those producing full functional responses to the drug in vivo.'},\n",
       " {'patient_id': 143975,\n",
       "  'drugName': 'Etonogestrel',\n",
       "  'condition': 'birth control',\n",
       "  'review': '\"My experience has been somewhat mixed. I have been using Implanon now for nearly 14 months and have decided to get it removed because I bleed every day, all day. I would occasionally stain my underwear and my sheets. It didn\\'t start out that way, for the first month I didn\\'t bleed at all, then I had an epic two week period, then everything was irregular, which has basically been my new norm, sadly. \\r\\r\\n\\r\\r\\nI decided to get rid of the Implanon because of the endless bleeding. I should mention that the bleeding was usually pretty light, a bit more than spotting but not quite a period either. Other than the endless bleeding, I\\'ve been pretty side effect free, except for minor acne.\\r\\r\\n\\r\\r\\nI didn\\'t get pregnant on it, so yeah! But...blood, a lot of blood.\"',\n",
       "  'rating': 7.0,\n",
       "  'date': 'August 7, 2016',\n",
       "  'usefulCount': 4,\n",
       "  'review_length': 136,\n",
       "  'meta': None,\n",
       "  'text': None}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(combined_dataset, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8a05f-3d5f-46ed-b293-2c1c341094ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
    "data_files = {\n",
    "    \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(30)],\n",
    "    \"validation\": base_url + \"val.jsonl.zst\",\n",
    "    \"test\": base_url + \"test.jsonl.zst\",\n",
    "}\n",
    "pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "next(iter(pile_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ac46dc6e-f766-4d3b-8551-05630d57c556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39ac3bc59cd46109f1b44d6c82f0931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 184622\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
    "data_files = {\n",
    "    \"train\": \"*.jsonl\"\n",
    "}\n",
    "#pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "pile_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "pile_dataset\n",
    "#next(iter(pile_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9f968d65-834f-4221-b211-ac4e48846e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': 163740,\n",
       " 'drugName': 'Mirtazapine',\n",
       " 'condition': 'depression',\n",
       " 'review': '\"I\\'ve tried a few antidepressants over the years (citalopram, fluoxetine, amitriptyline), but none of those helped with my depression, insomnia & anxiety. My doctor suggested and changed me onto 45mg mirtazapine and this medicine has saved my life. Thankfully I have had no side effects especially the most common - weight gain, I\\'ve actually lost alot of weight. I still have suicidal thoughts but mirtazapine has saved me.\"',\n",
       " 'rating': 10.0,\n",
       " 'date': 'February 28, 2012',\n",
       " 'usefulCount': 22,\n",
       " 'review_length': 68}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
    "data_files = {\n",
    "    \"train\": \"*.jsonl\"\n",
    "}\n",
    "pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "#pile_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "#pile_dataset\n",
    "next(iter(pile_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5ceb4107-72a4-4adf-a247-e941ee93959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'patient_id': 163740, 'drugName': 'Mirtazapine', 'condition': 'depression', 'review': '\"I\\'ve tried a few antidepressants over the years (citalopram, fluoxetine, amitriptyline), but none of those helped with my depression, insomnia & anxiety. My doctor suggested and changed me onto 45mg mirtazapine and this medicine has saved my life. Thankfully I have had no side effects especially the most common - weight gain, I\\'ve actually lost alot of weight. I still have suicidal thoughts but mirtazapine has saved me.\"', 'rating': 10.0, 'date': 'February 28, 2012', 'usefulCount': 22, 'review_length': 68}\n",
      "0 {'meta': {'pmid': 1673585, 'language': 'eng'}, 'text': 'Cardiac beta-adrenoceptor regulation and the effects of partial agonism.\\nThe in vivo effects of xamoterol on the regulation of rat cardiac beta adrenoceptors were investigated. Rats were implanted subcutaneously with osmotic minipumps and exposed to the following treatment regimens: (1) subcutaneous infusion of saline (control), isoprenaline or xamoterol for 6 days, (2) subcutaneous infusion of isoprenaline with co-administration of xamoterol for various periods up to 96 hours, and (3) subcutaneous infusion of xamoterol for up to 96 hours after previous treatment with isoprenaline for 72 hours. Xamoterol did not induce beta-adrenoceptor down-regulation after short-term (72-hour) or long-term (6-day) infusions. When coadministered with isoprenaline xamoterol did not affect the rate or extent of down-regulation induced by isoprenaline alone. In addition, recovery of beta adrenoceptors down-regulated by isoprenaline treatment was not influenced by xamoterol treatment. In all studies, doses of xamoterol were equivalent to those producing full functional responses to the drug in vivo.'}\n",
      "1 {'meta': {'pmid': 1673586, 'language': 'eng'}, 'text': 'Interrupting the adaptive changes in congestive heart failure.\\nCirculating plasma concentrations of norepinephrine, renin, angiotensin and vasopressin are increased in congestive heart failure. By increasing ventricular afterload, heart failure is further worsened, which in turn--in a vicious cycle--stimulates neurohumoral vasoconstrictor mechanisms. Furthermore, because of the compensatory but excessive stimulation of the sympathomimetic system, a down-regulation and desensitization particularly of the myocardial beta 1 receptors and depletion of myocardial catecholamine occurs in chronic heart failure. These defects may be restored toward normal by interventions that attenuate the activity of the sympathetic nervous system. A direct approach to modify the excessive vasoconstriction is to administer systemic vasodilator drugs, but despite favorable short-term effects, tolerance developed to most of these drugs during long-term treatment. One reason for the loss of effectiveness is the reflex activation of the sympathetic system, which increases vasoconstrictor hormone concentrations. Activation of the renin-angiotensin system can be modified effectively by angiotensin-converting enzyme inhibitors that have shown favorable responses in patients with chronic heart failure. Beta-blocking agents interfere with endogenous sympathetic activation and have produced beneficial effects in patients with congestive cardiomyopathy. Long-term treatment is associated with up-regulation of the number of beta receptors and an improved responsiveness to catecholamines. Owing to the negative inotropic effects of beta-blocking agents, some of the patients with severe heart failure deteriorated hemodynamically and clinically. Theoretically, it should be advantageous to have a substance that combines protection against excessive beta stimulation with a mild inotropic support to prevent cardiac decompensation. This may be achieved by a selective beta 1-partial agonist like xamoterol.'}\n",
      "2 {'meta': {'pmid': 1673587, 'language': 'eng'}, 'text': 'Acute and chronic hemodynamic effects of xamoterol in mild to moderate congestive heart failure.\\nXamoterol, a new beta 1 partial agonist, has the potential to modulate cardiac response to variations in sympathetic tone in patients with heart failure. Its properties should result in beta-receptor stimulatory effects at low levels of sympathetic tone and beta-receptor protective effects at higher levels of sympathetic tone. The acute effects of intravenous (i.v.) xamoterol on hemodynamics at rest and during exercise were studied in 30 patients with mild to moderate heart failure (13 patients in New York Heart Association class II; 17 in class III) due to ischemic (n = 24) or cardiomyopathic (n = 6) heart disease. Cardiac index, stroke volume and stroke work index at rest were significantly improved after i.v. administration of xamoterol and consistent with net agonist effects. During exercise, heart rate and double product were significantly reduced (net antagonist effects), but with preservation of the expected increases in cardiac index and systolic blood pressure. These hemodynamic findings confirm the ability of xamoterol to modulate cardiac response to variations in sympathetic tone. Tachyphylaxis and arrhythmogenicity limit the chronic use of drugs with full beta-agonist properties as positive inotropes in heart failure. The patients were therefore entered into a 6-month double-blind, placebo-controlled, crossover study of chronic oral xamoterol therapy, 200 mg twice daily, and the hemodynamic responses to i.v. xamoterol were repeated at the end of the trial. No impairment in either resting or exercise effects was observed, indicative of a maintained response and absence of tachyphylaxis after chronic therapy. Furthermore, 24-hour ambulatory electrocardiographic monitoring showed no change in ventricular arrhythmias during oral treatment.(ABSTRACT TRUNCATED AT 250 WORDS)'}\n",
      "3 {'meta': {'pmid': 1673588, 'language': 'eng'}, 'text': 'Local cardiac responses--alternative methods of control.\\nMuch attention has been paid to the influence of the beta-adrenoceptor system on cardiac function in heart failure. Full agonists and partial agonists acting on cardiac beta 1 receptors have been widely investigated, as has the density of these receptors in the failing heart. However, other cardiac control mechanisms may play important roles in the normal heart as well as in heart failure. The Frank-Starling mechanism of enhanced cardiac contraction produced by mechanical stretching of the ventricular myofibrils is well known. When treating patients with heart failure with diuretics, vasodilators and other drugs that influence preload, it is important to consider their overall effects in relation to the Starling curves. Atrial stretching also produces compensatory responses which are currently being intensively studied. Reflex release of atrial natriuretic factor after stimulation of atrial receptors has important physiologic effects in heart failure. The atria, but not the ventricles, are innervated by the vagus; the influence of the parasympathetic nervous system on the heart and circulation is often overlooked. The initial increase in heart rate during exercise is primarily due to withdrawal of vagal influence. Besides acetylcholine, the parasympathetic transmitter, many other local hormones may affect cardiac function; these include prostaglandins, 5-hydroxytryptamine and histamine. Although the activity of the sympathetic nervous system is mediated primarily through beta 1 adrenoceptors, both beta 2 and alpha receptors are also found in the heart. Myocardial alpha 1 receptors, which mediate a positive inotropic effect, have been identified, and prejunctional alpha 2 receptors may mediate inhibition of norepinephrine release from sympathetic nerves.(ABSTRACT TRUNCATED AT 250 WORDS)'}\n"
     ]
    }
   ],
   "source": [
    "#base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
    "data_files = {\n",
    "    \"train\": \"PUBMED_title_abstracts_2020_baseline/*.jsonl\",\n",
    "    \"test\": \"drug-reviews-test.jsonl\"\n",
    "}\n",
    "pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "#pile_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "#pile_dataset\n",
    "print(next(iter(pile_dataset[\"test\"])))\n",
    "for i, data in enumerate(pile_dataset[\"train\"]):\n",
    "    print(i, data)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90592400-bc35-41bd-92b9-db6f982f3ad1",
   "metadata": {},
   "source": [
    "여기에서 우리는 Python의 itertools 모듈에서 islice() 함수를 사용하여 결합된 데이터셋에서 처음 두 예제를 선택했으며 두 소스 데이터셋 각각의 첫 번째 예제와 일치하는 것을 볼 수 있습니다.\n",
    "\n",
    "마지막으로 Pile을 825GB 전체로 스트리밍하려면 다음과 같이 준비된 모든 파일을 가져올 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2e7ee6a8-d1e6-455f-b5f6-f0e92ac8e048",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://the-eye.eu/public/AI/pile/train/00.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/01.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/02.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/03.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/04.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/05.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/06.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/07.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/08.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/09.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/10.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/11.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/12.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/13.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/14.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/15.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/16.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/17.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/18.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/19.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/20.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/21.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/22.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/23.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/24.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/25.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/26.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/27.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/28.jsonl.zst', 'https://the-eye.eu/public/AI/pile/train/29.jsonl.zst']\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
    "#데이터셋은 JSON Lines 형식이고 zstandard 형식으로 압축\n",
    "train_file_list = [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(30)]\n",
    "print(train_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d4181555-dc0f-4674-94a9-0c7e03cfb024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find 'https://the-eye.eu/public/AI/pile/train/00.jsonl.zst'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://the-eye.eu/public/AI/pile/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m data_files \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: [base_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jsonl.zst\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m)],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: base_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval.jsonl.zst\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: base_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.jsonl.zst\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 7\u001b[0m pile_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(pile_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:2519\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2514\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2515\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2516\u001b[0m )\n\u001b[1;32m   2518\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2519\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2536\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:2192\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2190\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   2191\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 2192\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   2204\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:1735\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, **download_kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1726\u001b[0m \n\u001b[1;32m   1727\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[1;32m   1729\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:1120\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m   1119\u001b[0m patterns \u001b[38;5;241m=\u001b[39m sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path)\n\u001b[0;32m-> 1120\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m supports_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m _MODULE_SUPPORTS_METADATA\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m supports_metadata \u001b[38;5;129;01mand\u001b[39;00m patterns \u001b[38;5;241m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/data_files.py:689\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    686\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    688\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 689\u001b[0m         \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m patterns_for_key\n\u001b[1;32m    697\u001b[0m     )\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/data_files.py:594\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 594\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m         )\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/data_files.py:383\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find 'https://the-eye.eu/public/AI/pile/train/00.jsonl.zst'"
     ]
    }
   ],
   "source": [
    "base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
    "data_files = {\n",
    "    \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(30)],\n",
    "    \"validation\": base_url + \"val.jsonl.zst\",\n",
    "    \"test\": base_url + \"test.jsonl.zst\",\n",
    "}\n",
    "pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "next(iter(pile_dataset[\"train\"]))\n",
    "\n",
    "# https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt\n",
    "# {'meta': {'pile_set_name': 'Pile-CC'},\n",
    "# 'text': 'It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web...'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d494cd-84d9-44b4-aae5-9e7c47d539cc",
   "metadata": {},
   "source": [
    "✏️ Try it out! Use one of the large Common Crawl corpora like [mc4](https://huggingface.co/datasets/mc4) or [oscar](https://huggingface.co/datasets/oscar) to create a streaming multilingual dataset that represents the spoken proportions of languages in a country of your choice. For example, the four national languages in Switzerland are German, French, Italian, and Romansh, so you could try creating a Swiss corpus by sampling the Oscar subsets according to their spoken proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fa39c-f155-4c6d-939a-d87d5d34f2fa",
   "metadata": {},
   "source": [
    "### 4. 자신만의 데이터셋 만들기\n",
    "NLP 애플리케이션을 구축하는데 필요한 데이터셋이 존재하지 않는 경우에는 직접 만들어야 합니다. 이 섹션에서는 GitHub 리포지토리의 버그 또는 특징을 추적하는데 일반적으로 사용되는 GitHub issues 말뭉치를 만드는 방법을 보여줍니다. 이 말뭉치는 다음과 같은 다양한 목적으로 사용될 수 있습니다:\n",
    "\n",
    "미해결 이슈(Open issues) 또는 풀 리퀘스트(pull requests)를 종료하는데 얼마나 시간이 걸리는지 알아보기.\n",
    "\n",
    "문제에 대한 설명(issue's description)에 기반하여 메타데이터 형태의 이슈에 태그(\"bug\", \"enhancement\", 또는 \"question\")를 지정할 수 있는 다중 레이블 분류기(multilabel classifier) 학습\n",
    "\n",
    "사용자의 질의와 일치하는 이슈들을 검색하기 위한 시맨틱 검색 엔진(semantic search engine) 만들기\n",
    "\n",
    "여기에서는 말뭉치의 신규 구축에 초점을 맞추고 다음 섹션에서는 시맨틱 검색 엔진을 다룰 것입니다. 특히, 인기있는 오픈 소스 프로젝트인 🤗Datasets와 관련된 GitHub 이슈 집합을 사용해서 말뭉치를 구축합니다. 해당 데이터를 구하는 방법과 이러한 이슈들에 포함된 정보를 탐색하는 방법을 살펴보겠습니다.\n",
    "\n",
    "#### 데이터 가져오기\n",
    "---\n",
    "Repository의 [이슈 탭(Issues tab)](https://github.com/huggingface/datasets/issues)으로 이동하여 🤗Datasets 프로젝트의 모든 이슈들을 찾을 수 있습니다. 다음 스크린샷에서 볼 수 있듯이 이 문서를 작성할 당시 331개의 미해결 이슈들(open issues)과 668개의 해결된 이슈들(closed issues)이 있었습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fe144050-8de9-414d-9309-46f11b456170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#wiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "#len(wiki_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4a350-8667-42d0-9207-ff12ffa7c30f",
   "metadata": {},
   "source": [
    "모든 리포지토리의 이슈들을 다운로드하기 위해 [GitHub REST API](https://docs.github.com/en/rest?apiVersion=2022-11-28)를 사용하여 [Issues endpoint](https://docs.github.com/en/rest/issues?apiVersion=2022-11-28#list-repository-issues)를 폴링합니다. 이 endpoint는 제목과 설명은 물론 이슈 상태에 대한 메타데이터를 포함하는 많은 필드를 포함하는 개체들로 구성된 JSON 개체(objects) 목록을 반환합니다.\n",
    "\n",
    "이슈(issues)를 다운로드하는 한가지 편리한 방법은 Python에서 HTTP 요청을 만드는 표준 방법인 requests 라이브러리를 사용하는 것입니다. 다음을 실행하여 라이브러리를 설치할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "693ce874-3056-42b0-aeea-704b39cdbf82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: requests in /scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages (from requests) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ecc3d-781d-45cd-a376-ae16a78414f6",
   "metadata": {},
   "source": [
    "라이브러리가 설치되면 requests.get() 함수를 호출하여 이슈 엔드포인트(Issues endpoint)에 GET 요청을 할 수 있습니다. 예를 들어, 다음 명령을 실행하여 첫 페이지의 첫 번째 이슈를 검색할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0684d90a-d7c9-46f8-9cc6-9f5516c78500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=2\"\n",
    "response = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39150d-01ac-4b02-81ef-99e3660f9d99",
   "metadata": {
    "tags": []
   },
   "source": [
    "응답 객체에는 HTTP 상태 코드를 포함하여 요청에 대한 유용한 정보가 많이 포함되어 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a63fe0f-8eb9-4446-b674-212e21c24b86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd263b-2670-47aa-a686-ed184f050c0d",
   "metadata": {},
   "source": [
    "여기서 200 상태는 요청이 성공했음을 의미합니다(여기에서 HTTP 상태 코드 목록을 찾을 수 있음). 하지만 우리가 관심있는 것은 바이트, 문자열 또는 JSON과 같은 다양한 형식으로 액세스할 수 있는 payload 입니다. 이슈가 JSON 형식이라는 것을 알고 있으므로 다음과 같이 페이로드(payload)를 살펴보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25612480-7057-47e8-b043-0fb046439692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/7117',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7117/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7117/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7117/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/issues/7117',\n",
       "  'id': 2476555659,\n",
       "  'node_id': 'I_kwDODunzps6TnT2L',\n",
       "  'number': 7117,\n",
       "  'title': 'Audio dataset load everything in RAM and is very slow',\n",
       "  'user': {'login': 'Jourdelune',\n",
       "   'id': 64205064,\n",
       "   'node_id': 'MDQ6VXNlcjY0MjA1MDY0',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/64205064?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/Jourdelune',\n",
       "   'html_url': 'https://github.com/Jourdelune',\n",
       "   'followers_url': 'https://api.github.com/users/Jourdelune/followers',\n",
       "   'following_url': 'https://api.github.com/users/Jourdelune/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/Jourdelune/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/Jourdelune/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/Jourdelune/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/Jourdelune/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/Jourdelune/repos',\n",
       "   'events_url': 'https://api.github.com/users/Jourdelune/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/Jourdelune/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 0,\n",
       "  'created_at': '2024-08-20T21:18:12Z',\n",
       "  'updated_at': '2024-08-20T21:32:23Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'NONE',\n",
       "  'active_lock_reason': None,\n",
       "  'body': 'Hello, I\\'m working with an audio dataset. I want to transcribe the audio that the dataset contain, and for that I use whisper. My issue is that the dataset load everything in the RAM when I map the dataset, obviously, when RAM usage is too high, the program crashes.\\r\\n\\r\\nTo fix this issue, I\\'m using writer_batch_size that I set to 10, but in this case, the mapping of the dataset is extremely slow.\\r\\nTo illustrate this, on 50 examples, with `writer_batch_size` set to 10, it takes 123.24 seconds to process the dataset, but without `writer_batch_size` set to 10, it takes about ten seconds to process the dataset, but then the process remains blocked (I assume that it is writing the dataset and therefore suffers from the same problem as `writer_batch_size`)\\r\\n\\r\\n### Steps to reproduce the bug\\r\\n\\r\\nHug ram usage but fast (but actually slow when saving the dataset):\\r\\n```py\\r\\nfrom datasets import load_dataset\\r\\nimport time\\r\\n\\r\\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\\r\\n\\r\\n\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    row[\"transcribed\"] = True\\r\\n    return row\\r\\n\\r\\n\\r\\ntime1 = time.time()\\r\\nds = ds.map(\\r\\n    transcribe_audio\\r\\n) \\r\\n\\r\\nfor row in ds:\\r\\n    pass  # do nothing, just iterate to trigger the map function\\r\\n\\r\\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\\r\\n```\\r\\n\\r\\nLow ram usage but very very slow:\\r\\n\\r\\n```py\\r\\nfrom datasets import load_dataset\\r\\nimport time\\r\\n\\r\\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\\r\\n\\r\\n\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    row[\"transcribed\"] = True\\r\\n    return row\\r\\n\\r\\n\\r\\ntime1 = time.time()\\r\\nds = ds.map(\\r\\n    transcribe_audio, writer_batch_size=10\\r\\n)  # set low writer_batch_size to avoid memory issues\\r\\n\\r\\nfor row in ds:\\r\\n    pass  # do nothing, just iterate to trigger the map function\\r\\n\\r\\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\\r\\n```\\r\\n\\r\\n### Expected behavior\\r\\n\\r\\nI think the processing should be much faster, on only 50 audio examples, the mapping takes several minutes while nothing is done (just loading the audio).\\r\\n\\r\\n### Environment info\\r\\n\\r\\n- `datasets` version: 2.21.0\\r\\n- Platform: Linux-6.10.5-arch1-1-x86_64-with-glibc2.40\\r\\n- Python version: 3.10.4\\r\\n- `huggingface_hub` version: 0.24.5\\r\\n- PyArrow version: 17.0.0\\r\\n- Pandas version: 1.5.3\\r\\n- `fsspec` version: 2024.6.1\\r\\n\\r\\n# Extra\\r\\n\\r\\nThe dataset has been generated by using audio folder, so I don\\'t think anything specific in my code is causing this problem.\\r\\n```py\\r\\nimport argparse\\r\\n\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nparser = argparse.ArgumentParser()\\r\\nparser.add_argument(\"--folder\", help=\"folder path\", default=\"/media/works/test/\")\\r\\nargs = parser.parse_args()\\r\\n\\r\\ndataset = load_dataset(\"audiofolder\", data_dir=args.folder)\\r\\n\\r\\n# push the dataset to hub\\r\\ndataset.push_to_hub(\"WaveGenAI/audios\")\\r\\n```\\r\\n\\r\\nAlso, it\\'s the combination of `audio = row[\"audio\"]` and `row[\"transcribed\"] = True` which causes problems, `row[\"transcribed\"] = True `alone does nothing and `audio = row[\"audio\"]` alone sometimes causes problems, sometimes not.',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7117/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7117/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7116',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7116/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7116/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7116/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/issues/7116',\n",
       "  'id': 2475522721,\n",
       "  'node_id': 'I_kwDODunzps6TjXqh',\n",
       "  'number': 7116,\n",
       "  'title': 'datasets cannot handle nested json if features is given.',\n",
       "  'user': {'login': 'ljw20180420',\n",
       "   'id': 38550511,\n",
       "   'node_id': 'MDQ6VXNlcjM4NTUwNTEx',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/38550511?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/ljw20180420',\n",
       "   'html_url': 'https://github.com/ljw20180420',\n",
       "   'followers_url': 'https://api.github.com/users/ljw20180420/followers',\n",
       "   'following_url': 'https://api.github.com/users/ljw20180420/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/ljw20180420/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/ljw20180420/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/ljw20180420/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/ljw20180420/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/ljw20180420/repos',\n",
       "   'events_url': 'https://api.github.com/users/ljw20180420/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/ljw20180420/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 0,\n",
       "  'created_at': '2024-08-20T12:27:49Z',\n",
       "  'updated_at': '2024-08-20T12:27:49Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'NONE',\n",
       "  'active_lock_reason': None,\n",
       "  'body': '### Describe the bug\\n\\nI have a json named temp.json.\\r\\n```json\\r\\n{\"ref1\": \"ABC\", \"ref2\": \"DEF\", \"cuts\":[{\"cut1\": 3, \"cut2\": 5}]}\\r\\n```\\r\\nI want to load it.\\r\\n```python\\r\\nds = datasets.load_dataset(\\'json\\', data_files=\"./temp.json\", features=datasets.Features({\\r\\n    \\'ref1\\': datasets.Value(\\'string\\'),\\r\\n    \\'ref2\\': datasets.Value(\\'string\\'),\\r\\n    \\'cuts\\': datasets.Sequence({\\r\\n        \"cut1\": datasets.Value(\"uint16\"),\\r\\n        \"cut2\": datasets.Value(\"uint16\")\\r\\n    })\\r\\n}))\\r\\n```\\r\\nThe above code does not work. However, I can load it without giving features.\\r\\n```python\\r\\nds = datasets.load_dataset(\\'json\\', data_files=\"./temp.json\")\\r\\n```\\r\\nIs it possible to load integers as uint16 to save some memory?\\n\\n### Steps to reproduce the bug\\n\\nAs in the bug description.\\n\\n### Expected behavior\\n\\nThe data are loaded and integers are uint16.\\n\\n### Environment info\\n\\nCopy-and-paste the text below in your GitHub issue.\\r\\n\\r\\n- `datasets` version: 2.21.0\\r\\n- Platform: Linux-5.15.0-118-generic-x86_64-with-glibc2.35\\r\\n- Python version: 3.11.9\\r\\n- `huggingface_hub` version: 0.24.5\\r\\n- PyArrow version: 17.0.0\\r\\n- Pandas version: 2.2.2\\r\\n- `fsspec` version: 2024.5.0',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7116/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7116/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef4489d8-ea6b-4cbb-9d24-8793e96b8604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/7119',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/7119',\n",
       "  'id': 2477766493,\n",
       "  'node_id': 'PR_kwDODunzps54-GjY',\n",
       "  'number': 7119,\n",
       "  'title': 'Install transformers with numpy-2 CI',\n",
       "  'user': {'login': 'albertvillanova',\n",
       "   'id': 8515462,\n",
       "   'node_id': 'MDQ6VXNlcjg1MTU0NjI=',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/albertvillanova',\n",
       "   'html_url': 'https://github.com/albertvillanova',\n",
       "   'followers_url': 'https://api.github.com/users/albertvillanova/followers',\n",
       "   'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/albertvillanova/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/albertvillanova/repos',\n",
       "   'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/albertvillanova/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'closed',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 2,\n",
       "  'created_at': '2024-08-21T11:14:59Z',\n",
       "  'updated_at': '2024-08-21T11:42:35Z',\n",
       "  'closed_at': '2024-08-21T11:36:50Z',\n",
       "  'author_association': 'MEMBER',\n",
       "  'active_lock_reason': None,\n",
       "  'draft': False,\n",
       "  'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/7119',\n",
       "   'html_url': 'https://github.com/huggingface/datasets/pull/7119',\n",
       "   'diff_url': 'https://github.com/huggingface/datasets/pull/7119.diff',\n",
       "   'patch_url': 'https://github.com/huggingface/datasets/pull/7119.patch',\n",
       "   'merged_at': '2024-08-21T11:36:50Z'},\n",
       "  'body': 'Install transformers with numpy-2 CI.\\r\\n\\r\\nNote that transformers no longer pins numpy < 2 since transformers-4.43.0:\\r\\n- https://github.com/huggingface/transformers/pull/32018\\r\\n- https://github.com/huggingface/transformers/releases/tag/v4.43.0',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7118',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7118/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7118/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7118/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/7118',\n",
       "  'id': 2477676893,\n",
       "  'node_id': 'PR_kwDODunzps549yu4',\n",
       "  'number': 7118,\n",
       "  'title': 'Allow numpy-2.1 and test it without audio extra',\n",
       "  'user': {'login': 'albertvillanova',\n",
       "   'id': 8515462,\n",
       "   'node_id': 'MDQ6VXNlcjg1MTU0NjI=',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/albertvillanova',\n",
       "   'html_url': 'https://github.com/albertvillanova',\n",
       "   'followers_url': 'https://api.github.com/users/albertvillanova/followers',\n",
       "   'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/albertvillanova/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/albertvillanova/repos',\n",
       "   'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/albertvillanova/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'closed',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 2,\n",
       "  'created_at': '2024-08-21T10:29:35Z',\n",
       "  'updated_at': '2024-08-21T11:05:03Z',\n",
       "  'closed_at': '2024-08-21T10:58:15Z',\n",
       "  'author_association': 'MEMBER',\n",
       "  'active_lock_reason': None,\n",
       "  'draft': False,\n",
       "  'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/7118',\n",
       "   'html_url': 'https://github.com/huggingface/datasets/pull/7118',\n",
       "   'diff_url': 'https://github.com/huggingface/datasets/pull/7118.diff',\n",
       "   'patch_url': 'https://github.com/huggingface/datasets/pull/7118.patch',\n",
       "   'merged_at': '2024-08-21T10:58:15Z'},\n",
       "  'body': 'Allow numpy-2.1 and test it without audio extra.\\r\\n\\r\\nThis PR reverts:\\r\\n- #7114\\r\\n\\r\\nNote that audio extra tests can be included again with numpy-2.1 once next numba-0.61.0 version is released.',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7118/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7118/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=2&state=all\"\n",
    "response = requests.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a85c4a-0701-45fc-af8d-231e82c1cd56",
   "metadata": {},
   "source": [
    "와, 정보가 많네요! 이슈를 설명하는 제목(title), 본문(body) 및 번호(number)와 같은 유용한 필드와 해당 이슈를 제기한 GitHub 사용자에 대한 정보를 볼 수 있습니다.\n",
    "\n",
    "GitHub 문서에 설명된 대로 인증되지 않은 요청은 시간당 60개 요청으로 제한됩니다. per_page 쿼리 매개변수를 늘려 수행해야할 요청의 총수를 줄일 수 있지만, 이슈가 수천 개 이상인 저장소에서는 여전히 비율 제한에 도달하게 됩니다. 따라서 그 대신 시간당 5,000개 요청으로 속도 제한을 높일 수 있도록 개인 액세스 토큰(personal access token) 생성에 대한 GitHub의 지침을 따라야 합니다. 토큰이 있으면 요청 헤더(request header)의 일부로 포함할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7fb569b-aedd-4e2a-b27e-e8222ac9c276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = \"###########\"   ## 본인의 personal access token을 지정하세요.\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2fe28b80-7263-4efc-b3c0-f55d09d6f0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'token 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'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb49415-be1e-4a58-965e-f82573e19c4a",
   "metadata": {},
   "source": [
    "이제 액세스 토큰이 있으므로 GitHub 리포지토리에서 모든 이슈들을 다운로드할 수 있는 함수를 생성해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0266413-81c9-405b-a639-a0fa24922f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=10000,\n",
    "    rate_limit=5000,\n",
    "    issues_path=Path(\".\")\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100    # 페이지당 리턴받는 이슈의 개수\n",
    "    #per_page = 50\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # state=all로 질의하여 미해결, 해결 이슈들을 모두 가지고 온다.\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []    # 다음 주기를 위해서 batch를 비운다.\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367c233-acdd-4226-bd0d-36eeca020ebe",
   "metadata": {},
   "source": [
    "이제 fetch_issues()를 호출하면 시간당 요청 수에 대한 GitHub의 제한을 초과하지 않도록 모든 이슈를 일괄적으로 다운로드합니다. 결과는 repository_name-issues.jsonl 파일에 저장됩니다. 여기서 각 행은 이슈를 나타내는 JSON 객체입니다. 이 기능을 사용하여 🤗Datasets의 모든 이슈들을 파악해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b09a904a-8c7f-458a-bcd3-b3107cc392dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220c465410c0442aa90026cf5079f3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached GitHub rate limit. Sleeping for one hour ...\n",
      "Downloaded all the issues for datasets! Dataset stored at ./datasets-issues.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 인터넷 연결 상태에 따라, 몇 분이 걸릴 수 있습니다.\n",
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d2a81-6804-4ba0-8dc4-a93179a0d2ed",
   "metadata": {},
   "source": [
    "이슈들이 모두 다운로드되면 5.2에서 살펴본 기법을 사용하여 로컬에서 로드할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "abfcdc80-2f5b-4050-94fd-382192392fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed194d5641a0459b8bd9cc5811095e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/builder.py:1989\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/arrow_writer.py:584\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    583\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mcombine_chunks()\n\u001b[0;32m--> 584\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_local_files:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:2240\u001b[0m, in \u001b[0;36mtable_cast\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m!=\u001b[39m schema:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m!=\u001b[39m schema\u001b[38;5;241m.\u001b[39mmetadata:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:2199\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[1;32m   2195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2196\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m   2197\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[1;32m   2198\u001b[0m     )\n\u001b[0;32m-> 2199\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:2199\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[1;32m   2195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2196\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m   2197\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[1;32m   2198\u001b[0m     )\n\u001b[0;32m-> 2199\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1793\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray):\n\u001b[0;32m-> 1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([func(chunk, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1793\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray):\n\u001b[0;32m-> 1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1984\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 1984\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [_c(array\u001b[38;5;241m.\u001b[39mfield(name), subfeature) \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1984\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 1984\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1795\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:2065\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, (Sequence, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 2065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_number_to_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_number_to_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1795\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1936\u001b[0m, in \u001b[0;36marray_cast\u001b[0;34m(array, pa_type, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(pa_type) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(array\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m-> 1936\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpa_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\u001b[38;5;241m.\u001b[39mcast(pa_type)\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't cast array of type timestamp[s] to null",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#issues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"train\")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m issues_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets-issues.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m issues_dataset\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:2582\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2582\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2591\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2592\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2593\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2594\u001b[0m )\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/builder.py:1005\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/builder.py:1100\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1107\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/builder.py:1860\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1858\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1860\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1861\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1862\u001b[0m     ):\n\u001b[1;32m   1863\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1864\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/builder.py:2016\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[1;32m   2015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#issues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"train\")\n",
    "issues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67a2432e-f590-46fc-840c-2f290e7fbace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repository_url</th>\n",
       "      <th>labels_url</th>\n",
       "      <th>comments_url</th>\n",
       "      <th>events_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>author_association</th>\n",
       "      <th>active_lock_reason</th>\n",
       "      <th>draft</th>\n",
       "      <th>pull_request</th>\n",
       "      <th>body</th>\n",
       "      <th>reactions</th>\n",
       "      <th>timeline_url</th>\n",
       "      <th>performed_via_github_app</th>\n",
       "      <th>state_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/7119</td>\n",
       "      <td>2477766493</td>\n",
       "      <td>PR_kwDODunzps54-GjY</td>\n",
       "      <td>7119</td>\n",
       "      <td>Install transformers with numpy-2 CI</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-21 11:36:50+00:00</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>Install transformers with numpy-2 CI.\\r\\n\\r\\nN...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/7118</td>\n",
       "      <td>2477676893</td>\n",
       "      <td>PR_kwDODunzps549yu4</td>\n",
       "      <td>7118</td>\n",
       "      <td>Allow numpy-2.1 and test it without audio extra</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-21 10:58:15+00:00</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>Allow numpy-2.1 and test it without audio extr...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2476555659</td>\n",
       "      <td>I_kwDODunzps6TnT2L</td>\n",
       "      <td>7117</td>\n",
       "      <td>Audio dataset load everything in RAM and is ve...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Hello, I'm working with an audio dataset. I wa...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2475522721</td>\n",
       "      <td>I_kwDODunzps6TjXqh</td>\n",
       "      <td>7116</td>\n",
       "      <td>datasets cannot handle nested json if features...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>### Describe the bug\\n\\nI have a json named te...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2475363142</td>\n",
       "      <td>I_kwDODunzps6TiwtG</td>\n",
       "      <td>7115</td>\n",
       "      <td>module 'pyarrow.lib' has no attribute 'ListVie...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>### Describe the bug\\n\\nCode:\\r\\n`!pipuninstal...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                      repository_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datasets   \n",
       "1  https://api.github.com/repos/huggingface/datasets   \n",
       "2  https://api.github.com/repos/huggingface/datasets   \n",
       "3  https://api.github.com/repos/huggingface/datasets   \n",
       "4  https://api.github.com/repos/huggingface/datasets   \n",
       "\n",
       "                                          labels_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                        comments_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                          events_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                            html_url          id  \\\n",
       "0  https://github.com/huggingface/datasets/pull/7119  2477766493   \n",
       "1  https://github.com/huggingface/datasets/pull/7118  2477676893   \n",
       "2  https://github.com/huggingface/datasets/issues...  2476555659   \n",
       "3  https://github.com/huggingface/datasets/issues...  2475522721   \n",
       "4  https://github.com/huggingface/datasets/issues...  2475363142   \n",
       "\n",
       "               node_id  number  \\\n",
       "0  PR_kwDODunzps54-GjY    7119   \n",
       "1  PR_kwDODunzps549yu4    7118   \n",
       "2   I_kwDODunzps6TnT2L    7117   \n",
       "3   I_kwDODunzps6TjXqh    7116   \n",
       "4   I_kwDODunzps6TiwtG    7115   \n",
       "\n",
       "                                               title  ...  \\\n",
       "0               Install transformers with numpy-2 CI  ...   \n",
       "1    Allow numpy-2.1 and test it without audio extra  ...   \n",
       "2  Audio dataset load everything in RAM and is ve...  ...   \n",
       "3  datasets cannot handle nested json if features...  ...   \n",
       "4  module 'pyarrow.lib' has no attribute 'ListVie...  ...   \n",
       "\n",
       "                  closed_at author_association active_lock_reason  draft  \\\n",
       "0 2024-08-21 11:36:50+00:00             MEMBER                NaN    0.0   \n",
       "1 2024-08-21 10:58:15+00:00             MEMBER                NaN    0.0   \n",
       "2                       NaT               NONE                NaN    NaN   \n",
       "3                       NaT               NONE                NaN    NaN   \n",
       "4                       NaT               NONE                NaN    NaN   \n",
       "\n",
       "                                        pull_request  \\\n",
       "0  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "1  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                                body  \\\n",
       "0  Install transformers with numpy-2 CI.\\r\\n\\r\\nN...   \n",
       "1  Allow numpy-2.1 and test it without audio extr...   \n",
       "2  Hello, I'm working with an audio dataset. I wa...   \n",
       "3  ### Describe the bug\\n\\nI have a json named te...   \n",
       "4  ### Describe the bug\\n\\nCode:\\r\\n`!pipuninstal...   \n",
       "\n",
       "                                           reactions  \\\n",
       "0  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "1  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "2  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "3  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "4  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "\n",
       "                                        timeline_url performed_via_github_app  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "1  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "2  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "3  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "4  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "\n",
       "  state_reason  \n",
       "0         None  \n",
       "1         None  \n",
       "2         None  \n",
       "3         None  \n",
       "4         None  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json(\"datasets-issues.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a936954-1541-443b-b0df-e6dc23691ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason'],\n",
      "    num_rows: 7076\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/7119',\n",
       " 'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       " 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/labels{/name}',\n",
       " 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/comments',\n",
       " 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/events',\n",
       " 'html_url': 'https://github.com/huggingface/datasets/pull/7119',\n",
       " 'id': 2477766493,\n",
       " 'node_id': 'PR_kwDODunzps54-GjY',\n",
       " 'number': 7119,\n",
       " 'title': 'Install transformers with numpy-2 CI',\n",
       " 'user': {'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4',\n",
       "  'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}',\n",
       "  'followers_url': 'https://api.github.com/users/albertvillanova/followers',\n",
       "  'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}',\n",
       "  'gravatar_id': '',\n",
       "  'html_url': 'https://github.com/albertvillanova',\n",
       "  'id': 8515462,\n",
       "  'login': 'albertvillanova',\n",
       "  'node_id': 'MDQ6VXNlcjg1MTU0NjI=',\n",
       "  'organizations_url': 'https://api.github.com/users/albertvillanova/orgs',\n",
       "  'received_events_url': 'https://api.github.com/users/albertvillanova/received_events',\n",
       "  'repos_url': 'https://api.github.com/users/albertvillanova/repos',\n",
       "  'site_admin': False,\n",
       "  'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions',\n",
       "  'type': 'User',\n",
       "  'url': 'https://api.github.com/users/albertvillanova'},\n",
       " 'labels': [],\n",
       " 'state': 'closed',\n",
       " 'locked': False,\n",
       " 'assignee': None,\n",
       " 'assignees': [],\n",
       " 'milestone': None,\n",
       " 'comments': 2,\n",
       " 'created_at': Timestamp('2024-08-21 11:14:59+0000', tz='UTC'),\n",
       " 'updated_at': Timestamp('2024-08-21 11:42:35+0000', tz='UTC'),\n",
       " 'closed_at': Timestamp('2024-08-21 11:36:50+0000', tz='UTC'),\n",
       " 'author_association': 'MEMBER',\n",
       " 'active_lock_reason': None,\n",
       " 'draft': 0.0,\n",
       " 'pull_request': {'diff_url': 'https://github.com/huggingface/datasets/pull/7119.diff',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/7119',\n",
       "  'merged_at': '2024-08-21T11:36:50Z',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/7119.patch',\n",
       "  'url': 'https://api.github.com/repos/huggingface/datasets/pulls/7119'},\n",
       " 'body': 'Install transformers with numpy-2 CI.\\r\\n\\r\\nNote that transformers no longer pins numpy < 2 since transformers-4.43.0:\\r\\n- https://github.com/huggingface/transformers/pull/32018\\r\\n- https://github.com/huggingface/transformers/releases/tag/v4.43.0',\n",
       " 'reactions': {'+1': 0,\n",
       "  '-1': 0,\n",
       "  'confused': 0,\n",
       "  'eyes': 0,\n",
       "  'heart': 0,\n",
       "  'hooray': 0,\n",
       "  'laugh': 0,\n",
       "  'rocket': 0,\n",
       "  'total_count': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/reactions'},\n",
       " 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/timeline',\n",
       " 'performed_via_github_app': None,\n",
       " 'state_reason': None}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json(\"datasets-issues.jsonl\", lines=True)\n",
    "df.head()\n",
    "\n",
    "from datasets import Dataset\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "print(issues_dataset)\n",
    "sample = issues_dataset.select(range(3))\n",
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "845ce0c4-a0e4-4252-b24c-29fda81a99df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repository_url</th>\n",
       "      <th>labels_url</th>\n",
       "      <th>comments_url</th>\n",
       "      <th>events_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>author_association</th>\n",
       "      <th>active_lock_reason</th>\n",
       "      <th>draft</th>\n",
       "      <th>pull_request</th>\n",
       "      <th>body</th>\n",
       "      <th>reactions</th>\n",
       "      <th>timeline_url</th>\n",
       "      <th>performed_via_github_app</th>\n",
       "      <th>state_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/7119</td>\n",
       "      <td>2477766493</td>\n",
       "      <td>PR_kwDODunzps54-GjY</td>\n",
       "      <td>7119</td>\n",
       "      <td>Install transformers with numpy-2 CI</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-21 11:36:50+00:00</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>Install transformers with numpy-2 CI.\\r\\n\\r\\nN...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/7118</td>\n",
       "      <td>2477676893</td>\n",
       "      <td>PR_kwDODunzps549yu4</td>\n",
       "      <td>7118</td>\n",
       "      <td>Allow numpy-2.1 and test it without audio extra</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-21 10:58:15+00:00</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>Allow numpy-2.1 and test it without audio extr...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2476555659</td>\n",
       "      <td>I_kwDODunzps6TnT2L</td>\n",
       "      <td>7117</td>\n",
       "      <td>Audio dataset load everything in RAM and is ve...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Hello, I'm working with an audio dataset. I wa...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2475522721</td>\n",
       "      <td>I_kwDODunzps6TjXqh</td>\n",
       "      <td>7116</td>\n",
       "      <td>datasets cannot handle nested json if features...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>### Describe the bug\\n\\nI have a json named te...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2475363142</td>\n",
       "      <td>I_kwDODunzps6TiwtG</td>\n",
       "      <td>7115</td>\n",
       "      <td>module 'pyarrow.lib' has no attribute 'ListVie...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>### Describe the bug\\n\\nCode:\\r\\n`!pipuninstal...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/huggingf...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                      repository_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datasets   \n",
       "1  https://api.github.com/repos/huggingface/datasets   \n",
       "2  https://api.github.com/repos/huggingface/datasets   \n",
       "3  https://api.github.com/repos/huggingface/datasets   \n",
       "4  https://api.github.com/repos/huggingface/datasets   \n",
       "\n",
       "                                          labels_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                        comments_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                          events_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                            html_url          id  \\\n",
       "0  https://github.com/huggingface/datasets/pull/7119  2477766493   \n",
       "1  https://github.com/huggingface/datasets/pull/7118  2477676893   \n",
       "2  https://github.com/huggingface/datasets/issues...  2476555659   \n",
       "3  https://github.com/huggingface/datasets/issues...  2475522721   \n",
       "4  https://github.com/huggingface/datasets/issues...  2475363142   \n",
       "\n",
       "               node_id  number  \\\n",
       "0  PR_kwDODunzps54-GjY    7119   \n",
       "1  PR_kwDODunzps549yu4    7118   \n",
       "2   I_kwDODunzps6TnT2L    7117   \n",
       "3   I_kwDODunzps6TjXqh    7116   \n",
       "4   I_kwDODunzps6TiwtG    7115   \n",
       "\n",
       "                                               title  ...  \\\n",
       "0               Install transformers with numpy-2 CI  ...   \n",
       "1    Allow numpy-2.1 and test it without audio extra  ...   \n",
       "2  Audio dataset load everything in RAM and is ve...  ...   \n",
       "3  datasets cannot handle nested json if features...  ...   \n",
       "4  module 'pyarrow.lib' has no attribute 'ListVie...  ...   \n",
       "\n",
       "                  closed_at author_association active_lock_reason  draft  \\\n",
       "0 2024-08-21 11:36:50+00:00             MEMBER                NaN    0.0   \n",
       "1 2024-08-21 10:58:15+00:00             MEMBER                NaN    0.0   \n",
       "2                       NaT               NONE                NaN    NaN   \n",
       "3                       NaT               NONE                NaN    NaN   \n",
       "4                       NaT               NONE                NaN    NaN   \n",
       "\n",
       "                                        pull_request  \\\n",
       "0  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "1  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                                body  \\\n",
       "0  Install transformers with numpy-2 CI.\\r\\n\\r\\nN...   \n",
       "1  Allow numpy-2.1 and test it without audio extr...   \n",
       "2  Hello, I'm working with an audio dataset. I wa...   \n",
       "3  ### Describe the bug\\n\\nI have a json named te...   \n",
       "4  ### Describe the bug\\n\\nCode:\\r\\n`!pipuninstal...   \n",
       "\n",
       "                                           reactions  \\\n",
       "0  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "1  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "2  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "3  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "4  {'url': 'https://api.github.com/repos/huggingf...   \n",
       "\n",
       "                                        timeline_url performed_via_github_app  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "1  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "2  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "3  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "4  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "\n",
       "  state_reason  \n",
       "0         None  \n",
       "1         None  \n",
       "2         None  \n",
       "3         None  \n",
       "4         None  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3810b76-4afa-49fc-b878-9f247f17315e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/7119',\n",
       " 'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       " 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/labels{/name}',\n",
       " 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/comments',\n",
       " 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/events',\n",
       " 'html_url': 'https://github.com/huggingface/datasets/pull/7119',\n",
       " 'id': 2477766493,\n",
       " 'node_id': 'PR_kwDODunzps54-GjY',\n",
       " 'number': 7119,\n",
       " 'title': 'Install transformers with numpy-2 CI',\n",
       " 'user': {'login': 'albertvillanova',\n",
       "  'id': 8515462,\n",
       "  'node_id': 'MDQ6VXNlcjg1MTU0NjI=',\n",
       "  'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/albertvillanova',\n",
       "  'html_url': 'https://github.com/albertvillanova',\n",
       "  'followers_url': 'https://api.github.com/users/albertvillanova/followers',\n",
       "  'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/albertvillanova/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/albertvillanova/repos',\n",
       "  'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/albertvillanova/received_events',\n",
       "  'type': 'User',\n",
       "  'site_admin': False},\n",
       " 'labels': [],\n",
       " 'state': 'closed',\n",
       " 'locked': False,\n",
       " 'assignee': None,\n",
       " 'assignees': [],\n",
       " 'milestone': None,\n",
       " 'comments': 2,\n",
       " 'created_at': datetime.datetime(2024, 8, 21, 11, 14, 59),\n",
       " 'updated_at': datetime.datetime(2024, 8, 21, 11, 42, 35),\n",
       " 'closed_at': datetime.datetime(2024, 8, 21, 11, 36, 50),\n",
       " 'author_association': 'MEMBER',\n",
       " 'active_lock_reason': None,\n",
       " 'draft': False,\n",
       " 'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/7119',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/7119',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/7119.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/7119.patch',\n",
       "  'merged_at': datetime.datetime(2024, 8, 21, 11, 36, 50)},\n",
       " 'body': 'Install transformers with numpy-2 CI.\\r\\n\\r\\nNote that transformers no longer pins numpy < 2 since transformers-4.43.0:\\r\\n- https://github.com/huggingface/transformers/pull/32018\\r\\n- https://github.com/huggingface/transformers/releases/tag/v4.43.0',\n",
       " 'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/reactions',\n",
       "  'total_count': 0,\n",
       "  '+1': 0,\n",
       "  '-1': 0,\n",
       "  'laugh': 0,\n",
       "  'hooray': 0,\n",
       "  'confused': 0,\n",
       "  'heart': 0,\n",
       "  'rocket': 0,\n",
       "  'eyes': 0},\n",
       " 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/timeline',\n",
       " 'performed_via_github_app': None,\n",
       " 'state_reason': None}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset_streamed = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"train\", streaming=True)\n",
    "next(iter(issues_dataset_streamed))\n",
    "#issues_dataset_streamed = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", streaming=True)\n",
    "#next(iter(issues_dataset_streamed[\"train\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4446740-1403-4f7e-94fd-b2472807578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7119', 'repository_url': 'https://api.github.com/repos/huggingface/datasets', 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/labels{/name}', 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/comments', 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/events', 'html_url': 'https://github.com/huggingface/datasets/pull/7119', 'id': 2477766493, 'node_id': 'PR_kwDODunzps54-GjY', 'number': 7119, 'title': 'Install transformers with numpy-2 CI', 'user': {'login': 'albertvillanova', 'id': 8515462, 'node_id': 'MDQ6VXNlcjg1MTU0NjI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/albertvillanova', 'html_url': 'https://github.com/albertvillanova', 'followers_url': 'https://api.github.com/users/albertvillanova/followers', 'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}', 'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions', 'organizations_url': 'https://api.github.com/users/albertvillanova/orgs', 'repos_url': 'https://api.github.com/users/albertvillanova/repos', 'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}', 'received_events_url': 'https://api.github.com/users/albertvillanova/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': datetime.datetime(2024, 8, 21, 11, 14, 59), 'updated_at': datetime.datetime(2024, 8, 21, 11, 42, 35), 'closed_at': datetime.datetime(2024, 8, 21, 11, 36, 50), 'author_association': 'MEMBER', 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/7119', 'html_url': 'https://github.com/huggingface/datasets/pull/7119', 'diff_url': 'https://github.com/huggingface/datasets/pull/7119.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/7119.patch', 'merged_at': datetime.datetime(2024, 8, 21, 11, 36, 50)}, 'body': 'Install transformers with numpy-2 CI.\\r\\n\\r\\nNote that transformers no longer pins numpy < 2 since transformers-4.43.0:\\r\\n- https://github.com/huggingface/transformers/pull/32018\\r\\n- https://github.com/huggingface/transformers/releases/tag/v4.43.0', 'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/timeline', 'performed_via_github_app': None, 'state_reason': None}\n",
      "1 {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7118', 'repository_url': 'https://api.github.com/repos/huggingface/datasets', 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7118/labels{/name}', 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7118/comments', 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7118/events', 'html_url': 'https://github.com/huggingface/datasets/pull/7118', 'id': 2477676893, 'node_id': 'PR_kwDODunzps549yu4', 'number': 7118, 'title': 'Allow numpy-2.1 and test it without audio extra', 'user': {'login': 'albertvillanova', 'id': 8515462, 'node_id': 'MDQ6VXNlcjg1MTU0NjI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/albertvillanova', 'html_url': 'https://github.com/albertvillanova', 'followers_url': 'https://api.github.com/users/albertvillanova/followers', 'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}', 'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions', 'organizations_url': 'https://api.github.com/users/albertvillanova/orgs', 'repos_url': 'https://api.github.com/users/albertvillanova/repos', 'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}', 'received_events_url': 'https://api.github.com/users/albertvillanova/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 2, 'created_at': datetime.datetime(2024, 8, 21, 10, 29, 35), 'updated_at': datetime.datetime(2024, 8, 21, 11, 5, 3), 'closed_at': datetime.datetime(2024, 8, 21, 10, 58, 15), 'author_association': 'MEMBER', 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/7118', 'html_url': 'https://github.com/huggingface/datasets/pull/7118', 'diff_url': 'https://github.com/huggingface/datasets/pull/7118.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/7118.patch', 'merged_at': datetime.datetime(2024, 8, 21, 10, 58, 15)}, 'body': 'Allow numpy-2.1 and test it without audio extra.\\r\\n\\r\\nThis PR reverts:\\r\\n- #7114\\r\\n\\r\\nNote that audio extra tests can be included again with numpy-2.1 once next numba-0.61.0 version is released.', 'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7118/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7118/timeline', 'performed_via_github_app': None, 'state_reason': None}\n",
      "2 {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7117', 'repository_url': 'https://api.github.com/repos/huggingface/datasets', 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7117/labels{/name}', 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7117/comments', 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7117/events', 'html_url': 'https://github.com/huggingface/datasets/issues/7117', 'id': 2476555659, 'node_id': 'I_kwDODunzps6TnT2L', 'number': 7117, 'title': 'Audio dataset load everything in RAM and is very slow', 'user': {'login': 'Jourdelune', 'id': 64205064, 'node_id': 'MDQ6VXNlcjY0MjA1MDY0', 'avatar_url': 'https://avatars.githubusercontent.com/u/64205064?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/Jourdelune', 'html_url': 'https://github.com/Jourdelune', 'followers_url': 'https://api.github.com/users/Jourdelune/followers', 'following_url': 'https://api.github.com/users/Jourdelune/following{/other_user}', 'gists_url': 'https://api.github.com/users/Jourdelune/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/Jourdelune/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/Jourdelune/subscriptions', 'organizations_url': 'https://api.github.com/users/Jourdelune/orgs', 'repos_url': 'https://api.github.com/users/Jourdelune/repos', 'events_url': 'https://api.github.com/users/Jourdelune/events{/privacy}', 'received_events_url': 'https://api.github.com/users/Jourdelune/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': datetime.datetime(2024, 8, 20, 21, 18, 12), 'updated_at': datetime.datetime(2024, 8, 20, 21, 32, 23), 'closed_at': None, 'author_association': 'NONE', 'active_lock_reason': None, 'draft': None, 'pull_request': None, 'body': 'Hello, I\\'m working with an audio dataset. I want to transcribe the audio that the dataset contain, and for that I use whisper. My issue is that the dataset load everything in the RAM when I map the dataset, obviously, when RAM usage is too high, the program crashes.\\r\\n\\r\\nTo fix this issue, I\\'m using writer_batch_size that I set to 10, but in this case, the mapping of the dataset is extremely slow.\\r\\nTo illustrate this, on 50 examples, with `writer_batch_size` set to 10, it takes 123.24 seconds to process the dataset, but without `writer_batch_size` set to 10, it takes about ten seconds to process the dataset, but then the process remains blocked (I assume that it is writing the dataset and therefore suffers from the same problem as `writer_batch_size`)\\r\\n\\r\\n### Steps to reproduce the bug\\r\\n\\r\\nHug ram usage but fast (but actually slow when saving the dataset):\\r\\n```py\\r\\nfrom datasets import load_dataset\\r\\nimport time\\r\\n\\r\\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\\r\\n\\r\\n\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    row[\"transcribed\"] = True\\r\\n    return row\\r\\n\\r\\n\\r\\ntime1 = time.time()\\r\\nds = ds.map(\\r\\n    transcribe_audio\\r\\n) \\r\\n\\r\\nfor row in ds:\\r\\n    pass  # do nothing, just iterate to trigger the map function\\r\\n\\r\\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\\r\\n```\\r\\n\\r\\nLow ram usage but very very slow:\\r\\n\\r\\n```py\\r\\nfrom datasets import load_dataset\\r\\nimport time\\r\\n\\r\\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\\r\\n\\r\\n\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    row[\"transcribed\"] = True\\r\\n    return row\\r\\n\\r\\n\\r\\ntime1 = time.time()\\r\\nds = ds.map(\\r\\n    transcribe_audio, writer_batch_size=10\\r\\n)  # set low writer_batch_size to avoid memory issues\\r\\n\\r\\nfor row in ds:\\r\\n    pass  # do nothing, just iterate to trigger the map function\\r\\n\\r\\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\\r\\n```\\r\\n\\r\\n### Expected behavior\\r\\n\\r\\nI think the processing should be much faster, on only 50 audio examples, the mapping takes several minutes while nothing is done (just loading the audio).\\r\\n\\r\\n### Environment info\\r\\n\\r\\n- `datasets` version: 2.21.0\\r\\n- Platform: Linux-6.10.5-arch1-1-x86_64-with-glibc2.40\\r\\n- Python version: 3.10.4\\r\\n- `huggingface_hub` version: 0.24.5\\r\\n- PyArrow version: 17.0.0\\r\\n- Pandas version: 1.5.3\\r\\n- `fsspec` version: 2024.6.1\\r\\n\\r\\n# Extra\\r\\n\\r\\nThe dataset has been generated by using audio folder, so I don\\'t think anything specific in my code is causing this problem.\\r\\n```py\\r\\nimport argparse\\r\\n\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nparser = argparse.ArgumentParser()\\r\\nparser.add_argument(\"--folder\", help=\"folder path\", default=\"/media/works/test/\")\\r\\nargs = parser.parse_args()\\r\\n\\r\\ndataset = load_dataset(\"audiofolder\", data_dir=args.folder)\\r\\n\\r\\n# push the dataset to hub\\r\\ndataset.push_to_hub(\"WaveGenAI/audios\")\\r\\n```\\r\\n\\r\\nAlso, it\\'s the combination of `audio = row[\"audio\"]` and `row[\"transcribed\"] = True` which causes problems, `row[\"transcribed\"] = True `alone does nothing and `audio = row[\"audio\"]` alone sometimes causes problems, sometimes not.', 'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7117/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7117/timeline', 'performed_via_github_app': None, 'state_reason': None}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(issues_dataset_streamed):\n",
    "    if i < 3:\n",
    "        print(i, data)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595012f0-0fb6-4d2a-9499-99fbccc8b548",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### URL과 pull request 엔트리를 출력합니다.\n",
    "```\n",
    "sample = issues_dataset.shuffle(seed=777).select(range(3))\n",
    "## URL과 pull request 엔트리를 출력합니다.\n",
    "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Pull request: {pr}\\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe4ed9c6-8a16-4960-a9aa-61aecc14849d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shuffled_dataset = issues_dataset_streamed.shuffle(buffer_size=10000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37a532fd-4016-494e-9757-6707d3398cba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> URL: https://github.com/huggingface/datasets/pull/5007\n",
      ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/5007', 'html_url': 'https://github.com/huggingface/datasets/pull/5007', 'diff_url': 'https://github.com/huggingface/datasets/pull/5007.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/5007.patch', 'merged_at': datetime.datetime(2022, 9, 22, 10, 14, 6)}\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/3719\n",
      ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/3719', 'html_url': 'https://github.com/huggingface/datasets/pull/3719', 'diff_url': 'https://github.com/huggingface/datasets/pull/3719.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/3719.patch', 'merged_at': datetime.datetime(2022, 2, 14, 19, 19, 21)}\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/4086\n",
      ">> Pull request: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = shuffled_dataset.take(3)\n",
    "for sample in samples:\n",
    "   (url, pr) = (sample[\"html_url\"], sample[\"pull_request\"])\n",
    "   print(f\">> URL: {url}\")\n",
    "   print(f\">> Pull request: {pr}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cbc395a2-5c14-4351-aab8-c68969fd941d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d23137c75824588817eb7c2dba9b801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/16.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec87eba0103d4e5497a93946197b72a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
      "    num_rows: 3651\n",
      "})\n",
      "3651\n"
     ]
    }
   ],
   "source": [
    "spasis_issues_dataset = load_dataset(\"spasis/datasets-github-issues\", split=\"train\")\n",
    "print(spasis_issues_dataset)\n",
    "print(len(spasis_issues_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9b530ce-b7d3-4567-aad3-4d6cc65cc4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/5007', 'repository_url': 'https://api.github.com/repos/huggingface/datasets', 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/5007/labels{/name}', 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/5007/comments', 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/5007/events', 'html_url': 'https://github.com/huggingface/datasets/pull/5007', 'id': 1381007607, 'node_id': 'PR_kwDODunzps4_WvFQ', 'number': 5007, 'title': 'Add some note about running the transformers ci before a release', 'user': {'login': 'lhoestq', 'id': 42851186, 'node_id': 'MDQ6VXNlcjQyODUxMTg2', 'avatar_url': 'https://avatars.githubusercontent.com/u/42851186?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/lhoestq', 'html_url': 'https://github.com/lhoestq', 'followers_url': 'https://api.github.com/users/lhoestq/followers', 'following_url': 'https://api.github.com/users/lhoestq/following{/other_user}', 'gists_url': 'https://api.github.com/users/lhoestq/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/lhoestq/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/lhoestq/subscriptions', 'organizations_url': 'https://api.github.com/users/lhoestq/orgs', 'repos_url': 'https://api.github.com/users/lhoestq/repos', 'events_url': 'https://api.github.com/users/lhoestq/events{/privacy}', 'received_events_url': 'https://api.github.com/users/lhoestq/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 1, 'created_at': datetime.datetime(2022, 9, 21, 14, 14, 25), 'updated_at': datetime.datetime(2022, 9, 22, 10, 16, 14), 'closed_at': datetime.datetime(2022, 9, 22, 10, 14, 6), 'author_association': 'MEMBER', 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/5007', 'html_url': 'https://github.com/huggingface/datasets/pull/5007', 'diff_url': 'https://github.com/huggingface/datasets/pull/5007.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/5007.patch', 'merged_at': datetime.datetime(2022, 9, 22, 10, 14, 6)}, 'body': None, 'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/5007/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/5007/timeline', 'performed_via_github_app': None, 'state_reason': None}\n",
      "--------------------------------------------------\n",
      "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/3719', 'repository_url': 'https://api.github.com/repos/huggingface/datasets', 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/3719/labels{/name}', 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/3719/comments', 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/3719/events', 'html_url': 'https://github.com/huggingface/datasets/pull/3719', 'id': 1137237622, 'node_id': 'PR_kwDODunzps4yyFv7', 'number': 3719, 'title': 'Check if indices values in `Dataset.select` are within bounds', 'user': {'login': 'mariosasko', 'id': 47462742, 'node_id': 'MDQ6VXNlcjQ3NDYyNzQy', 'avatar_url': 'https://avatars.githubusercontent.com/u/47462742?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/mariosasko', 'html_url': 'https://github.com/mariosasko', 'followers_url': 'https://api.github.com/users/mariosasko/followers', 'following_url': 'https://api.github.com/users/mariosasko/following{/other_user}', 'gists_url': 'https://api.github.com/users/mariosasko/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/mariosasko/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/mariosasko/subscriptions', 'organizations_url': 'https://api.github.com/users/mariosasko/orgs', 'repos_url': 'https://api.github.com/users/mariosasko/repos', 'events_url': 'https://api.github.com/users/mariosasko/events{/privacy}', 'received_events_url': 'https://api.github.com/users/mariosasko/received_events', 'type': 'User', 'site_admin': False}, 'labels': [], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': datetime.datetime(2022, 2, 14, 12, 31, 41), 'updated_at': datetime.datetime(2022, 2, 14, 19, 19, 22), 'closed_at': datetime.datetime(2022, 2, 14, 19, 19, 22), 'author_association': 'COLLABORATOR', 'active_lock_reason': None, 'draft': False, 'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/3719', 'html_url': 'https://github.com/huggingface/datasets/pull/3719', 'diff_url': 'https://github.com/huggingface/datasets/pull/3719.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/3719.patch', 'merged_at': datetime.datetime(2022, 2, 14, 19, 19, 21)}, 'body': 'Fix #3707 \\r\\n\\r\\nInstead of reusing `_check_valid_index_key` from `datasets.formatting`, I defined a new function to provide a more meaningful error message.\\r\\n', 'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/3719/reactions', 'total_count': 1, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 1, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/3719/timeline', 'performed_via_github_app': None, 'state_reason': None}\n",
      "--------------------------------------------------\n",
      "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/4086', 'repository_url': 'https://api.github.com/repos/huggingface/datasets', 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/4086/labels{/name}', 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/4086/comments', 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/4086/events', 'html_url': 'https://github.com/huggingface/datasets/issues/4086', 'id': 1191373374, 'node_id': 'I_kwDODunzps5HAuo-', 'number': 4086, 'title': 'Dataset viewer issue for McGill-NLP/feedbackQA', 'user': {'login': 'cslizc', 'id': 54827718, 'node_id': 'MDQ6VXNlcjU0ODI3NzE4', 'avatar_url': 'https://avatars.githubusercontent.com/u/54827718?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/cslizc', 'html_url': 'https://github.com/cslizc', 'followers_url': 'https://api.github.com/users/cslizc/followers', 'following_url': 'https://api.github.com/users/cslizc/following{/other_user}', 'gists_url': 'https://api.github.com/users/cslizc/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/cslizc/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/cslizc/subscriptions', 'organizations_url': 'https://api.github.com/users/cslizc/orgs', 'repos_url': 'https://api.github.com/users/cslizc/repos', 'events_url': 'https://api.github.com/users/cslizc/events{/privacy}', 'received_events_url': 'https://api.github.com/users/cslizc/received_events', 'type': 'User', 'site_admin': False}, 'labels': [{'id': 3470211881, 'node_id': 'LA_kwDODunzps7O1zsp', 'url': 'https://api.github.com/repos/huggingface/datasets/labels/dataset-viewer', 'name': 'dataset-viewer', 'color': 'E5583E', 'default': False, 'description': 'Related to the dataset viewer on huggingface.co'}], 'state': 'closed', 'locked': False, 'assignee': {'login': 'albertvillanova', 'id': 8515462, 'node_id': 'MDQ6VXNlcjg1MTU0NjI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/albertvillanova', 'html_url': 'https://github.com/albertvillanova', 'followers_url': 'https://api.github.com/users/albertvillanova/followers', 'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}', 'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions', 'organizations_url': 'https://api.github.com/users/albertvillanova/orgs', 'repos_url': 'https://api.github.com/users/albertvillanova/repos', 'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}', 'received_events_url': 'https://api.github.com/users/albertvillanova/received_events', 'type': 'User', 'site_admin': False}, 'assignees': [{'login': 'albertvillanova', 'id': 8515462, 'node_id': 'MDQ6VXNlcjg1MTU0NjI=', 'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/albertvillanova', 'html_url': 'https://github.com/albertvillanova', 'followers_url': 'https://api.github.com/users/albertvillanova/followers', 'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}', 'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions', 'organizations_url': 'https://api.github.com/users/albertvillanova/orgs', 'repos_url': 'https://api.github.com/users/albertvillanova/repos', 'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}', 'received_events_url': 'https://api.github.com/users/albertvillanova/received_events', 'type': 'User', 'site_admin': False}], 'milestone': None, 'comments': 2, 'created_at': datetime.datetime(2022, 4, 4, 7, 27, 20), 'updated_at': datetime.datetime(2022, 4, 4, 22, 29, 53), 'closed_at': datetime.datetime(2022, 4, 4, 8, 1, 45), 'author_association': 'NONE', 'active_lock_reason': None, 'draft': None, 'pull_request': None, 'body': \"## Dataset viewer issue for '*McGill-NLP/feedbackQA*'\\r\\n\\r\\n**Link:** *[link to the dataset viewer page](https://huggingface.co/datasets/McGill-NLP/feedbackQA)*\\r\\n\\r\\n*short description of the issue*\\r\\nThe dataset can be loaded correctly with `load_dataset` but the preview doesn't work. Error message:\\r\\n\\r\\n```\\r\\nStatus code:   400\\r\\nException:     Status400Error\\r\\nMessage:       Not found. Maybe the cache is missing, or maybe the dataset does not exist.\\r\\n```\\r\\n\\r\\nAm I the one who added this dataset ? Yes\\r\\n\", 'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/4086/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/4086/timeline', 'performed_via_github_app': None, 'state_reason': 'completed'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sample in samples:\n",
    "    print(sample)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f963964-a917-4135-8ae6-8479e573e946",
   "metadata": {},
   "source": [
    "여기에서 각 pull 요청이 다양한 URL과 연결되어 있는 반면 일반 이슈에는 None 항목이 있음을 알 수 있습니다. 이 구분을 사용하여 pull_request 필드가 None인지 여부를 확인하는 새로운 is_pull_request 열을 만들 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17d740e8-7ed2-45ea-80b8-e8295293792e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shuffled_dataset = shuffled_dataset.map(lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dea5a7ae-51b4-4d15-bcb8-c51219e53da7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/5007',\n",
       " 'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       " 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/5007/labels{/name}',\n",
       " 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/5007/comments',\n",
       " 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/5007/events',\n",
       " 'html_url': 'https://github.com/huggingface/datasets/pull/5007',\n",
       " 'id': 1381007607,\n",
       " 'node_id': 'PR_kwDODunzps4_WvFQ',\n",
       " 'number': 5007,\n",
       " 'title': 'Add some note about running the transformers ci before a release',\n",
       " 'user': {'login': 'lhoestq',\n",
       "  'id': 42851186,\n",
       "  'node_id': 'MDQ6VXNlcjQyODUxMTg2',\n",
       "  'avatar_url': 'https://avatars.githubusercontent.com/u/42851186?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/lhoestq',\n",
       "  'html_url': 'https://github.com/lhoestq',\n",
       "  'followers_url': 'https://api.github.com/users/lhoestq/followers',\n",
       "  'following_url': 'https://api.github.com/users/lhoestq/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/lhoestq/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/lhoestq/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/lhoestq/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/lhoestq/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/lhoestq/repos',\n",
       "  'events_url': 'https://api.github.com/users/lhoestq/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/lhoestq/received_events',\n",
       "  'type': 'User',\n",
       "  'site_admin': False},\n",
       " 'labels': [],\n",
       " 'state': 'closed',\n",
       " 'locked': False,\n",
       " 'assignee': None,\n",
       " 'assignees': [],\n",
       " 'milestone': None,\n",
       " 'comments': 1,\n",
       " 'created_at': datetime.datetime(2022, 9, 21, 14, 14, 25),\n",
       " 'updated_at': datetime.datetime(2022, 9, 22, 10, 16, 14),\n",
       " 'closed_at': datetime.datetime(2022, 9, 22, 10, 14, 6),\n",
       " 'author_association': 'MEMBER',\n",
       " 'active_lock_reason': None,\n",
       " 'draft': False,\n",
       " 'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/5007',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/5007',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/5007.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/5007.patch',\n",
       "  'merged_at': datetime.datetime(2022, 9, 22, 10, 14, 6)},\n",
       " 'body': None,\n",
       " 'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/5007/reactions',\n",
       "  'total_count': 0,\n",
       "  '+1': 0,\n",
       "  '-1': 0,\n",
       "  'laugh': 0,\n",
       "  'hooray': 0,\n",
       "  'confused': 0,\n",
       "  'heart': 0,\n",
       "  'rocket': 0,\n",
       "  'eyes': 0},\n",
       " 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/5007/timeline',\n",
       " 'performed_via_github_app': None,\n",
       " 'state_reason': None,\n",
       " 'is_pull_request': True}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(shuffled_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ddbc1084-6d2b-4353-be12-3acce5a54687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/7119',\n",
       " 'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       " 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/labels{/name}',\n",
       " 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/comments',\n",
       " 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/events',\n",
       " 'html_url': 'https://github.com/huggingface/datasets/pull/7119',\n",
       " 'id': 2477766493,\n",
       " 'node_id': 'PR_kwDODunzps54-GjY',\n",
       " 'number': 7119,\n",
       " 'title': 'Install transformers with numpy-2 CI',\n",
       " 'user': {'login': 'albertvillanova',\n",
       "  'id': 8515462,\n",
       "  'node_id': 'MDQ6VXNlcjg1MTU0NjI=',\n",
       "  'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/albertvillanova',\n",
       "  'html_url': 'https://github.com/albertvillanova',\n",
       "  'followers_url': 'https://api.github.com/users/albertvillanova/followers',\n",
       "  'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/albertvillanova/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/albertvillanova/repos',\n",
       "  'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/albertvillanova/received_events',\n",
       "  'type': 'User',\n",
       "  'site_admin': False},\n",
       " 'labels': [],\n",
       " 'state': 'closed',\n",
       " 'locked': False,\n",
       " 'assignee': None,\n",
       " 'assignees': [],\n",
       " 'milestone': None,\n",
       " 'comments': 2,\n",
       " 'created_at': datetime.datetime(2024, 8, 21, 11, 14, 59),\n",
       " 'updated_at': datetime.datetime(2024, 8, 21, 11, 42, 35),\n",
       " 'closed_at': datetime.datetime(2024, 8, 21, 11, 36, 50),\n",
       " 'author_association': 'MEMBER',\n",
       " 'active_lock_reason': None,\n",
       " 'draft': False,\n",
       " 'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/7119',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/7119',\n",
       "  'diff_url': 'https://github.com/huggingface/datasets/pull/7119.diff',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/7119.patch',\n",
       "  'merged_at': datetime.datetime(2024, 8, 21, 11, 36, 50)},\n",
       " 'body': 'Install transformers with numpy-2 CI.\\r\\n\\r\\nNote that transformers no longer pins numpy < 2 since transformers-4.43.0:\\r\\n- https://github.com/huggingface/transformers/pull/32018\\r\\n- https://github.com/huggingface/transformers/releases/tag/v4.43.0',\n",
       " 'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/reactions',\n",
       "  'total_count': 0,\n",
       "  '+1': 0,\n",
       "  '-1': 0,\n",
       "  'laugh': 0,\n",
       "  'hooray': 0,\n",
       "  'confused': 0,\n",
       "  'heart': 0,\n",
       "  'rocket': 0,\n",
       "  'eyes': 0},\n",
       " 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/7119/timeline',\n",
       " 'performed_via_github_app': None,\n",
       " 'state_reason': None,\n",
       " 'is_pull_request': True}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#issues_dataset_streamed = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"train\", streaming=True)\n",
    "issues_dataset_column_added = issues_dataset_streamed.map(lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True})\n",
    "next(iter(issues_dataset_column_added))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da1e26-d2a6-4a7f-a5c9-d6b6c707831b",
   "metadata": {},
   "source": [
    "### Create the Issues dataset with comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0057ab7-8965-4352-8fdd-0aa211cea4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=10000,\n",
    "    rate_limit=5000,\n",
    "    issues_path=Path(\".\")\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100    # 페이지당 리턴받는 이슈의 개수\n",
    "    #per_page = 50\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # state=all로 질의하여 미해결, 해결 이슈들을 모두 가지고 온다.\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []    # 다음 주기를 위해서 batch를 비운다.\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21110497-b2c1-4626-b5c9-66785d50b2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  \\\n",
      "0  https://api.github.com/repos/huggingface/datas...   \n",
      "1  https://api.github.com/repos/huggingface/datas...   \n",
      "2  https://api.github.com/repos/huggingface/datas...   \n",
      "3  https://api.github.com/repos/huggingface/datas...   \n",
      "4  https://api.github.com/repos/huggingface/datas...   \n",
      "\n",
      "                                      repository_url  \\\n",
      "0  https://api.github.com/repos/huggingface/datasets   \n",
      "1  https://api.github.com/repos/huggingface/datasets   \n",
      "2  https://api.github.com/repos/huggingface/datasets   \n",
      "3  https://api.github.com/repos/huggingface/datasets   \n",
      "4  https://api.github.com/repos/huggingface/datasets   \n",
      "\n",
      "                                          labels_url  \\\n",
      "0  https://api.github.com/repos/huggingface/datas...   \n",
      "1  https://api.github.com/repos/huggingface/datas...   \n",
      "2  https://api.github.com/repos/huggingface/datas...   \n",
      "3  https://api.github.com/repos/huggingface/datas...   \n",
      "4  https://api.github.com/repos/huggingface/datas...   \n",
      "\n",
      "                                        comments_url  \\\n",
      "0  https://api.github.com/repos/huggingface/datas...   \n",
      "1  https://api.github.com/repos/huggingface/datas...   \n",
      "2  https://api.github.com/repos/huggingface/datas...   \n",
      "3  https://api.github.com/repos/huggingface/datas...   \n",
      "4  https://api.github.com/repos/huggingface/datas...   \n",
      "\n",
      "                                          events_url  \\\n",
      "0  https://api.github.com/repos/huggingface/datas...   \n",
      "1  https://api.github.com/repos/huggingface/datas...   \n",
      "2  https://api.github.com/repos/huggingface/datas...   \n",
      "3  https://api.github.com/repos/huggingface/datas...   \n",
      "4  https://api.github.com/repos/huggingface/datas...   \n",
      "\n",
      "                                            html_url          id  \\\n",
      "0  https://github.com/huggingface/datasets/pull/7119  2477766493   \n",
      "1  https://github.com/huggingface/datasets/pull/7118  2477676893   \n",
      "2  https://github.com/huggingface/datasets/issues...  2476555659   \n",
      "3  https://github.com/huggingface/datasets/issues...  2475522721   \n",
      "4  https://github.com/huggingface/datasets/issues...  2475363142   \n",
      "\n",
      "               node_id  number  \\\n",
      "0  PR_kwDODunzps54-GjY    7119   \n",
      "1  PR_kwDODunzps549yu4    7118   \n",
      "2   I_kwDODunzps6TnT2L    7117   \n",
      "3   I_kwDODunzps6TjXqh    7116   \n",
      "4   I_kwDODunzps6TiwtG    7115   \n",
      "\n",
      "                                               title  ...  \\\n",
      "0               Install transformers with numpy-2 CI  ...   \n",
      "1    Allow numpy-2.1 and test it without audio extra  ...   \n",
      "2  Audio dataset load everything in RAM and is ve...  ...   \n",
      "3  datasets cannot handle nested json if features...  ...   \n",
      "4  module 'pyarrow.lib' has no attribute 'ListVie...  ...   \n",
      "\n",
      "                  closed_at author_association active_lock_reason  draft  \\\n",
      "0 2024-08-21 11:36:50+00:00             MEMBER                NaN    0.0   \n",
      "1 2024-08-21 10:58:15+00:00             MEMBER                NaN    0.0   \n",
      "2                       NaT               NONE                NaN    NaN   \n",
      "3                       NaT               NONE                NaN    NaN   \n",
      "4                       NaT               NONE                NaN    NaN   \n",
      "\n",
      "                                        pull_request  \\\n",
      "0  {'url': 'https://api.github.com/repos/huggingf...   \n",
      "1  {'url': 'https://api.github.com/repos/huggingf...   \n",
      "2                                               None   \n",
      "3                                               None   \n",
      "4                                               None   \n",
      "\n",
      "                                                body  \\\n",
      "0  Install transformers with numpy-2 CI.\\r\\n\\r\\nN...   \n",
      "1  Allow numpy-2.1 and test it without audio extr...   \n",
      "2  Hello, I'm working with an audio dataset. I wa...   \n",
      "3  ### Describe the bug\\n\\nI have a json named te...   \n",
      "4  ### Describe the bug\\n\\nCode:\\r\\n`!pipuninstal...   \n",
      "\n",
      "                                           reactions  \\\n",
      "0  {'url': 'https://api.github.com/repos/huggingf...   \n",
      "1  {'url': 'https://api.github.com/repos/huggingf...   \n",
      "2  {'url': 'https://api.github.com/repos/huggingf...   \n",
      "3  {'url': 'https://api.github.com/repos/huggingf...   \n",
      "4  {'url': 'https://api.github.com/repos/huggingf...   \n",
      "\n",
      "                                        timeline_url performed_via_github_app  \\\n",
      "0  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
      "1  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
      "2  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
      "3  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
      "4  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
      "\n",
      "  state_reason  \n",
      "0         None  \n",
      "1         None  \n",
      "2         None  \n",
      "3         None  \n",
      "4         None  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Dataset({\n",
      "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason'],\n",
      "    num_rows: 7076\n",
      "})\n",
      ">> URL: https://github.com/huggingface/datasets/pull/145\n",
      ">> Pull request: {'diff_url': 'https://github.com/huggingface/datasets/pull/145.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/145', 'merged_at': '2020-05-16T13:54:22Z', 'patch_url': 'https://github.com/huggingface/datasets/pull/145.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/145'}\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/7072\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/1843\n",
      ">> Pull request: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json(\"datasets-issues.jsonl\", lines=True)\n",
    "print(df.head())\n",
    "\n",
    "from datasets import Dataset\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "print(issues_dataset)\n",
    "\n",
    "sample = issues_dataset.shuffle(seed=66).select(range(3))\n",
    "\n",
    "# Print out the URL and pull request entries\n",
    "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Pull request: {pr}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb940031-b485-4fbb-88bb-35fe41862b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/145', 'repository_url': 'https://api.github.com/repos/huggingface/datasets', 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/145/labels{/name}', 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/145/comments', 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/145/events', 'html_url': 'https://github.com/huggingface/datasets/pull/145', 'id': 619480549, 'node_id': 'MDExOlB1bGxSZXF1ZXN0NDE4OTcxMjg0', 'number': 145, 'title': '[AWS Tests] Follow-up PR from #144', 'user': {'avatar_url': 'https://avatars.githubusercontent.com/u/23423619?v=4', 'events_url': 'https://api.github.com/users/patrickvonplaten/events{/privacy}', 'followers_url': 'https://api.github.com/users/patrickvonplaten/followers', 'following_url': 'https://api.github.com/users/patrickvonplaten/following{/other_user}', 'gists_url': 'https://api.github.com/users/patrickvonplaten/gists{/gist_id}', 'gravatar_id': '', 'html_url': 'https://github.com/patrickvonplaten', 'id': 23423619, 'login': 'patrickvonplaten', 'node_id': 'MDQ6VXNlcjIzNDIzNjE5', 'organizations_url': 'https://api.github.com/users/patrickvonplaten/orgs', 'received_events_url': 'https://api.github.com/users/patrickvonplaten/received_events', 'repos_url': 'https://api.github.com/users/patrickvonplaten/repos', 'site_admin': False, 'starred_url': 'https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/patrickvonplaten/subscriptions', 'type': 'User', 'url': 'https://api.github.com/users/patrickvonplaten'}, 'labels': [], 'state': 'closed', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': Timestamp('2020-05-16 13:53:46+0000', tz='UTC'), 'updated_at': Timestamp('2020-05-16 13:54:23+0000', tz='UTC'), 'closed_at': Timestamp('2020-05-16 13:54:22+0000', tz='UTC'), 'author_association': 'CONTRIBUTOR', 'active_lock_reason': None, 'draft': 0.0, 'pull_request': {'diff_url': 'https://github.com/huggingface/datasets/pull/145.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/145', 'merged_at': '2020-05-16T13:54:22Z', 'patch_url': 'https://github.com/huggingface/datasets/pull/145.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/145'}, 'body': 'I forgot to add this line in PR #145 . ', 'reactions': {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, 'heart': 0, 'hooray': 0, 'laugh': 0, 'rocket': 0, 'total_count': 0, 'url': 'https://api.github.com/repos/huggingface/datasets/issues/145/reactions'}, 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/145/timeline', 'performed_via_github_app': None, 'state_reason': None}\n"
     ]
    }
   ],
   "source": [
    "print(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef56df8-841c-4aa1-bef8-515099376d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#issues_dataset[\"number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e9f7f12-b140-4bc8-8d5e-548c7feb59ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7076\n"
     ]
    }
   ],
   "source": [
    "issues_number_list = issues_dataset[\"number\"]\n",
    "print(len(issues_number_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57a60df5-a27f-4f0e-a5ea-63b2f8b0d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = \"#######################\"   ## 본인의 personal access token을 지정하세요.\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4131d7-8d98-45de-9650-072eed9f54e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74df82fa641d43f68da6b0f296d7fe96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7076 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached GitHub rate limit. Sleeping for one hour ...\n",
      "comment_issues length: 7076\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "\n",
    "def get_comments(issue_number):\n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return[r[\"body\"] for r in response.json()]\n",
    "\n",
    "\n",
    "def fetch_issues_comments(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=10000,\n",
    "    #rate_limit=5000,\n",
    "    rate_limit=4000\n",
    "):\n",
    "      \n",
    "    batch = []\n",
    "    all_comment_issues = []\n",
    "    per_page = 100    # 페이지당 리턴받는 이슈의 개수\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    #url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "\n",
    "    for issue_no in tqdm(issues_number_list):\n",
    "        # state=all로 질의하여 미해결, 해결 이슈들을 모두 가지고 온다.\n",
    "        query = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_no}/comments\"\n",
    "        response = requests.get(query, headers=headers)\n",
    "        comments = [r[\"body\"] for r in response.json()]\n",
    "        #print(f\"{issue_no}: comment length: {len(comments)}\")\n",
    "        batch.extend([comments])\n",
    "        #print(f\"batch length: {len(batch)}\")\n",
    "        #print(batch)\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_comment_issues) < num_issues:\n",
    "            all_comment_issues.extend(batch)\n",
    "            batch = []    # 다음 주기를 위해서 batch를 비운다.\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_comment_issues.extend(batch)\n",
    "    print(f\"comment_issues length: {len(all_comment_issues)}\")\n",
    "    return(all_comment_issues)\n",
    "\n",
    "\n",
    "df['comments'] = fetch_issues_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8654ce9b-36c1-473e-a278-a72f6ab9efec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason'],\n",
      "    num_rows: 7076\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "print(issues_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419f46d0-85d2-4938-b746-e2f0083ba57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e622a1a44a234d3c9417dad29142de48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7076 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'is_pull_request'],\n",
       "    num_rows: 7076\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.map(\n",
    "    lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True}\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cd4fc76-8024-4959-a10a-b0ea4a83f732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> URL: https://github.com/huggingface/datasets/pull/145\n",
      ">> Pull request: {'diff_url': 'https://github.com/huggingface/datasets/pull/145.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/145', 'merged_at': '2020-05-16T13:54:22Z', 'patch_url': 'https://github.com/huggingface/datasets/pull/145.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/145'}\n",
      "\n",
      ">> Comments: []\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/7072\n",
      ">> Pull request: None\n",
      "\n",
      ">> Comments: []\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/1843\n",
      ">> Pull request: None\n",
      "\n",
      ">> Comments: ['Hi @patrickvonplaten  I would like to work on this dataset. \\r\\n\\r\\nThanks! ', \"That's awesome! Actually, I just noticed that this dataset might become a bit too big!\\r\\n\\r\\nMuST-C is the main dataset used for IWSLT19 and should probably be added as a standalone dataset. Would you be interested also in adding `datasets/MuST-C` instead?\\r\\n\\r\\nDescription: \\r\\n_MuST-C is a multilingual speech translation corpus whose size and quality facilitates the training of end-to-end systems for speech translation from English into several languages. For each target language, MuST-C comprises several hundred hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations._\\r\\n\\r\\nPaper: https://www.aclweb.org/anthology/N19-1202.pdf\\r\\n\\r\\nDataset: https://ict.fbk.eu/must-c/ (One needs to fill out a short from to download the data, but it's very easy).\\r\\n\\r\\nIt would be awesome if you're interested in adding this datates. I'm very happy to guide you through the PR! I think the easiest way to start would probably be to read [this README on how to add a dataset](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md) and open a PR. Think you can copy & paste some code from:\\r\\n\\r\\n- Librispeech_asr: https://github.com/huggingface/datasets/blob/master/datasets/librispeech_asr/librispeech_asr.py\\r\\n- Flores Translation: https://github.com/huggingface/datasets/blob/master/datasets/flores/flores.py\\r\\n\\r\\nThink all the rest can be handled on the PR :-) \", 'Hi @patrickvonplaten \\r\\nI have tried downloading this dataset, but the connection seems to reset all the time. I have tried it via the browser, wget, and using gdown . But it gives me an error message. _\"The server is busy or down, pls try again\"_ (rephrasing the message here)\\r\\n\\r\\nI have completed adding 4 datasets in the previous data sprint (including the IWSLT dataset #1676 ) ...so just checking if you are able to download it at your end. Otherwise will write to the dataset authors to update the links. \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n', 'Let me check tomorrow! Thanks for leaving this message!', 'cc @patil-suraj for notification ', \"@skyprince999, I think I'm getting the same error you're getting :-/\\r\\n\\r\\n```\\r\\nSorry, you can't view or download this file at this time.\\r\\n\\r\\nToo many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can't access a file after 24 hours, contact your domain administrator.\\r\\n```\\r\\n\\r\\nIt would be great if you could write the authors to see whether they can fix it.\\r\\nAlso cc @lhoestq - do you think we could mirror the dataset? \", \"Also there are huge those datasets. Think downloading MuST-C v1.2 amounts to ~ 1000GB... because there are 14 possible configs each around 60-70GB. I think users mostly will only use one of the 14 configs so that they would only need, in theory, will have to download ~60GB which is ok. But I think this functionality doesn't exist yet in `datasets` no? cc @lhoestq \", \"> Also cc @lhoestq - do you think we could mirror the dataset?\\r\\n\\r\\nYes we can mirror it if the authors are fine with it. You can create a dataset repo on huggingface.co (possibly under the relevant org) and add the mirrored data files.\\r\\n\\r\\n> I think users mostly will only use one of the 14 configs so that they would only need, in theory, will have to download ~60GB which is ok. But I think this functionality doesn't exist yet in datasets no? cc @lhoestq\\r\\n\\r\\nIf there are different download links for each configuration we can make the dataset builder download only the files related to the requested configuration.\", \"I have written to the dataset authors, highlighting this issue. Waiting for their response. \\r\\n\\r\\nUpdate on 25th Feb: \\r\\nThe authors have replied back, they are updating the download link and will revert back shortly! \\r\\n\\r\\n```\\r\\nfirst of all thanks a lot for being interested in MuST-C and for building the data-loader.\\r\\n\\r\\nBefore answering your request, I'd like to clarify that the creation, maintenance, and expansion of MuST-c are not supported by any funded project, so this means that we need to find economic support for all these activities. This also includes permanently moving all the data to AWS or GCP.  We are working at this with the goal of facilitating the use of MuST-C, but this is not something that can happen today. We hope to have some news ASAP and you will be among the first to be informed.\\r\\n\\r\\nI hope you understand our situation.\\r\\n```\\r\\n\\r\\n\", \"Awesome, actually @lhoestq let's just ask the authors if we should host the dataset no? They could just use our links then as well for their website - what do you think? Is it fine to use our AWS dataset storage also as external links? \", 'Yes definitely. Shall we suggest them to create a dataset repository under their org on huggingface.co ? @julien-c \\r\\nThe dataset is around 1TB', 'Sounds good! \\r\\n\\r\\nOrder of magnitude is storage costs ~$20 per TB per month (not including bandwidth). \\r\\n\\r\\nHappy to provide this to the community as I feel this is an important dataset. Let us know what the authors want to do!\\r\\n\\r\\n', 'Great! @skyprince999, do you think you could ping the authors here or link to this thread? I think it could be a cool idea to host the dataset on our side then', 'Done. They replied back, and they want to have a call over a meet/ skype. Is that possible ? \\r\\nBtw @patrickvonplaten you are looped in that email (_pls check you gmail account_)  ', 'Hello! Any news on this?', \"@gegallego  there were some concerns regarding dataset usage & attribution by a for-profit company, so couldn't take it forward. Also the download links were unstable. \\r\\nBut I guess if you want to test the fairseq benchmarks, you can connect with them directly for downloading the dataset.  \", 'Yes, that dataset is not easy to download... I had to copy it to my Google Drive and use `rsync` to be able to download it.\\r\\nHowever, we could add the dataset with a manual download, right?', \"yes that is possible. I couldn't unfortunately complete this PR, If you would like to add it, please feel free to do it. \"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = issues_dataset.shuffle(seed=66).select(range(3))\n",
    "\n",
    "# Print out the URL and pull request entries\n",
    "for url, pr, comments in zip(sample[\"html_url\"], sample[\"pull_request\"], sample[\"comments\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Pull request: {pr}\\n\")\n",
    "    print(f\">> Comments: {comments}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "517dc649-b11b-455b-a279-ed81a63593d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b63b1824c3413aaf204b3c0b4b9995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "46285055"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset.to_json(\"datasets-issues-with-comments.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "759d1300-cd56-447b-b668-85289ca38415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'is_pull_request'],\n",
       "    num_rows: 7076\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]\n",
    "\n",
    "df.to_json(\"datasets-issues-with-comments-1.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "issues_dataset.reset_format()\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f064b8c-5ade-4977-bb2f-37f62cc580d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets on Hub: 104\n",
      "DatasetInfo(id='amirveyseh/acronym_identification', author='amirveyseh', sha='15ef643450d589d5883e289ffadeb03563e80a9e', created_at=datetime.datetime(2022, 3, 2, 23, 29, 22, tzinfo=datetime.timezone.utc), last_modified=datetime.datetime(2024, 1, 9, 11, 39, 57, tzinfo=datetime.timezone.utc), private=False, gated=False, disabled=False, downloads=179, likes=19, paperswithcode_id='acronym-identification', tags=['task_categories:token-classification', 'annotations_creators:expert-generated', 'language_creators:found', 'multilinguality:monolingual', 'source_datasets:original', 'language:en', 'license:mit', 'size_categories:10K<n<100K', 'format:parquet', 'modality:text', 'library:datasets', 'library:pandas', 'library:mlcroissant', 'library:polars', 'arxiv:2010.14678', 'region:us', 'acronym-identification'], card_data=None, siblings=None)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from huggingface_hub import list_datasets\n",
    "\n",
    "all_datasets = list_datasets()\n",
    "print(f\"Number of datasets on Hub: {sys.getsizeof(all_datasets)}\")\n",
    "print(next(all_datasets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4138c3ef-05e9-4146-b32e-bb04f0596fbb",
   "metadata": {},
   "source": [
    "#### Upload dataset-issues-with-comments to Hugging Face Dataset\n",
    "\n",
    "```\n",
    "from huggingface_hub import notebook_login\n",
    " notebook_login()\n",
    "\n",
    "from huggingface_hub import create_repo\n",
    " repo_url = create_repo(\"huggingface-datasets-issues-2024-03-20\", repo_type=\"dataset\")\n",
    "\n",
    "from huggingface_hub import Repository\r",
    " repo = Repository(local_dir=\"huggingface-datasets-issues-2024-03-20\", clone_from=repo_url)\n",
    "\n",
    "!cp datasets-issues-with-comments.jsonl huggingface-datasets-issues-2024-03-20/\n",
    "repo.lfs_track(\"*.jsonl\")\n",
    "repo.push_to_hub()\n",
    "```\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51b822a0-e393-4ca0-a850-6027b6da148e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5160f2fcc354571b4f4af5d713affcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1989\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_writer.py:574\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    573\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mcombine_chunks()\n\u001b[0;32m--> 574\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_local_files:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2322\u001b[0m, in \u001b[0;36mtable_cast\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m!=\u001b[39m schema:\n\u001b[0;32m-> 2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m!=\u001b[39m schema\u001b[38;5;241m.\u001b[39mmetadata:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2281\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[1;32m   2277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2278\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m   2279\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[1;32m   2280\u001b[0m     )\n\u001b[0;32m-> 2281\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2281\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[1;32m   2277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2278\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m   2279\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[1;32m   2280\u001b[0m     )\n\u001b[0;32m-> 2281\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1797\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray):\n\u001b[0;32m-> 1797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([func(chunk, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1797\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray):\n\u001b[0;32m-> 1797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2040\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 2040\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [_c(array\u001b[38;5;241m.\u001b[39mfield(name), subfeature) \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2040\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 2040\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2110\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, (Sequence, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 2110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_number_to_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_number_to_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1992\u001b[0m, in \u001b[0;36marray_cast\u001b[0;34m(array, pa_type, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(pa_type) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(array\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m-> 1992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpa_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\u001b[38;5;241m.\u001b[39mcast(pa_type)\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't cast array of type timestamp[s] to null",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m issues_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhwang2006/huggingface-datasets-issues-2024-03-20\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:2549\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2546\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2549\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2556\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2559\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2560\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2561\u001b[0m )\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1005\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1100\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1107\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1860\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1858\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1860\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1861\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1862\u001b[0m     ):\n\u001b[1;32m   1863\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1864\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:2016\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[1;32m   2015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "issues_dataset = load_dataset(\"hwang2006/huggingface-datasets-issues-2024-03-20\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56a26ada-d53d-4b9c-9fc5-38194b0d35fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/hwang2006/huggingface-datasets-issues-2024-03-20/resolve/main/datasets-issues-with-comments.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repository_url</th>\n",
       "      <th>labels_url</th>\n",
       "      <th>comments_url</th>\n",
       "      <th>events_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>author_association</th>\n",
       "      <th>active_lock_reason</th>\n",
       "      <th>body</th>\n",
       "      <th>reactions</th>\n",
       "      <th>timeline_url</th>\n",
       "      <th>performed_via_github_app</th>\n",
       "      <th>state_reason</th>\n",
       "      <th>draft</th>\n",
       "      <th>pull_request</th>\n",
       "      <th>is_pull_request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2193172074</td>\n",
       "      <td>I_kwDODunzps6CuSZq</td>\n",
       "      <td>6740</td>\n",
       "      <td>Support for loading geotiff files as a part of...</td>\n",
       "      <td>...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>### Feature request\\n\\nRequest for adding rast...</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/pull/6739</td>\n",
       "      <td>2192730134</td>\n",
       "      <td>PR_kwDODunzps5p-Bwe</td>\n",
       "      <td>6739</td>\n",
       "      <td>Transpose images with EXIF Orientation tag</td>\n",
       "      <td>...</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Closes https://github.com/huggingface/datasets...</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'diff_url': 'https://github.com/huggingface/d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2192386536</td>\n",
       "      <td>I_kwDODunzps6CrSno</td>\n",
       "      <td>6738</td>\n",
       "      <td>Dict feature is non-nullable while nested dict...</td>\n",
       "      <td>...</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When i try to create a `Dataset` object with N...</td>\n",
       "      <td>{'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2190198425</td>\n",
       "      <td>I_kwDODunzps6Ci8aZ</td>\n",
       "      <td>6737</td>\n",
       "      <td>Invalid pattern: '**' can only be an entire pa...</td>\n",
       "      <td>...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>### Describe the bug\\n\\nValueError: Invalid pa...</td>\n",
       "      <td>{'+1': 2, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datasets</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>2190181422</td>\n",
       "      <td>I_kwDODunzps6Ci4Qu</td>\n",
       "      <td>6736</td>\n",
       "      <td>Mosaic Streaming (MDS) Support</td>\n",
       "      <td>...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>### Feature request\\n\\nI'm a huge fan of the c...</td>\n",
       "      <td>{'+1': 1, '-1': 0, 'confused': 0, 'eyes': 0, '...</td>\n",
       "      <td>https://api.github.com/repos/huggingface/datas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                      repository_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datasets   \n",
       "1  https://api.github.com/repos/huggingface/datasets   \n",
       "2  https://api.github.com/repos/huggingface/datasets   \n",
       "3  https://api.github.com/repos/huggingface/datasets   \n",
       "4  https://api.github.com/repos/huggingface/datasets   \n",
       "\n",
       "                                          labels_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                        comments_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                          events_url  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...   \n",
       "1  https://api.github.com/repos/huggingface/datas...   \n",
       "2  https://api.github.com/repos/huggingface/datas...   \n",
       "3  https://api.github.com/repos/huggingface/datas...   \n",
       "4  https://api.github.com/repos/huggingface/datas...   \n",
       "\n",
       "                                            html_url          id  \\\n",
       "0  https://github.com/huggingface/datasets/issues...  2193172074   \n",
       "1  https://github.com/huggingface/datasets/pull/6739  2192730134   \n",
       "2  https://github.com/huggingface/datasets/issues...  2192386536   \n",
       "3  https://github.com/huggingface/datasets/issues...  2190198425   \n",
       "4  https://github.com/huggingface/datasets/issues...  2190181422   \n",
       "\n",
       "               node_id  number  \\\n",
       "0   I_kwDODunzps6CuSZq    6740   \n",
       "1  PR_kwDODunzps5p-Bwe    6739   \n",
       "2   I_kwDODunzps6CrSno    6738   \n",
       "3   I_kwDODunzps6Ci8aZ    6737   \n",
       "4   I_kwDODunzps6Ci4Qu    6736   \n",
       "\n",
       "                                               title  ... author_association  \\\n",
       "0  Support for loading geotiff files as a part of...  ...               NONE   \n",
       "1         Transpose images with EXIF Orientation tag  ...        CONTRIBUTOR   \n",
       "2  Dict feature is non-nullable while nested dict...  ...        CONTRIBUTOR   \n",
       "3  Invalid pattern: '**' can only be an entire pa...  ...               NONE   \n",
       "4                     Mosaic Streaming (MDS) Support  ...               NONE   \n",
       "\n",
       "  active_lock_reason                                               body  \\\n",
       "0                NaN  ### Feature request\\n\\nRequest for adding rast...   \n",
       "1                NaN  Closes https://github.com/huggingface/datasets...   \n",
       "2                NaN  When i try to create a `Dataset` object with N...   \n",
       "3                NaN  ### Describe the bug\\n\\nValueError: Invalid pa...   \n",
       "4                NaN  ### Feature request\\n\\nI'm a huge fan of the c...   \n",
       "\n",
       "                                           reactions  \\\n",
       "0  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "1  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "2  {'+1': 0, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "3  {'+1': 2, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "4  {'+1': 1, '-1': 0, 'confused': 0, 'eyes': 0, '...   \n",
       "\n",
       "                                        timeline_url performed_via_github_app  \\\n",
       "0  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "1  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "2  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "3  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "4  https://api.github.com/repos/huggingface/datas...                      NaN   \n",
       "\n",
       "  state_reason draft                                       pull_request  \\\n",
       "0         None   NaN                                               None   \n",
       "1         None   0.0  {'diff_url': 'https://github.com/huggingface/d...   \n",
       "2         None   NaN                                               None   \n",
       "3         None   NaN                                               None   \n",
       "4         None   NaN                                               None   \n",
       "\n",
       "  is_pull_request  \n",
       "0           False  \n",
       "1            True  \n",
       "2           False  \n",
       "3           False  \n",
       "4           False  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_url\n",
    "import pandas as pd\n",
    "\n",
    "# returns 'https://huggingface.co/datasets/Evan/spaCy-github-issues/resolve/main/spacy-issues.jsonl'\n",
    "data_files = hf_hub_url(repo_id=\"hwang2006/huggingface-datasets-issues-2024-03-20\", filename=\"datasets-issues-with-comments.jsonl\", repo_type=\"dataset\")\n",
    "print(data_files)\n",
    "# throws TypeError: Couldn't cast array of type\n",
    "#dset = load_dataset(\"json\", data_files=data_files, split=\"test\")\n",
    "# no problem with pandas - note this take a while as the file is >2GB\n",
    "df = pd.read_json(data_files, orient=\"records\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c983e8e-0656-4cf6-8b65-ea39f8291366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json(\"datasets-issues.jsonl\", lines=True)\n",
    "df.head()\n",
    "\n",
    "from datasets import Dataset\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "print(issues_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23ddd2-4a3f-4590-911c-bc76f076e125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "issue_number = 4996\n",
    "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b3d2665-0f61-4f3e-80a6-94faec0d31b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The script uses `Dataset.load_from_disk`, which as you can expect, doesn't work in streaming mode.\\r\\n\\r\\nIt would probably be more practical to load the dataset locally using `Dataset.load_from_disk` first and then `push_to_hub` to upload it in Parquet on the Hub\",\n",
       " \"I've transferred this issue to the Hub repo: https://huggingface.co/datasets/Jean-Baptiste/wikiner_fr/discussions/3\\r\\n\\r\\nI'm closing this.\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_comments(issue_number):\n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return[r[\"body\"] for r in response.json()]\n",
    "\n",
    "get_comments(4996)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02b5a7fb-d149-4c06-8c0a-6261299b9a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf59182862e24283bf31eb32c37a7b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6707 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Depending on your internet connection, this can take a few minutes...\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m issues_with_comments_dataset \u001b[38;5;241m=\u001b[39m \u001b[43missues_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_comments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumber\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_dataset.py:3093\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3088\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3089\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3090\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3091\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3092\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3093\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3094\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3095\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_dataset.py:3446\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3444\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3446\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3448\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_dataset.py:3349\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3348\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3349\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3351\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3352\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3353\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Depending on your internet connection, this can take a few minutes...\u001b[39;00m\n\u001b[1;32m      2\u001b[0m issues_with_comments_dataset \u001b[38;5;241m=\u001b[39m issues_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomments\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mget_comments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumber\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m      4\u001b[0m )\n",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m, in \u001b[0;36mget_comments\u001b[0;34m(issue_number)\u001b[0m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.github.com/repos/huggingface/datasets/issues/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00missue_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/comments\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m[r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/requests/models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# Depending on your internet connection, this can take a few minutes...\n",
    "issues_with_comments_dataset = issues_dataset.map(\n",
    "    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc53e3-8c8d-45d1-9c1e-549a2234f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_with_comments_dataset.to_json(\"datasets-issues-with-comments.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4df1f27a-1542-42e6-94f4-2301751c416b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/6542',\n",
       " 'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       " 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/6542/labels{/name}',\n",
       " 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/6542/comments',\n",
       " 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/6542/events',\n",
       " 'html_url': 'https://github.com/huggingface/datasets/issues/6542',\n",
       " 'id': 2059198575,\n",
       " 'node_id': 'I_kwDODunzps56vOBv',\n",
       " 'number': 6542,\n",
       " 'title': 'Datasets : wikipedia 20220301.en error ',\n",
       " 'user': {'login': 'ppx666',\n",
       "  'id': 53203620,\n",
       "  'node_id': 'MDQ6VXNlcjUzMjAzNjIw',\n",
       "  'avatar_url': 'https://avatars.githubusercontent.com/u/53203620?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/ppx666',\n",
       "  'html_url': 'https://github.com/ppx666',\n",
       "  'followers_url': 'https://api.github.com/users/ppx666/followers',\n",
       "  'following_url': 'https://api.github.com/users/ppx666/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/ppx666/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/ppx666/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/ppx666/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/ppx666/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/ppx666/repos',\n",
       "  'events_url': 'https://api.github.com/users/ppx666/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/ppx666/received_events',\n",
       "  'type': 'User',\n",
       "  'site_admin': False},\n",
       " 'labels': [],\n",
       " 'state': 'open',\n",
       " 'locked': False,\n",
       " 'assignee': None,\n",
       " 'assignees': [],\n",
       " 'milestone': None,\n",
       " 'comments': [],\n",
       " 'created_at': datetime.datetime(2023, 12, 29, 8, 34, 51),\n",
       " 'updated_at': datetime.datetime(2023, 12, 29, 8, 34, 51),\n",
       " 'closed_at': None,\n",
       " 'author_association': 'NONE',\n",
       " 'active_lock_reason': None,\n",
       " 'body': '### Describe the bug\\n\\nWhen I used load_dataset to download this data set, the following error occurred. The main problem was that the target data did not exist.\\n\\n### Steps to reproduce the bug\\n\\n1.I tried downloading directly.\\r\\n```python\\r\\nwiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\")\\r\\n```\\r\\nAn exception occurred\\r\\n```\\r\\nMissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/\\r\\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \\r\\nExample of usage: \\r\\n\\t`load_dataset(\\'wikipedia\\', \\'20220301.en\\', beam_runner=\\'DirectRunner\\')`\\r\\n```\\r\\n2.I modified the code as prompted.\\r\\n```python\\r\\nwiki_dataset = load_dataset(\\'wikipedia\\', \\'20220301.en\\', beam_runner=\\'DirectRunner\\')\\r\\n```\\r\\nAn exception occurred:\\r\\n```\\r\\nFileNotFoundError: Couldn\\'t find file at https://dumps.wikimedia.org/enwiki/20220301/dumpstatus.json\\r\\n```\\r\\n\\n\\n### Expected behavior\\n\\nI searched in the parent directory of the corresponding URL, but there was no corresponding \"20220301\" directory.\\r\\nI really need this data set and hope to provide a download method.\\n\\n### Environment info\\n\\npython 3.8\\r\\ndatasets 2.16.0\\r\\napache-beam 2.52.0\\r\\ndill 0.3.7\\r\\n',\n",
       " 'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/6542/reactions',\n",
       "  'total_count': 0,\n",
       "  '+1': 0,\n",
       "  '-1': 0,\n",
       "  'laugh': 0,\n",
       "  'hooray': 0,\n",
       "  'confused': 0,\n",
       "  'heart': 0,\n",
       "  'rocket': 0,\n",
       "  'eyes': 0},\n",
       " 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/6542/timeline',\n",
       " 'performed_via_github_app': None,\n",
       " 'state_reason': None,\n",
       " 'draft': None,\n",
       " 'pull_request': None}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인터넷 연결 상태에 따라 몇분이 소요될 수도 있습니다...\n",
    "issues_with_comments_dataset = issues_dataset_streamed.map(lambda x: {\"comments\": get_comments(x[\"number\"])})\n",
    "next(iter(issues_with_comments_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c77b7-a8a7-4e9c-a62d-75d77bdaa73a",
   "metadata": {},
   "source": [
    "#### Can I convert an IterableDataset to Dataset?\n",
    "You must cache an IterableDataset to disk to load it as a Dataset. One way to do this is with Dataset.from_generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b93e124d-43ff-48c1-bd1b-41516713be5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2c03d0408d4421a9bcc5d2d20a0994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1742\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1741\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mencode_example(record) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m record\n\u001b[0;32m-> 1742\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_writer.py:492\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[0;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhkey_record \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_writer.py:450\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m         batch_examples[col] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    447\u001b[0m             row[\u001b[38;5;241m0\u001b[39m][col]\u001b[38;5;241m.\u001b[39mto_pylist()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;241m0\u001b[39m][col], (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;241m0\u001b[39m][col]\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples\n\u001b[1;32m    449\u001b[0m         ]\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_writer.py:557\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    556\u001b[0m typed_sequence \u001b[38;5;241m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mcol_type, try_type\u001b[38;5;241m=\u001b[39mcol_try_type, col\u001b[38;5;241m=\u001b[39mcol)\n\u001b[0;32m--> 557\u001b[0m arrays\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    558\u001b[0m inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/pyarrow/array.pxi:248\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/pyarrow/array.pxi:113\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_writer.py:206\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# We use cast_array_to_feature to support casting to custom types like Audio and Image\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# We only do it if trying_type is False - since this is what the user asks for.\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_number_to_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrying_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2040\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 2040\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [_c(array\u001b[38;5;241m.\u001b[39mfield(name), subfeature) \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2040\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 2040\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2110\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, (Sequence, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 2110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_number_to_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_number_to_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1992\u001b[0m, in \u001b[0;36marray_cast\u001b[0;34m(array, pa_type, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(pa_type) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(array\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m-> 1992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpa_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\u001b[38;5;241m.\u001b[39mcast(pa_type)\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't cast array of type timestamp[us] to null",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1751\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1750\u001b[0m num_shards \u001b[38;5;241m=\u001b[39m shard_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1751\u001b[0m num_examples, num_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_writer.py:588\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[0;34m(self, close_stream)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhkey_record \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# If schema is known, infer features even if no examples were written\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_writer.py:450\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m         batch_examples[col] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    447\u001b[0m             row[\u001b[38;5;241m0\u001b[39m][col]\u001b[38;5;241m.\u001b[39mto_pylist()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;241m0\u001b[39m][col], (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;241m0\u001b[39m][col]\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples\n\u001b[1;32m    449\u001b[0m         ]\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_writer.py:557\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    556\u001b[0m typed_sequence \u001b[38;5;241m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mcol_type, try_type\u001b[38;5;241m=\u001b[39mcol_try_type, col\u001b[38;5;241m=\u001b[39mcol)\n\u001b[0;32m--> 557\u001b[0m arrays\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    558\u001b[0m inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/pyarrow/array.pxi:248\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/pyarrow/array.pxi:113\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_writer.py:206\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# We use cast_array_to_feature to support casting to custom types like Audio and Image\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# We only do it if trying_type is False - since this is what the user asks for.\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_number_to_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrying_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2040\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 2040\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [_c(array\u001b[38;5;241m.\u001b[39mfield(name), subfeature) \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2040\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 2040\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2110\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, (Sequence, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 2110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_number_to_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_number_to_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1992\u001b[0m, in \u001b[0;36marray_cast\u001b[0;34m(array, pa_type, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(pa_type) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(array\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m-> 1992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpa_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\u001b[38;5;241m.\u001b[39mcast(pa_type)\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't cast array of type timestamp[us] to null",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[210], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_from_iterable_dataset\u001b[39m(iterable_ds):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m iterable_ds\n\u001b[0;32m----> 7\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_from_iterable_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43missues_with_comments_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43missues_with_comments_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_dataset.py:1073\u001b[0m, in \u001b[0;36mDataset.from_generator\u001b[0;34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a Dataset from a generator.\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeneratorDatasetInputStream\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGeneratorDatasetInputStream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1073\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/io/generator.py:47\u001b[0m, in \u001b[0;36mGeneratorDatasetInputStream.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m     verification_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mas_dataset(\n\u001b[1;32m     56\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, verification_mode\u001b[38;5;241m=\u001b[39mverification_mode, in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_in_memory\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1003\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1765\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1765\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1098\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1105\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1603\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1601\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1603\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1604\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1605\u001b[0m     ):\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1607\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1760\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, SchemaInferenceError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1759\u001b[0m         e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m-> 1760\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from datasets import Dataset\n",
    "\n",
    "def gen_from_iterable_dataset(iterable_ds):\n",
    "    yield from iterable_ds\n",
    "\n",
    "ds = Dataset.from_generator(partial(gen_from_iterable_dataset, issues_with_comments_dataset), features=issues_with_comments_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5aeacc95-e94f-4d28-b211-7aff3a04676a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IterableDataset' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[207], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43missues_with_comments_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets-issues-with-comments.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IterableDataset' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": [
    "issues_with_comments_dataset.to_json(\"datasets-issues-with-comments.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "fd7fe478-983c-4285-9f19-098dfb307ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets on Hub: 90433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'acronym_identification'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from huggingface_hub import list_datasets\n",
    "from datasets import list_datasets\n",
    "\n",
    "all_datasets = list_datasets()\n",
    "print(f\"Number of datasets on Hub: {len(all_datasets)}\")\n",
    "all_datasets[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2d5eb012-fe2e-4b0c-8eff-407fea5956c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets on Hub: 90433\n",
      "['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews', 'allocine', 'alt', 'amazon_polarity', 'amazon_reviews_multi', 'amazon_us_reviews', 'ambig_qa', 'americas_nli', 'ami', 'amttl', 'anli']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from datasets import list_datasets\n",
    "\n",
    "all_datasets = list_datasets()\n",
    "print(f\"Number of datasets on Hub: {len(all_datasets)}\")\n",
    "print(all_datasets[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd298080-a89a-4b33-a641-934451c7feb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309a3a51f3034e358061444121e1cd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "838cbe97-2e5c-433b-be11-cb9f4e94243f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/datasets/hwang2006/datasets-github-issues-2024-08-23', endpoint='https://huggingface.co', repo_type='dataset', repo_id='hwang2006/datasets-github-issues-2024-08-23')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "#repo_url = create_repo(name=\"datasets-github-issues\", repo_type=\"dataset\")\n",
    "repo_url = create_repo(\"datasets-github-issues-2024-08-23\", repo_type=\"dataset\")\n",
    "repo_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86739b0f-9dbb-4034-9b4d-b2664dff692d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: lfs in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (0.2)\n"
     ]
    }
   ],
   "source": [
    "#!pip install lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "999a8bb7-33fa-47be-afd2-05ff065c936b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/datasets/hwang2006/datasets-github-issues-2024-08-23 into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(local_dir=\"datasets-github-issues-2024-08-23\", clone_from=repo_url)\n",
    "#!cp datasets-issues-with-comments.jsonl datasets-github-issues/\n",
    "!cp datasets-issues-with-comments.jsonl datasets-github-issues-2024-08-23/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b41243f-fba4-4176-bfd3-2b4398692c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.lfs_track(\"*.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92629136-7e33-49e6-bfc2-476b489f790c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97aa78c3a4514736af3868fa2a68ab3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file datasets-issues-with-comments.jsonl:   0%|          | 1.00/44.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://user: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@huggingface.co/datasets/hwang2006/datasets-github-issues-2024-08-23\n",
      "   9504e24..cf02692  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/hwang2006/datasets-github-issues-2024-08-23/commit/cf026929beaa61abdfd5c764de51edb7dbb3ff4d'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "24399a5f-91e8-44f3-8ae0-aa63b7fe1da8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3651\n",
       "})"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remote_dataset = load_dataset(\"qualis2006/datasets-github-issues\", split=\"train\")\n",
    "remote_dataset = load_dataset(\"spasis/datasets-github-issues\", split=\"train\")\n",
    "remote_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4975de90-fe05-4010-9c6d-6b0087727de0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#spasis_issues_dataset = load_dataset(\"spasis/datasets-github-issues\", split=\"train\")\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m issues_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlewtun/github-issues\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(issues_dataset)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(issues_dataset))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#spasis_issues_dataset = load_dataset(\"spasis/datasets-github-issues\", split=\"train\")\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "print(issues_dataset)\n",
    "print(len(issues_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b6b4c07-f726-4dba-910e-47be381fde23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> URL: https://github.com/huggingface/datasets/pull/535\n",
      ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/535', 'html_url': 'https://github.com/huggingface/datasets/pull/535', 'diff_url': 'https://github.com/huggingface/datasets/pull/535.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/535.patch'}\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/762\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/808\n",
      ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/808', 'html_url': 'https://github.com/huggingface/datasets/pull/808', 'diff_url': 'https://github.com/huggingface/datasets/pull/808.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/808.patch'}\n",
      "\n",
      "{'url': ['https://api.github.com/repos/huggingface/datasets/issues/535', 'https://api.github.com/repos/huggingface/datasets/issues/762', 'https://api.github.com/repos/huggingface/datasets/issues/808'], 'repository_url': ['https://api.github.com/repos/huggingface/datasets', 'https://api.github.com/repos/huggingface/datasets', 'https://api.github.com/repos/huggingface/datasets'], 'labels_url': ['https://api.github.com/repos/huggingface/datasets/issues/535/labels{/name}', 'https://api.github.com/repos/huggingface/datasets/issues/762/labels{/name}', 'https://api.github.com/repos/huggingface/datasets/issues/808/labels{/name}'], 'comments_url': ['https://api.github.com/repos/huggingface/datasets/issues/535/comments', 'https://api.github.com/repos/huggingface/datasets/issues/762/comments', 'https://api.github.com/repos/huggingface/datasets/issues/808/comments'], 'events_url': ['https://api.github.com/repos/huggingface/datasets/issues/535/events', 'https://api.github.com/repos/huggingface/datasets/issues/762/events', 'https://api.github.com/repos/huggingface/datasets/issues/808/events'], 'html_url': ['https://github.com/huggingface/datasets/pull/535', 'https://github.com/huggingface/datasets/issues/762', 'https://github.com/huggingface/datasets/pull/808'], 'id': [686238315, 730586972, 737638942], 'node_id': ['MDExOlB1bGxSZXF1ZXN0NDczODM3Njg0', 'MDU6SXNzdWU3MzA1ODY5NzI=', 'MDExOlB1bGxSZXF1ZXN0NTE2NjQ0NDc0'], 'number': [535, 762, 808], 'title': ['Benchmarks', '[GEM] Add Czech Restaurant data-to-text generation dataset', 'dataset(dgs): initial dataset loading script'], 'user': [{'login': 'thomwolf', 'id': 7353373, 'node_id': 'MDQ6VXNlcjczNTMzNzM=', 'avatar_url': 'https://avatars.githubusercontent.com/u/7353373?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/thomwolf', 'html_url': 'https://github.com/thomwolf', 'followers_url': 'https://api.github.com/users/thomwolf/followers', 'following_url': 'https://api.github.com/users/thomwolf/following{/other_user}', 'gists_url': 'https://api.github.com/users/thomwolf/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/thomwolf/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/thomwolf/subscriptions', 'organizations_url': 'https://api.github.com/users/thomwolf/orgs', 'repos_url': 'https://api.github.com/users/thomwolf/repos', 'events_url': 'https://api.github.com/users/thomwolf/events{/privacy}', 'received_events_url': 'https://api.github.com/users/thomwolf/received_events', 'type': 'User', 'site_admin': False}, {'login': 'yjernite', 'id': 10469459, 'node_id': 'MDQ6VXNlcjEwNDY5NDU5', 'avatar_url': 'https://avatars.githubusercontent.com/u/10469459?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/yjernite', 'html_url': 'https://github.com/yjernite', 'followers_url': 'https://api.github.com/users/yjernite/followers', 'following_url': 'https://api.github.com/users/yjernite/following{/other_user}', 'gists_url': 'https://api.github.com/users/yjernite/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/yjernite/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/yjernite/subscriptions', 'organizations_url': 'https://api.github.com/users/yjernite/orgs', 'repos_url': 'https://api.github.com/users/yjernite/repos', 'events_url': 'https://api.github.com/users/yjernite/events{/privacy}', 'received_events_url': 'https://api.github.com/users/yjernite/received_events', 'type': 'User', 'site_admin': False}, {'login': 'AmitMY', 'id': 5757359, 'node_id': 'MDQ6VXNlcjU3NTczNTk=', 'avatar_url': 'https://avatars.githubusercontent.com/u/5757359?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/AmitMY', 'html_url': 'https://github.com/AmitMY', 'followers_url': 'https://api.github.com/users/AmitMY/followers', 'following_url': 'https://api.github.com/users/AmitMY/following{/other_user}', 'gists_url': 'https://api.github.com/users/AmitMY/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/AmitMY/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/AmitMY/subscriptions', 'organizations_url': 'https://api.github.com/users/AmitMY/orgs', 'repos_url': 'https://api.github.com/users/AmitMY/repos', 'events_url': 'https://api.github.com/users/AmitMY/events{/privacy}', 'received_events_url': 'https://api.github.com/users/AmitMY/received_events', 'type': 'User', 'site_admin': False}], 'labels': [[], [{'id': 2067376369, 'node_id': 'MDU6TGFiZWwyMDY3Mzc2MzY5', 'url': 'https://api.github.com/repos/huggingface/datasets/labels/dataset%20request', 'name': 'dataset request', 'color': 'e99695', 'default': False, 'description': 'Requesting to add a new dataset'}], []], 'state': ['closed', 'closed', 'closed'], 'locked': [False, False, False], 'assignee': [None, None, None], 'assignees': [[], [], []], 'milestone': [None, None, None], 'comments': [[], [], ['Hi @AmitMY, \\r\\n\\r\\nWere you able to figure this out?', \"I did not.\\r\\nWith all the limitations this repo currently has, I had to create a repo of my own using tfds to mitigate them. \\r\\nhttps://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/dgs_corpus\\r\\n\\r\\nClosing as I don't know how to support this PR further\"]], 'created_at': [1598440886000, 1603814447000, 1604657683000], 'updated_at': [1598517600000, 1607002664000, 1616480335000], 'closed_at': [1598517599000, 1607002664000, 1616480335000], 'author_association': ['MEMBER', 'MEMBER', 'CONTRIBUTOR'], 'active_lock_reason': [None, None, None], 'pull_request': [{'url': 'https://api.github.com/repos/huggingface/datasets/pulls/535', 'html_url': 'https://github.com/huggingface/datasets/pull/535', 'diff_url': 'https://github.com/huggingface/datasets/pull/535.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/535.patch'}, None, {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/808', 'html_url': 'https://github.com/huggingface/datasets/pull/808', 'diff_url': 'https://github.com/huggingface/datasets/pull/808.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/808.patch'}], 'body': [\"Adding some benchmarks with DVC/CML\\r\\n\\r\\nTo add a new tracked benchmark:\\r\\n- create a new python benchmarking script in `./benchmarks/`. The script can use the utilities in `./benchmarks/utils.py` and should output a JSON file with results in `./benchmarks/results/`.\\r\\n- add a new pipeline stage in [dvc.yaml](./dvc.yaml) with the name of your new benchmark.\\r\\n\\r\\nThat's it\", '- Paper: https://www.aclweb.org/anthology/W19-8670.pdf\\r\\n- Data: https://github.com/UFAL-DSG/cs_restaurant_dataset\\r\\n- The dataset will likely be part of the GEM benchmark', \"When trying to create dummy data I get:\\r\\n\\r\\n> Dataset datasets with config None seems to already open files in the method `_split_generators(...)`. You might consider to instead only open files in the method `_generate_examples(...)` instead. If this is not possible the dummy data has t o be created with less guidance. Make sure you create the file dummy_data.\\r\\n\\r\\nI am not sure how to manually create the dummy_data (what exactly it should contain)\\r\\n\\r\\nAlso note, this library says:\\r\\n> ImportError: To be able to use this dataset, you need to install the following dependencies['pympi'] using 'pip install pympi' for instance'\\r\\n\\r\\nWhen you actually need to `pip install pympi-ling`\\r\\n\"], 'timeline_url': ['https://api.github.com/repos/huggingface/datasets/issues/535/timeline', 'https://api.github.com/repos/huggingface/datasets/issues/762/timeline', 'https://api.github.com/repos/huggingface/datasets/issues/808/timeline'], 'performed_via_github_app': [None, None, None], 'is_pull_request': [True, False, True]}\n"
     ]
    }
   ],
   "source": [
    "sample = issues_dataset.shuffle(seed=777).select(range(3))\n",
    "\n",
    "# URL과 pull request 엔트리를 출력합니다.\n",
    "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Pull request: {pr}\\n\")\n",
    "\n",
    "print(sample[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d7cf361-50cc-4510-a567-a50a0fda3a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ddeb5c1dee4fca88ed354948a22a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.map(lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8795e75-8859-41df-8422-c84c2dda1743",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-897594128',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
       "  'id': 897594128,\n",
       "  'node_id': 'IC_kwDODunzps41gDMQ',\n",
       "  'user': {'login': 'bhavitvyamalik',\n",
       "   'id': 19718818,\n",
       "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
       "   'html_url': 'https://github.com/bhavitvyamalik',\n",
       "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
       "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
       "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2021-08-12T12:21:52Z',\n",
       "  'updated_at': '2021-08-12T12:31:17Z',\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'body': \"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-898644889',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
       "  'id': 898644889,\n",
       "  'node_id': 'IC_kwDODunzps41kDuZ',\n",
       "  'user': {'login': 'bhavitvyamalik',\n",
       "   'id': 19718818,\n",
       "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
       "   'html_url': 'https://github.com/bhavitvyamalik',\n",
       "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
       "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
       "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2021-08-13T18:28:27Z',\n",
       "  'updated_at': '2021-08-13T18:28:27Z',\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'body': 'Thanks for the help, @albertvillanova! All tests are passing now.',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_number = 2792\n",
    "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2a22e-3084-473d-8e4d-4642c6023cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#issue_number = 2862\n",
    "issues_number = 2792\n",
    "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "response = requests.get(url, headers=headers)\n",
    "response\n",
    "#response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6fb49976-cc08-4999-befa-7c0547134036",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\", 'Thanks for the help, @albertvillanova! All tests are passing now.']\n",
      "['N/A']\n"
     ]
    }
   ],
   "source": [
    "def get_comments(issue_number):\n",
    "    #print(issue_number)\n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    #print(response.status_code)\n",
    "    if response.status_code == 200:\n",
    "        return[r[\"body\"] for r in response.json()]\n",
    "    return([\"N/A\"]) #\n",
    "\n",
    "print(get_comments(2792))\n",
    "print(get_comments(2862))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15810dfa-845c-43a9-b06f-3c06e4c56a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(issues_dataset[:][\"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1673ec9f-67da-4286-8703-f517a3e96c64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4046c6be33d944a28b51270529ad3097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 인터넷 연결 상태에 따라 몇분이 소요될 수도 있습니다...\n",
    "issues_with_comments_dataset = issues_dataset.map(lambda x: {\"comments\": get_comments(x[\"number\"])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5535abd2-1858-460d-b9f5-636f68b8883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/datasets/Evan/spaCy-github-issues/resolve/main/spacy-issues.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>repository_url</th>\n",
       "      <th>labels_url</th>\n",
       "      <th>comments_url</th>\n",
       "      <th>events_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>author_association</th>\n",
       "      <th>active_lock_reason</th>\n",
       "      <th>draft</th>\n",
       "      <th>pull_request</th>\n",
       "      <th>body</th>\n",
       "      <th>reactions</th>\n",
       "      <th>timeline_url</th>\n",
       "      <th>performed_via_github_app</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://github.com/explosion/spaCy/pull/10484</td>\n",
       "      <td>1167535225</td>\n",
       "      <td>PR_kwDOAUeP5s40Wp7a</td>\n",
       "      <td>10484</td>\n",
       "      <td>Updated explenation for for classy classification</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-03-13 10:01:42+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/explosio...</td>\n",
       "      <td>Reviewed and simplified description and code e...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/explosio...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://github.com/explosion/spaCy/pull/10479</td>\n",
       "      <td>1166118293</td>\n",
       "      <td>PR_kwDOAUeP5s40SKvj</td>\n",
       "      <td>10479</td>\n",
       "      <td>Auto-format code with black</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-03-11 11:20:24+00:00</td>\n",
       "      <td>2022-03-11 11:20:24+00:00</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/explosio...</td>\n",
       "      <td>_This PR is auto-generated._</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/explosio...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://github.com/explosion/spaCy/pull/10476</td>\n",
       "      <td>1165160224</td>\n",
       "      <td>PR_kwDOAUeP5s40PBSY</td>\n",
       "      <td>10476</td>\n",
       "      <td>Support more internal methods for SpanGroup</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-03-11 10:21:01+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/explosio...</td>\n",
       "      <td>&lt;!--- Provide a general summary of your change...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/explosio...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://github.com/explosion/spaCy/issues/10474</td>\n",
       "      <td>1164901335</td>\n",
       "      <td>I_kwDOAUeP5s5FbvvX</td>\n",
       "      <td>10474</td>\n",
       "      <td>Update `Vocab` docs to avoid `len()` and `is_o...</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-03-10 11:39:02+00:00</td>\n",
       "      <td>2022-03-10 11:39:02+00:00</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Partially because some things were different i...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/explosio...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>https://github.com/explosion/spaCy/pull/10471</td>\n",
       "      <td>1164552054</td>\n",
       "      <td>PR_kwDOAUeP5s40NBjt</td>\n",
       "      <td>10471</td>\n",
       "      <td>small cleanup</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-03-11 08:10:15+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/explosio...</td>\n",
       "      <td>this isn't used until the exception block, so ...</td>\n",
       "      <td>{'url': 'https://api.github.com/repos/explosio...</td>\n",
       "      <td>https://api.github.com/repos/explosion/spaCy/i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "1  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "2  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "3  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "4  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "\n",
       "                                 repository_url  \\\n",
       "0  https://api.github.com/repos/explosion/spaCy   \n",
       "1  https://api.github.com/repos/explosion/spaCy   \n",
       "2  https://api.github.com/repos/explosion/spaCy   \n",
       "3  https://api.github.com/repos/explosion/spaCy   \n",
       "4  https://api.github.com/repos/explosion/spaCy   \n",
       "\n",
       "                                          labels_url  \\\n",
       "0  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "1  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "2  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "3  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "4  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "\n",
       "                                        comments_url  \\\n",
       "0  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "1  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "2  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "3  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "4  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "\n",
       "                                          events_url  \\\n",
       "0  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "1  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "2  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "3  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "4  https://api.github.com/repos/explosion/spaCy/i...   \n",
       "\n",
       "                                          html_url          id  \\\n",
       "0    https://github.com/explosion/spaCy/pull/10484  1167535225   \n",
       "1    https://github.com/explosion/spaCy/pull/10479  1166118293   \n",
       "2    https://github.com/explosion/spaCy/pull/10476  1165160224   \n",
       "3  https://github.com/explosion/spaCy/issues/10474  1164901335   \n",
       "4    https://github.com/explosion/spaCy/pull/10471  1164552054   \n",
       "\n",
       "               node_id  number  \\\n",
       "0  PR_kwDOAUeP5s40Wp7a   10484   \n",
       "1  PR_kwDOAUeP5s40SKvj   10479   \n",
       "2  PR_kwDOAUeP5s40PBSY   10476   \n",
       "3   I_kwDOAUeP5s5FbvvX   10474   \n",
       "4  PR_kwDOAUeP5s40NBjt   10471   \n",
       "\n",
       "                                               title  ...  \\\n",
       "0  Updated explenation for for classy classification  ...   \n",
       "1                        Auto-format code with black  ...   \n",
       "2        Support more internal methods for SpanGroup  ...   \n",
       "3  Update `Vocab` docs to avoid `len()` and `is_o...  ...   \n",
       "4                                      small cleanup  ...   \n",
       "\n",
       "                 updated_at                 closed_at author_association  \\\n",
       "0 2022-03-13 10:01:42+00:00                       NaT        CONTRIBUTOR   \n",
       "1 2022-03-11 11:20:24+00:00 2022-03-11 11:20:24+00:00        CONTRIBUTOR   \n",
       "2 2022-03-11 10:21:01+00:00                       NaT             MEMBER   \n",
       "3 2022-03-10 11:39:02+00:00 2022-03-10 11:39:02+00:00        CONTRIBUTOR   \n",
       "4 2022-03-11 08:10:15+00:00                       NaT               NONE   \n",
       "\n",
       "   active_lock_reason draft  \\\n",
       "0                None   0.0   \n",
       "1                None   0.0   \n",
       "2                None   0.0   \n",
       "3                None   NaN   \n",
       "4                None   0.0   \n",
       "\n",
       "                                        pull_request  \\\n",
       "0  {'url': 'https://api.github.com/repos/explosio...   \n",
       "1  {'url': 'https://api.github.com/repos/explosio...   \n",
       "2  {'url': 'https://api.github.com/repos/explosio...   \n",
       "3                                               None   \n",
       "4  {'url': 'https://api.github.com/repos/explosio...   \n",
       "\n",
       "                                                body  \\\n",
       "0  Reviewed and simplified description and code e...   \n",
       "1                       _This PR is auto-generated._   \n",
       "2  <!--- Provide a general summary of your change...   \n",
       "3  Partially because some things were different i...   \n",
       "4  this isn't used until the exception block, so ...   \n",
       "\n",
       "                                           reactions  \\\n",
       "0  {'url': 'https://api.github.com/repos/explosio...   \n",
       "1  {'url': 'https://api.github.com/repos/explosio...   \n",
       "2  {'url': 'https://api.github.com/repos/explosio...   \n",
       "3  {'url': 'https://api.github.com/repos/explosio...   \n",
       "4  {'url': 'https://api.github.com/repos/explosio...   \n",
       "\n",
       "                                        timeline_url performed_via_github_app  \n",
       "0  https://api.github.com/repos/explosion/spaCy/i...                      NaN  \n",
       "1  https://api.github.com/repos/explosion/spaCy/i...                      NaN  \n",
       "2  https://api.github.com/repos/explosion/spaCy/i...                      NaN  \n",
       "3  https://api.github.com/repos/explosion/spaCy/i...                      NaN  \n",
       "4  https://api.github.com/repos/explosion/spaCy/i...                      NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_url\n",
    "import pandas as pd\n",
    "\n",
    "# returns 'https://huggingface.co/datasets/Evan/spaCy-github-issues/resolve/main/spacy-issues.jsonl'\n",
    "data_files = hf_hub_url(repo_id=\"Evan/spaCy-github-issues\", filename=\"spacy-issues.jsonl\", repo_type=\"dataset\")\n",
    "print(data_files)\n",
    "# throws TypeError: Couldn't cast array of type\n",
    "#dset = load_dataset(\"json\", data_files=data_files, split=\"test\")\n",
    "# no problem with pandas - note this take a while as the file is >2GB\n",
    "df = pd.read_json(data_files, orient=\"records\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1410bbac-976c-4196-acc9-112534817bba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app'],\n",
       "    num_rows: 486494\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "issues_dataset\n",
    "#sample = issues_dataset.shuffle(seed=666).select(range(3))\n",
    "#sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ac2d982-c039-4a50-8c3d-78742b1ca0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486494"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(issues_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c448e6b-d828-4fd7-84d8-449d6b32df0a",
   "metadata": {},
   "source": [
    "#### Datasets load error for saved github issues #5422\n",
    "https://github.com/huggingface/datasets/issues/5422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b91ab093-7281-46db-b3b5-0440a7ce6896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request'],\n",
       "    num_rows: 6513\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json(\"datasets-issues.jsonl\", lines=True)\n",
    "#df.head()\n",
    "\n",
    "from datasets import Dataset\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "issues_dataset\n",
    "#sample = issues_dataset.shuffle(seed=666).select(range(3))\n",
    "#sample[0]\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6364965-4b04-4bad-98c7-cb1365819139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6513"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(issues_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cc34f07-be71-451b-a39c-6fa87f7910ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/6542',\n",
       " 'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       " 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/6542/labels{/name}',\n",
       " 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/6542/comments',\n",
       " 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/6542/events',\n",
       " 'html_url': 'https://github.com/huggingface/datasets/issues/6542',\n",
       " 'id': 2059198575,\n",
       " 'node_id': 'I_kwDODunzps56vOBv',\n",
       " 'number': 6542,\n",
       " 'title': 'Datasets : wikipedia 20220301.en error ',\n",
       " 'user': {'avatar_url': 'https://avatars.githubusercontent.com/u/53203620?v=4',\n",
       "  'events_url': 'https://api.github.com/users/ppx666/events{/privacy}',\n",
       "  'followers_url': 'https://api.github.com/users/ppx666/followers',\n",
       "  'following_url': 'https://api.github.com/users/ppx666/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/ppx666/gists{/gist_id}',\n",
       "  'gravatar_id': '',\n",
       "  'html_url': 'https://github.com/ppx666',\n",
       "  'id': 53203620,\n",
       "  'login': 'ppx666',\n",
       "  'node_id': 'MDQ6VXNlcjUzMjAzNjIw',\n",
       "  'organizations_url': 'https://api.github.com/users/ppx666/orgs',\n",
       "  'received_events_url': 'https://api.github.com/users/ppx666/received_events',\n",
       "  'repos_url': 'https://api.github.com/users/ppx666/repos',\n",
       "  'site_admin': False,\n",
       "  'starred_url': 'https://api.github.com/users/ppx666/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/ppx666/subscriptions',\n",
       "  'type': 'User',\n",
       "  'url': 'https://api.github.com/users/ppx666'},\n",
       " 'labels': [],\n",
       " 'state': 'open',\n",
       " 'locked': False,\n",
       " 'assignee': None,\n",
       " 'assignees': [],\n",
       " 'milestone': None,\n",
       " 'comments': 0,\n",
       " 'created_at': Timestamp('2023-12-29 08:34:51+0000', tz='UTC'),\n",
       " 'updated_at': Timestamp('2023-12-29 08:34:51+0000', tz='UTC'),\n",
       " 'closed_at': None,\n",
       " 'author_association': 'NONE',\n",
       " 'active_lock_reason': None,\n",
       " 'body': '### Describe the bug\\n\\nWhen I used load_dataset to download this data set, the following error occurred. The main problem was that the target data did not exist.\\n\\n### Steps to reproduce the bug\\n\\n1.I tried downloading directly.\\r\\n```python\\r\\nwiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\")\\r\\n```\\r\\nAn exception occurred\\r\\n```\\r\\nMissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/\\r\\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \\r\\nExample of usage: \\r\\n\\t`load_dataset(\\'wikipedia\\', \\'20220301.en\\', beam_runner=\\'DirectRunner\\')`\\r\\n```\\r\\n2.I modified the code as prompted.\\r\\n```python\\r\\nwiki_dataset = load_dataset(\\'wikipedia\\', \\'20220301.en\\', beam_runner=\\'DirectRunner\\')\\r\\n```\\r\\nAn exception occurred:\\r\\n```\\r\\nFileNotFoundError: Couldn\\'t find file at https://dumps.wikimedia.org/enwiki/20220301/dumpstatus.json\\r\\n```\\r\\n\\n\\n### Expected behavior\\n\\nI searched in the parent directory of the corresponding URL, but there was no corresponding \"20220301\" directory.\\r\\nI really need this data set and hope to provide a download method.\\n\\n### Environment info\\n\\npython 3.8\\r\\ndatasets 2.16.0\\r\\napache-beam 2.52.0\\r\\ndill 0.3.7\\r\\n',\n",
       " 'reactions': {'+1': 0,\n",
       "  '-1': 0,\n",
       "  'confused': 0,\n",
       "  'eyes': 0,\n",
       "  'heart': 0,\n",
       "  'hooray': 0,\n",
       "  'laugh': 0,\n",
       "  'rocket': 0,\n",
       "  'total_count': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/datasets/issues/6542/reactions'},\n",
       " 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/6542/timeline',\n",
       " 'performed_via_github_app': None,\n",
       " 'state_reason': None,\n",
       " 'draft': None,\n",
       " 'pull_request': None,\n",
       " 'is_pull_request': False}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cad39296-c9a5-432f-b97b-6991db273c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> URL: https://github.com/huggingface/datasets/issues/6542\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6541\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6540\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6539\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6538\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6537\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6536\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6535\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6534\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6533\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6532\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6531\n",
      ">> Pull request: {'diff_url': 'https://github.com/huggingface/datasets/pull/6531.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/6531', 'merged_at': None, 'patch_url': 'https://github.com/huggingface/datasets/pull/6531.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6531'}\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6530\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6529\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6528\n",
      ">> Pull request: {'diff_url': 'https://github.com/huggingface/datasets/pull/6528.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/6528', 'merged_at': '2023-12-22T14:25:34Z', 'patch_url': 'https://github.com/huggingface/datasets/pull/6528.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6528'}\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6527\n",
      ">> Pull request: {'diff_url': 'https://github.com/huggingface/datasets/pull/6527.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/6527', 'merged_at': '2023-12-22T14:17:55Z', 'patch_url': 'https://github.com/huggingface/datasets/pull/6527.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6527'}\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6526\n",
      ">> Pull request: {'diff_url': 'https://github.com/huggingface/datasets/pull/6526.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/6526', 'merged_at': '2023-12-22T11:36:14Z', 'patch_url': 'https://github.com/huggingface/datasets/pull/6526.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6526'}\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6525\n",
      ">> Pull request: {'diff_url': 'https://github.com/huggingface/datasets/pull/6525.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/6525', 'merged_at': None, 'patch_url': 'https://github.com/huggingface/datasets/pull/6525.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6525'}\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6524\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6523\n",
      ">> Pull request: {'diff_url': 'https://github.com/huggingface/datasets/pull/6523.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/6523', 'merged_at': '2023-12-21T15:50:38Z', 'patch_url': 'https://github.com/huggingface/datasets/pull/6523.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6523'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sample = issues_dataset.shuffle(seed=777).select(range(10))\n",
    "sample = issues_dataset.select(range(20))\n",
    "\n",
    "# URL과 pull request 엔트리를 출력합니다.\n",
    "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Pull request: {pr}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ead43352-2be6-431d-94b9-8c85c6f2623b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2575837c70e74c41bf449e30b91c962d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6513 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.map(lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28c2c3bc-808b-49cd-b088-bd5c92f2f5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> URL: https://github.com/huggingface/datasets/issues/6542\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6541\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6540\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6539\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6538\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6537\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6536\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6535\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6534\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6533\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6532\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6531\n",
      ">> Is Pull request: True\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6530\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6529\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6528\n",
      ">> Is Pull request: True\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6527\n",
      ">> Is Pull request: True\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6526\n",
      ">> Is Pull request: True\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6525\n",
      ">> Is Pull request: True\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6524\n",
      ">> Is Pull request: False\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/pull/6523\n",
      ">> Is Pull request: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sample = issues_dataset.shuffle(seed=777).select(range(10))\n",
    "sample = issues_dataset.select(range(20))\n",
    "\n",
    "# URL과 pull request 엔트리를 출력합니다.\n",
    "for url, pr in zip(sample[\"html_url\"], sample[\"is_pull_request\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Is Pull request: {pr}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90226564-dc4a-42ea-bcdc-c93a9dc9ce05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/1872544422',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/issues/6542#issuecomment-1872544422',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/6542',\n",
       "  'id': 1872544422,\n",
       "  'node_id': 'IC_kwDODunzps5vnMKm',\n",
       "  'user': {'login': 'lhoestq',\n",
       "   'id': 42851186,\n",
       "   'node_id': 'MDQ6VXNlcjQyODUxMTg2',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/42851186?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/lhoestq',\n",
       "   'html_url': 'https://github.com/lhoestq',\n",
       "   'followers_url': 'https://api.github.com/users/lhoestq/followers',\n",
       "   'following_url': 'https://api.github.com/users/lhoestq/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/lhoestq/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/lhoestq/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/lhoestq/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/lhoestq/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/lhoestq/repos',\n",
       "   'events_url': 'https://api.github.com/users/lhoestq/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/lhoestq/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2023-12-30T15:03:35Z',\n",
       "  'updated_at': '2023-12-30T15:03:35Z',\n",
       "  'author_association': 'MEMBER',\n",
       "  'body': 'Hi ! We now recommend using the `wikimedia/wikipedia` dataset, can you try loading this one instead ?\\r\\n\\r\\n```python\\r\\nwiki_dataset  = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")\\r\\n```',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/1872544422/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_number = 6542\n",
    "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f4bb6dc-fa9a-42e5-a6c1-3f8bf1490a66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\", 'Thanks for the help, @albertvillanova! All tests are passing now.']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def get_comments(issue_number):\n",
    "    #print(issue_number)\n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    #print(response.status_code)\n",
    "    if response.status_code == 200:\n",
    "        return[r[\"body\"] for r in response.json()]\n",
    "    #return([\"N/A\"]) #\n",
    "\n",
    "print(get_comments(2792))\n",
    "print(get_comments(2862))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c351607-4deb-41c5-ab0e-bac4743de987",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109d775a02a7420fa4e4a8a1f5a48474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6513 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 인터넷 연결 상태에 따라 몇분이 소요될 수도 있습니다...\n",
    "issues_with_comments_dataset = issues_dataset.map(lambda x: {\"comments\": get_comments(x[\"number\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "160c7604-71c7-411e-9dfa-f38413bf3dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b44194e5d94b419894811ad6e688cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "41732009"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_with_comments_dataset.to_json(\"datasets-issues-with-comments.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6917097b-3adf-4b61-b690-2b811b018815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://api.github.com/repos/huggingface/datasets/issues/3705',\n",
       " 'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       " 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/3705/labels{/name}',\n",
       " 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/3705/comments',\n",
       " 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/3705/events',\n",
       " 'html_url': 'https://github.com/huggingface/datasets/pull/3705',\n",
       " 'id': 1132053226,\n",
       " 'node_id': 'PR_kwDODunzps4yfhyj',\n",
       " 'number': 3705,\n",
       " 'title': 'Raise informative error when loading a save_to_disk dataset',\n",
       " 'user': {'avatar_url': 'https://avatars.githubusercontent.com/u/8515462?v=4',\n",
       "  'events_url': 'https://api.github.com/users/albertvillanova/events{/privacy}',\n",
       "  'followers_url': 'https://api.github.com/users/albertvillanova/followers',\n",
       "  'following_url': 'https://api.github.com/users/albertvillanova/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/albertvillanova/gists{/gist_id}',\n",
       "  'gravatar_id': '',\n",
       "  'html_url': 'https://github.com/albertvillanova',\n",
       "  'id': 8515462,\n",
       "  'login': 'albertvillanova',\n",
       "  'node_id': 'MDQ6VXNlcjg1MTU0NjI=',\n",
       "  'organizations_url': 'https://api.github.com/users/albertvillanova/orgs',\n",
       "  'received_events_url': 'https://api.github.com/users/albertvillanova/received_events',\n",
       "  'repos_url': 'https://api.github.com/users/albertvillanova/repos',\n",
       "  'site_admin': False,\n",
       "  'starred_url': 'https://api.github.com/users/albertvillanova/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/albertvillanova/subscriptions',\n",
       "  'type': 'User',\n",
       "  'url': 'https://api.github.com/users/albertvillanova'},\n",
       " 'labels': [],\n",
       " 'state': 'closed',\n",
       " 'locked': False,\n",
       " 'assignee': None,\n",
       " 'assignees': [],\n",
       " 'milestone': None,\n",
       " 'comments': [],\n",
       " 'created_at': Timestamp('2022-02-11 08:21:03+0000', tz='UTC'),\n",
       " 'updated_at': Timestamp('2022-02-11 22:56:40+0000', tz='UTC'),\n",
       " 'closed_at': Timestamp('2022-02-11 22:56:39+0000', tz='UTC'),\n",
       " 'author_association': 'MEMBER',\n",
       " 'active_lock_reason': None,\n",
       " 'body': 'People recurrently report error when trying to load a dataset (using `load_dataset`) that was previously saved using `save_to_disk`.\\r\\n\\r\\nThis PR raises an informative error message telling them they should use `load_from_disk` instead.\\r\\n\\r\\nClose #3700.',\n",
       " 'reactions': {'+1': 0,\n",
       "  '-1': 0,\n",
       "  'confused': 0,\n",
       "  'eyes': 0,\n",
       "  'heart': 0,\n",
       "  'hooray': 0,\n",
       "  'laugh': 0,\n",
       "  'rocket': 0,\n",
       "  'total_count': 0,\n",
       "  'url': 'https://api.github.com/repos/huggingface/datasets/issues/3705/reactions'},\n",
       " 'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/3705/timeline',\n",
       " 'performed_via_github_app': None,\n",
       " 'state_reason': None,\n",
       " 'draft': 0.0,\n",
       " 'pull_request': {'diff_url': 'https://github.com/huggingface/datasets/pull/3705.diff',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/3705',\n",
       "  'merged_at': '2022-02-11T22:56:39Z',\n",
       "  'patch_url': 'https://github.com/huggingface/datasets/pull/3705.patch',\n",
       "  'url': 'https://api.github.com/repos/huggingface/datasets/pulls/3705'},\n",
       " 'is_pull_request': True}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_with_comments_dataset[2863]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5dd8757e-4b60-4ff7-b6ea-2e31809c8950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6513"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(issues_with_comments_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bf187fa-b2a7-4d57-8c0c-6d75fd65209c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8c25e2805c4b49a121ac2a8adc0509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0df4d2b7-3905-422a-8ab1-f31b796bdffe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/datasets/qualis2006/datasets-github-issues', endpoint='https://huggingface.co', repo_type='dataset', repo_id='qualis2006/datasets-github-issues')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repo_url = create_repo(\"datasets-github-issues\", repo_type=\"dataset\")\n",
    "repo_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b10d0cd-ab83-4673-ba05-d4bb912f0c55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git-lfs\n",
      "  Downloading git_lfs-1.6-py2.py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: git-lfs\n",
      "Successfully installed git-lfs-1.6\n"
     ]
    }
   ],
   "source": [
    "#!pip install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe7f060-cf04-4443-bf53-c3d1fe832557",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/repository.py:591\u001b[0m, in \u001b[0;36mRepository.check_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     lfs_version \u001b[38;5;241m=\u001b[39m \u001b[43mrun_subprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgit-lfs --version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/utils/_subprocess.py:83\u001b[0m, in \u001b[0;36mrun_subprocess\u001b[0;34m(command, folder, check, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(folder)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# if not utf-8, replace char by �\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/subprocess.py:503\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'git-lfs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Repository\n\u001b[0;32m----> 3\u001b[0m repo \u001b[38;5;241m=\u001b[39m \u001b[43mRepository\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets-github-issues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp datasets-issues-with-comments.jsonl datasets-github-issues/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:128\u001b[0m, in \u001b[0;36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     warning_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message\n\u001b[1;32m    127\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/repository.py:521\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, token, git_user, git_email, revision, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_lfs_files \u001b[38;5;241m=\u001b[39m skip_lfs_files\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m client \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m HfApi()\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_git_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhuggingface_token: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/repository.py:593\u001b[0m, in \u001b[0;36mRepository.check_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    591\u001b[0m     lfs_version \u001b[38;5;241m=\u001b[39m run_subprocess(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit-lfs --version\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_dir)\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLooks like you do not have git-lfs installed, please install.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You can install from https://git-lfs.github.com/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Then run `git lfs install` (you only have to do this once).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m     )\n\u001b[1;32m    598\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(git_version \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m lfs_version)\n",
      "\u001b[0;31mOSError\u001b[0m: Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once)."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(local_dir=\"datasets-github-issues\", clone_from=repo_url)\n",
    "!cp datasets-issues-with-comments.jsonl datasets-github-issues/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5edbf9f7-cca8-4039-b29b-5cd756249408",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b19c3bb181044168f2a20c03c2ec471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1989\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_writer.py:574\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    573\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mcombine_chunks()\n\u001b[0;32m--> 574\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_local_files:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2322\u001b[0m, in \u001b[0;36mtable_cast\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m!=\u001b[39m schema:\n\u001b[0;32m-> 2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m!=\u001b[39m schema\u001b[38;5;241m.\u001b[39mmetadata:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2281\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[1;32m   2277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2278\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m   2279\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[1;32m   2280\u001b[0m     )\n\u001b[0;32m-> 2281\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2281\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[1;32m   2277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2278\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m   2279\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[1;32m   2280\u001b[0m     )\n\u001b[0;32m-> 2281\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1797\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray):\n\u001b[0;32m-> 1797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([func(chunk, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1797\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray):\n\u001b[0;32m-> 1797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2040\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 2040\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [_c(array\u001b[38;5;241m.\u001b[39mfield(name), subfeature) \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2040\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 2040\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2110\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, (Sequence, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 2110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_number_to_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_number_to_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1992\u001b[0m, in \u001b[0;36marray_cast\u001b[0;34m(array, pa_type, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(pa_type) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(array\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m-> 1992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpa_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\u001b[38;5;241m.\u001b[39mcast(pa_type)\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't cast array of type timestamp[s] to null",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m issues_with_comments_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets-issues-with-comments.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:2549\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2546\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2549\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2556\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2559\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2560\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2561\u001b[0m )\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1005\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1100\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1107\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1860\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1858\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1860\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1861\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1862\u001b[0m     ):\n\u001b[1;32m   1863\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1864\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:2016\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[1;32m   2015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_with_comments_dataset = load_dataset(\"json\", data_files=\"datasets-issues-with-comments.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4119d5-44b4-4661-b3ad-7ea20b135548",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 6707\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json(\"datasets-issues-with-comments.jsonl\", lines=True)\n",
    "#df.head()\n",
    "\n",
    "from datasets import Dataset\n",
    "issues_with_comments_dataset = Dataset.from_pandas(df)\n",
    "issues_with_comments_dataset\n",
    "#sample = issues_dataset.shuffle(seed=666).select(range(3))\n",
    "#sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d9fb8a-d16f-490a-b9e1-291064d51b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<details>\\n<summary>Show benchmarks</summary>\\n\\nPyArrow==8.0.0\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.005295 / 0.011353 (-0.006058) | 0.003402 / 0.011008 (-0.007606) | 0.062860 / 0.038508 (0.024352) | 0.029627 / 0.023109 (0.006518) | 0.238359 / 0.275898 (-0.037539) | 0.262940 / 0.323480 (-0.060540) | 0.003077 / 0.007986 (-0.004909) | 0.002676 / 0.004328 (-0.001652) | 0.048731 / 0.004250 (0.044480) | 0.043989 / 0.037052 (0.006936) | 0.255702 / 0.258489 (-0.002787) | 0.282667 / 0.293841 (-0.011174) | 0.028019 / 0.128546 (-0.100527) | 0.010195 / 0.075646 (-0.065451) | 0.205472 / 0.419271 (-0.213800) | 0.036551 / 0.043533 (-0.006982) | 0.243282 / 0.255139 (-0.011857) | 0.261925 / 0.283200 (-0.021274) | 0.020506 / 0.141683 (-0.121177) | 1.137228 / 1.452155 (-0.314927) | 1.183935 / 1.492716 (-0.308782) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.100290 / 0.018006 (0.082284) | 0.316279 / 0.000490 (0.315790) | 0.000239 / 0.000200 (0.000039) | 0.000043 / 0.000054 (-0.000011) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.017979 / 0.037411 (-0.019432) | 0.061616 / 0.014526 (0.047090) | 0.072989 / 0.176557 (-0.103568) | 0.118667 / 0.737135 (-0.618468) | 0.074266 / 0.296338 (-0.222072) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.287971 / 0.215209 (0.072762) | 2.845235 / 2.077655 (0.767581) | 1.501983 / 1.504120 (-0.002137) | 1.389824 / 1.541195 (-0.151370) | 1.415616 / 1.468490 (-0.052874) | 0.568727 / 4.584777 (-4.016050) | 2.368330 / 3.745712 (-1.377382) | 2.844329 / 5.269862 (-2.425532) | 1.809038 / 4.565676 (-2.756639) | 0.063699 / 0.424275 (-0.360576) | 0.004972 / 0.007607 (-0.002635) | 0.340092 / 0.226044 (0.114048) | 3.369146 / 2.268929 (1.100217) | 1.863423 / 55.444624 (-53.581201) | 1.608334 / 6.876477 (-5.268142) | 1.624479 / 2.142072 (-0.517594) | 0.632439 / 4.805227 (-4.172788) | 0.116862 / 6.500664 (-6.383802) | 0.042558 / 0.075469 (-0.032911) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.967922 / 1.841788 (-0.873866) | 11.730612 / 8.074308 (3.656304) | 9.321333 / 10.191392 (-0.870059) | 0.142604 / 0.680424 (-0.537819) | 0.013934 / 0.534201 (-0.520267) | 0.285992 / 0.579283 (-0.293292) | 0.267639 / 0.434364 (-0.166724) | 0.324972 / 0.540337 (-0.215365) | 0.427077 / 1.386936 (-0.959859) |\\n\\n</details>\\nPyArrow==latest\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.005806 / 0.011353 (-0.005547) | 0.003771 / 0.011008 (-0.007237) | 0.049542 / 0.038508 (0.011034) | 0.030182 / 0.023109 (0.007073) | 0.303923 / 0.275898 (0.028025) | 0.325623 / 0.323480 (0.002143) | 0.004327 / 0.007986 (-0.003659) | 0.002818 / 0.004328 (-0.001510) | 0.048237 / 0.004250 (0.043987) | 0.047490 / 0.037052 (0.010437) | 0.316556 / 0.258489 (0.058067) | 0.348352 / 0.293841 (0.054512) | 0.029444 / 0.128546 (-0.099102) | 0.010544 / 0.075646 (-0.065102) | 0.057382 / 0.419271 (-0.361890) | 0.056210 / 0.043533 (0.012677) | 0.305495 / 0.255139 (0.050356) | 0.321570 / 0.283200 (0.038370) | 0.019546 / 0.141683 (-0.122137) | 1.141732 / 1.452155 (-0.310423) | 1.223626 / 1.492716 (-0.269091) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.093864 / 0.018006 (0.075858) | 0.309715 / 0.000490 (0.309226) | 0.000217 / 0.000200 (0.000017) | 0.000053 / 0.000054 (-0.000002) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.022047 / 0.037411 (-0.015364) | 0.074885 / 0.014526 (0.060359) | 0.088440 / 0.176557 (-0.088117) | 0.127033 / 0.737135 (-0.610103) | 0.089048 / 0.296338 (-0.207290) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.292624 / 0.215209 (0.077415) | 2.877592 / 2.077655 (0.799937) | 1.607036 / 1.504120 (0.102916) | 1.487819 / 1.541195 (-0.053376) | 1.517318 / 1.468490 (0.048828) | 0.553321 / 4.584777 (-4.031456) | 2.415577 / 3.745712 (-1.330135) | 2.691411 / 5.269862 (-2.578450) | 1.743395 / 4.565676 (-2.822282) | 0.062187 / 0.424275 (-0.362088) | 0.005073 / 0.007607 (-0.002534) | 0.342907 / 0.226044 (0.116863) | 3.402054 / 2.268929 (1.133126) | 1.979481 / 55.444624 (-53.465143) | 1.702885 / 6.876477 (-5.173592) | 1.868279 / 2.142072 (-0.273794) | 0.640095 / 4.805227 (-4.165132) | 0.117138 / 6.500664 (-6.383526) | 0.042197 / 0.075469 (-0.033272) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 1.007495 / 1.841788 (-0.834292) | 12.037309 / 8.074308 (3.963001) | 10.227670 / 10.191392 (0.036278) | 0.149533 / 0.680424 (-0.530891) | 0.015282 / 0.534201 (-0.518919) | 0.287357 / 0.579283 (-0.291926) | 0.285109 / 0.434364 (-0.149255) | 0.324027 / 0.540337 (-0.216311) | 0.442482 / 1.386936 (-0.944454) |\\n\\n</details>\\n</details>\\n\\n![](https://cml.dev/watermark.png#19b40860acf3b3ba8db727fcf3b1b99ebb8d7e33 \"CML watermark\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(len(issues_with_comments_dataset[1]['comments']))\n",
    "issues_with_comments_dataset[1]['comments'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e902c93-92ad-4f4a-a7f4-9d83ea5ae03a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fff11332024c3d8266cd9cea5b0ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/41.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea7a9fb798847e79b2f0a0ac242b235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1987\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1987\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/arrow_writer.py:574\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    573\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mcombine_chunks()\n\u001b[0;32m--> 574\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_local_files:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2322\u001b[0m, in \u001b[0;36mtable_cast\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m!=\u001b[39m schema:\n\u001b[0;32m-> 2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m!=\u001b[39m schema\u001b[38;5;241m.\u001b[39mmetadata:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2281\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[1;32m   2277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2278\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m   2279\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[1;32m   2280\u001b[0m     )\n\u001b[0;32m-> 2281\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2281\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[1;32m   2277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2278\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m   2279\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[1;32m   2280\u001b[0m     )\n\u001b[0;32m-> 2281\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1797\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray):\n\u001b[0;32m-> 1797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([func(chunk, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1797\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray):\n\u001b[0;32m-> 1797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2040\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 2040\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [_c(array\u001b[38;5;241m.\u001b[39mfield(name), subfeature) \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2040\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 2040\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:2110\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, (Sequence, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 2110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_number_to_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_number_to_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/table.py:1992\u001b[0m, in \u001b[0;36marray_cast\u001b[0;34m(array, pa_type, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(pa_type) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(array\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m-> 1992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpa_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\u001b[38;5;241m.\u001b[39mcast(pa_type)\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't cast array of type timestamp[s] to null",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m remote_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqualis2006/datasets-github-issues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m remote_dataset\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:2545\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2542\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2545\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2555\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2556\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2557\u001b[0m )\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1003\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1098\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1105\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:1858\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1856\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1858\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1859\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1860\u001b[0m     ):\n\u001b[1;32m   1861\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1862\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:2014\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   2012\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[1;32m   2013\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 2014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "remote_dataset = load_dataset(\"qualis2006/datasets-github-issues\", split=\"train\")\n",
    "remote_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e323e-75ae-4734-90c2-5e0cd24b0159",
   "metadata": {},
   "source": [
    "#### TypeError: Couldn't cast array of type for JSONLines dataset #3965\n",
    "https://github.com/huggingface/datasets/issues/3965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e132017-d5e0-4d57-b16a-924ae19158bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 6513\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_url\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# returns 'https://huggingface.co/datasets/Evan/spaCy-github-issues/resolve/main/spacy-issues.jsonl'\n",
    "data_files = hf_hub_url(repo_id=\"qualis2006/datasets-github-issues\", filename=\"datasets-issues-with-comments.jsonl\", repo_type=\"dataset\")\n",
    "# throws TypeError: Couldn't cast array of type\n",
    "#dset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
    "#dset = load_dataset(\"json\", data_files=data_files)\n",
    "# no problem with pandas - note this take a while as the file is >2GB\n",
    "df = pd.read_json(data_files, orient=\"records\", lines=True)\n",
    "#df.head()\n",
    "remote_dataset = Dataset.from_pandas(df)\n",
    "remote_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a47fb415-9b31-440d-80df-c52933030aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'active_lock_reason': [None, None, None],\n",
      " 'assignee': [None, None, None],\n",
      " 'assignees': [[], [], []],\n",
      " 'author_association': ['NONE', 'MEMBER', 'NONE'],\n",
      " 'body': ['## Describe the bug\\r\\n'\n",
      "          'A clear and concise description of what the bug is.\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '## Steps to reproduce the bug\\r\\n'\n",
      "          '```python\\r\\n'\n",
      "          'from datasets import load_dataset, load_metric\\r\\n'\n",
      "          'seqeval = load_metric(\"seqeval\")\\r\\n'\n",
      "          \"seqeval.compute(predictions=[['A']], references=[['A']])\\r\\n\"\n",
      "          '```\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '## Expected results\\r\\n'\n",
      "          'The function computes a dict with metrics\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '## Actual results\\r\\n'\n",
      "          '```\\r\\n'\n",
      "          '---------------------------------------------------------------------------\\r\\n'\n",
      "          'TypeError                                 Traceback (most recent '\n",
      "          'call last)\\r\\n'\n",
      "          '<ipython-input-39-69a57f5cf06f> in <module>\\r\\n'\n",
      "          '      1 from datasets import load_dataset, load_metric\\r\\n'\n",
      "          '      2 seqeval = load_metric(\"seqeval\")\\r\\n'\n",
      "          \"----> 3 seqeval.compute(predictions=[['A']], references=[['A']])\\r\\n\"\n",
      "          '\\r\\n'\n",
      "          '~/p3/lib/python3.7/site-packages/datasets/metric.py in '\n",
      "          'compute(self, *args, **kwargs)\\r\\n'\n",
      "          '    396             references = self.data[\"references\"]\\r\\n'\n",
      "          '    397             with temp_seed(self.seed):\\r\\n'\n",
      "          '--> 398                 output = '\n",
      "          'self._compute(predictions=predictions, references=references, '\n",
      "          '**kwargs)\\r\\n'\n",
      "          '    399 \\r\\n'\n",
      "          '    400             if self.buf_writer is not None:\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '~/.cache/huggingface/modules/datasets_modules/metrics/seqeval/81eda1ff004361d4fa48754a446ec69bb7aa9cf4d14c7215f407d1475941c5ff/seqeval.py '\n",
      "          'in _compute(self, predictions, references, suffix)\\r\\n'\n",
      "          '     95 \\r\\n'\n",
      "          '     96     def _compute(self, predictions, references, '\n",
      "          'suffix=False):\\r\\n'\n",
      "          '---> 97         report = classification_report(y_true=references, '\n",
      "          'y_pred=predictions, suffix=suffix, output_dict=True)\\r\\n'\n",
      "          '     98         report.pop(\"macro avg\")\\r\\n'\n",
      "          '     99         report.pop(\"weighted avg\")\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'TypeError: classification_report() got an unexpected keyword '\n",
      "          \"argument 'output_dict'\\r\\n\"\n",
      "          '```\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '## Environment info\\r\\n'\n",
      "          'sklearn=0.24\\r\\n'\n",
      "          'datasets=1.1.3\\r\\n'\n",
      "          '\\r\\n',\n",
      "          '**Is your feature request related to a problem? Please '\n",
      "          'describe.**\\r\\n'\n",
      "          'Several of the [Wikipedia '\n",
      "          'corpora](https://huggingface.co/datasets?search=wiki) on the Hub '\n",
      "          'involve quite large files that would be a good candidate for '\n",
      "          'streaming. Currently it is not possible to stream these corpora:\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '```python\\r\\n'\n",
      "          'from datasets import load_dataset\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '# Throws ValueError: Builder wikipedia is not streamable.\\r\\n'\n",
      "          'wiki_dataset_streamed = load_dataset(\"wikipedia\", \"20200501.en\", '\n",
      "          'split=\"train\", streaming=True)\\r\\n'\n",
      "          '```\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'Given that these corpora are derived from Wikipedia dumps in XML '\n",
      "          'format which are then processed with Apache Beam, I am not sure '\n",
      "          'whether streaming is possible in principle. The goal of this issue '\n",
      "          'is to discuss whether this feature even makes sense :)\\r\\n'\n",
      "          '\\r\\n'\n",
      "          \"**Describe the solution you'd like**\\r\\n\"\n",
      "          'It would be nice to be able to stream Wikipedia corpora from the '\n",
      "          'Hub with something like\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '```python\\r\\n'\n",
      "          'from datasets import load_dataset\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'wiki_dataset_streamed = load_dataset(\"wikipedia\", \"20200501.en\", '\n",
      "          'split=\"train\", streaming=True)\\r\\n'\n",
      "          '```',\n",
      "          '## Adding a Dataset\\n'\n",
      "          '- **Name:** *name of the dataset*\\n'\n",
      "          '- **Description:** *short description of the dataset (or link to '\n",
      "          'social media or blog post)*\\n'\n",
      "          '- **Paper:** *link to the dataset paper if available*\\n'\n",
      "          '- **Data:** *link to the Github repository or current dataset '\n",
      "          'location*\\n'\n",
      "          '- **Motivation:** *what are some good reasons to have this '\n",
      "          'dataset*\\n'\n",
      "          '\\n'\n",
      "          'Instructions to add a new dataset can be found '\n",
      "          '[here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).'],\n",
      " 'closed_at': [Timestamp('2021-06-17 15:46:07'),\n",
      "               Timestamp('2023-07-20 13:45:30'),\n",
      "               Timestamp('2021-09-10 11:45:54')],\n",
      " 'comments': [['Sorry, I was using an old version of sequeval'],\n",
      "              ['Closing as this has been addressed in '\n",
      "               'https://github.com/huggingface/datasets/pull/5689.'],\n",
      "              []],\n",
      " 'comments_url': ['https://api.github.com/repos/huggingface/datasets/issues/2512/comments',\n",
      "                  'https://api.github.com/repos/huggingface/datasets/issues/2808/comments',\n",
      "                  'https://api.github.com/repos/huggingface/datasets/issues/2889/comments'],\n",
      " 'created_at': [Timestamp('2021-06-17 15:36:02'),\n",
      "                Timestamp('2021-08-16 15:59:12'),\n",
      "                Timestamp('2021-09-10 07:32:07')],\n",
      " 'draft': [None, None, None],\n",
      " 'events_url': ['https://api.github.com/repos/huggingface/datasets/issues/2512/events',\n",
      "                'https://api.github.com/repos/huggingface/datasets/issues/2808/events',\n",
      "                'https://api.github.com/repos/huggingface/datasets/issues/2889/events'],\n",
      " 'html_url': ['https://github.com/huggingface/datasets/issues/2512',\n",
      "              'https://github.com/huggingface/datasets/issues/2808',\n",
      "              'https://github.com/huggingface/datasets/issues/2889'],\n",
      " 'id': [924069353, 971882320, 992968382],\n",
      " 'is_pull_request': [False, False, False],\n",
      " 'labels': [[{'color': 'd73a4a',\n",
      "              'default': True,\n",
      "              'description': \"Something isn't working\",\n",
      "              'id': 1935892857,\n",
      "              'name': 'bug',\n",
      "              'node_id': 'MDU6TGFiZWwxOTM1ODkyODU3',\n",
      "              'url': 'https://api.github.com/repos/huggingface/datasets/labels/bug'}],\n",
      "            [{'color': 'a2eeef',\n",
      "              'default': True,\n",
      "              'description': 'New feature or request',\n",
      "              'id': 1935892871,\n",
      "              'name': 'enhancement',\n",
      "              'node_id': 'MDU6TGFiZWwxOTM1ODkyODcx',\n",
      "              'url': 'https://api.github.com/repos/huggingface/datasets/labels/enhancement'}],\n",
      "            [{'color': 'e99695',\n",
      "              'default': False,\n",
      "              'description': 'Requesting to add a new dataset',\n",
      "              'id': 2067376369,\n",
      "              'name': 'dataset request',\n",
      "              'node_id': 'MDU6TGFiZWwyMDY3Mzc2MzY5',\n",
      "              'url': 'https://api.github.com/repos/huggingface/datasets/labels/dataset%20request'}]],\n",
      " 'labels_url': ['https://api.github.com/repos/huggingface/datasets/issues/2512/labels{/name}',\n",
      "                'https://api.github.com/repos/huggingface/datasets/issues/2808/labels{/name}',\n",
      "                'https://api.github.com/repos/huggingface/datasets/issues/2889/labels{/name}'],\n",
      " 'locked': [False, False, False],\n",
      " 'milestone': [None, None, None],\n",
      " 'node_id': ['MDU6SXNzdWU5MjQwNjkzNTM=',\n",
      "             'MDU6SXNzdWU5NzE4ODIzMjA=',\n",
      "             'MDU6SXNzdWU5OTI5NjgzODI='],\n",
      " 'number': [2512, 2808, 2889],\n",
      " 'performed_via_github_app': [None, None, None],\n",
      " 'pull_request': [None, None, None],\n",
      " 'reactions': [{'+1': 0,\n",
      "                '-1': 0,\n",
      "                'confused': 0,\n",
      "                'eyes': 0,\n",
      "                'heart': 0,\n",
      "                'hooray': 0,\n",
      "                'laugh': 0,\n",
      "                'rocket': 0,\n",
      "                'total_count': 0,\n",
      "                'url': 'https://api.github.com/repos/huggingface/datasets/issues/2512/reactions'},\n",
      "               {'+1': 5,\n",
      "                '-1': 0,\n",
      "                'confused': 0,\n",
      "                'eyes': 0,\n",
      "                'heart': 0,\n",
      "                'hooray': 0,\n",
      "                'laugh': 0,\n",
      "                'rocket': 0,\n",
      "                'total_count': 5,\n",
      "                'url': 'https://api.github.com/repos/huggingface/datasets/issues/2808/reactions'},\n",
      "               {'+1': 0,\n",
      "                '-1': 0,\n",
      "                'confused': 0,\n",
      "                'eyes': 0,\n",
      "                'heart': 0,\n",
      "                'hooray': 0,\n",
      "                'laugh': 0,\n",
      "                'rocket': 0,\n",
      "                'total_count': 0,\n",
      "                'url': 'https://api.github.com/repos/huggingface/datasets/issues/2889/reactions'}],\n",
      " 'repository_url': ['https://api.github.com/repos/huggingface/datasets',\n",
      "                    'https://api.github.com/repos/huggingface/datasets',\n",
      "                    'https://api.github.com/repos/huggingface/datasets'],\n",
      " 'state': ['closed', 'closed', 'closed'],\n",
      " 'state_reason': ['completed', 'completed', 'completed'],\n",
      " 'timeline_url': ['https://api.github.com/repos/huggingface/datasets/issues/2512/timeline',\n",
      "                  'https://api.github.com/repos/huggingface/datasets/issues/2808/timeline',\n",
      "                  'https://api.github.com/repos/huggingface/datasets/issues/2889/timeline'],\n",
      " 'title': ['seqeval metric does not work with a recent version of sklearn: '\n",
      "           'classification_report() got an unexpected keyword argument '\n",
      "           \"'output_dict'\",\n",
      "           'Enable streaming for Wikipedia corpora',\n",
      "           'Coc'],\n",
      " 'updated_at': [Timestamp('2021-06-17 15:46:07'),\n",
      "                Timestamp('2023-07-20 13:45:30'),\n",
      "                Timestamp('2021-09-10 11:45:54')],\n",
      " 'url': ['https://api.github.com/repos/huggingface/datasets/issues/2512',\n",
      "         'https://api.github.com/repos/huggingface/datasets/issues/2808',\n",
      "         'https://api.github.com/repos/huggingface/datasets/issues/2889'],\n",
      " 'user': [{'avatar_url': 'https://avatars.githubusercontent.com/u/8642136?v=4',\n",
      "           'events_url': 'https://api.github.com/users/avidale/events{/privacy}',\n",
      "           'followers_url': 'https://api.github.com/users/avidale/followers',\n",
      "           'following_url': 'https://api.github.com/users/avidale/following{/other_user}',\n",
      "           'gists_url': 'https://api.github.com/users/avidale/gists{/gist_id}',\n",
      "           'gravatar_id': '',\n",
      "           'html_url': 'https://github.com/avidale',\n",
      "           'id': 8642136,\n",
      "           'login': 'avidale',\n",
      "           'node_id': 'MDQ6VXNlcjg2NDIxMzY=',\n",
      "           'organizations_url': 'https://api.github.com/users/avidale/orgs',\n",
      "           'received_events_url': 'https://api.github.com/users/avidale/received_events',\n",
      "           'repos_url': 'https://api.github.com/users/avidale/repos',\n",
      "           'site_admin': False,\n",
      "           'starred_url': 'https://api.github.com/users/avidale/starred{/owner}{/repo}',\n",
      "           'subscriptions_url': 'https://api.github.com/users/avidale/subscriptions',\n",
      "           'type': 'User',\n",
      "           'url': 'https://api.github.com/users/avidale'},\n",
      "          {'avatar_url': 'https://avatars.githubusercontent.com/u/26859204?v=4',\n",
      "           'events_url': 'https://api.github.com/users/lewtun/events{/privacy}',\n",
      "           'followers_url': 'https://api.github.com/users/lewtun/followers',\n",
      "           'following_url': 'https://api.github.com/users/lewtun/following{/other_user}',\n",
      "           'gists_url': 'https://api.github.com/users/lewtun/gists{/gist_id}',\n",
      "           'gravatar_id': '',\n",
      "           'html_url': 'https://github.com/lewtun',\n",
      "           'id': 26859204,\n",
      "           'login': 'lewtun',\n",
      "           'node_id': 'MDQ6VXNlcjI2ODU5MjA0',\n",
      "           'organizations_url': 'https://api.github.com/users/lewtun/orgs',\n",
      "           'received_events_url': 'https://api.github.com/users/lewtun/received_events',\n",
      "           'repos_url': 'https://api.github.com/users/lewtun/repos',\n",
      "           'site_admin': False,\n",
      "           'starred_url': 'https://api.github.com/users/lewtun/starred{/owner}{/repo}',\n",
      "           'subscriptions_url': 'https://api.github.com/users/lewtun/subscriptions',\n",
      "           'type': 'User',\n",
      "           'url': 'https://api.github.com/users/lewtun'},\n",
      "          {'avatar_url': 'https://avatars.githubusercontent.com/u/90444264?v=4',\n",
      "           'events_url': 'https://api.github.com/users/Bwiggity/events{/privacy}',\n",
      "           'followers_url': 'https://api.github.com/users/Bwiggity/followers',\n",
      "           'following_url': 'https://api.github.com/users/Bwiggity/following{/other_user}',\n",
      "           'gists_url': 'https://api.github.com/users/Bwiggity/gists{/gist_id}',\n",
      "           'gravatar_id': '',\n",
      "           'html_url': 'https://github.com/Bwiggity',\n",
      "           'id': 90444264,\n",
      "           'login': 'Bwiggity',\n",
      "           'node_id': 'MDQ6VXNlcjkwNDQ0MjY0',\n",
      "           'organizations_url': 'https://api.github.com/users/Bwiggity/orgs',\n",
      "           'received_events_url': 'https://api.github.com/users/Bwiggity/received_events',\n",
      "           'repos_url': 'https://api.github.com/users/Bwiggity/repos',\n",
      "           'site_admin': False,\n",
      "           'starred_url': 'https://api.github.com/users/Bwiggity/starred{/owner}{/repo}',\n",
      "           'subscriptions_url': 'https://api.github.com/users/Bwiggity/subscriptions',\n",
      "           'type': 'User',\n",
      "           'url': 'https://api.github.com/users/Bwiggity'}]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "sample = remote_dataset.shuffle(seed=777).select(range(3))\n",
    "pprint(sample[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb8d8e-0c4c-4a1a-8843-7fb0011717c2",
   "metadata": {},
   "source": [
    "### 5. FAISS를 이용한 시맨틱 검색\n",
    "이전 섹션에서는 🤗Datasets 저장소의 GitHub 이슈(issues) 및 의견(comments) 데이터셋을 만들어봤습니다. 이 섹션에서는 이 정보를 사용하여 🤗Datasets 라이브러리에 대한 가장 시급한 질문에 대한 답변을 찾는데 도움이 될 수 있는 검색 엔진을 구축할 것입니다!\n",
    "\n",
    "\n",
    "#### 시맨틱 검색을 위한 임베딩 사용하기\n",
    "---\n",
    "1장에서 보았듯이 트랜스포머(Transformer) 기반 언어 모델은 텍스트 내의 각 토큰을 임베딩 벡터(embedding vector) 로 나타냅니다. 개별 임베딩을 \"풀링(pooling)\"하여 전체 문장, 단락 또는 (경우에 따라) 문서에 대한 벡터 표현을 생성할 수 있습니다. 그런 다음 이러한 임베딩을 사용하여 각 임베딩 사이의 내적 유사도(dot-product similarity), 또는 다른 유사도 메트릭(similarity metric)을 계산하고 가장 많이 겹치는 문서를 반환하여 코퍼스에서 유사 문서 검색을 수행할 수 있습니다.\n",
    "\n",
    "이 섹션에서는 임베딩을 사용하여 시맨틱 검색 엔진을 개발할 것입니다. 이러한 검색 엔진은 쿼리와 문서의 키워드 매칭을 기반으로 하는 기존 접근 방식에 비해 몇 가지 장점을 제공합니다.\n",
    "\n",
    "#### 데이터셋 로딩 및 준비 작업\n",
    "---\n",
    "가장 먼저 해야 할 일은 이전 섹션에서 우리가 만든 GitHub 이슈 데이터셋을 다운로드하는 것입니다. 이에 🤗Hub 라이브러리를 사용하여 Hugging Face Hub에서 파일이 저장된 URL을 확인합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9307ff11-7a09-413e-84da-36bd4e31f702",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (3652608768.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    issues_dataset = load_dataset(\"json\", datafiles='hwang2006/datasets-github-issues-2024-08-23/datasets-issues-with-comments.jsonl\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "issues_dataset = load_dataset(\"json\", datafiles='hwang2006/datasets-github-issues-2024-08-23/datasets-issues-with-comments.jsonl\n",
    "', split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ec4e23-d627-40e7-88d0-6fe3c6d85a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_url\n",
    "\n",
    "data_files = hf_hub_url(\n",
    "    repo_id=\"spasis/datasets-github-issues\",\n",
    "    #repo_id=\"qualis2006/datasets-github-issues\",\n",
    "    filename=\"datasets-issues-with-comments.jsonl\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "648c3320-379d-455a-90ff-59d9e4faedc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_url\n",
    "\n",
    "data_files = hf_hub_url(\n",
    "    repo_id=\"hwang2006/datasets-github-issues-2024-08-23\",\n",
    "    #repo_id=\"qualis2006/datasets-github-issues\",\n",
    "    filename=\"datasets-issues-with-comments.jsonl\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "862ebd5e-99cc-4bea-b6f3-c397499c9fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/hwang2006/datasets-github-issues-2024-08-23/resolve/main/datasets-issues-with-comments.jsonl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e1bdfb0-6d7c-4e4b-9143-11cd88e62726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9314411dbc884224b2e8af805a153522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/builder.py:1989\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/arrow_writer.py:584\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    583\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mcombine_chunks()\n\u001b[0;32m--> 584\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_local_files:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:2240\u001b[0m, in \u001b[0;36mtable_cast\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m!=\u001b[39m schema:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m!=\u001b[39m schema\u001b[38;5;241m.\u001b[39mmetadata:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:2199\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[1;32m   2195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2196\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m   2197\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[1;32m   2198\u001b[0m     )\n\u001b[0;32m-> 2199\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [cast_array_to_feature(table[name], feature) \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:2199\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[1;32m   2195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2196\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m   2197\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[1;32m   2198\u001b[0m     )\n\u001b[0;32m-> 2199\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1793\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray):\n\u001b[0;32m-> 1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([func(chunk, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1793\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray):\n\u001b[0;32m-> 1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mchunked_array([\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array\u001b[38;5;241m.\u001b[39mchunks])\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1984\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 1984\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [_c(array\u001b[38;5;241m.\u001b[39mfield(name), subfeature) \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1984\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m-> 1984\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfeature\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(feature), mask\u001b[38;5;241m=\u001b[39marray\u001b[38;5;241m.\u001b[39mis_null())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1795\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:2065\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[0;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, (Sequence, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 2065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_number_to_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_number_to_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1795\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[0;34m(array, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/table.py:1936\u001b[0m, in \u001b[0;36marray_cast\u001b[0;34m(array, pa_type, allow_number_to_str)\u001b[0m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(pa_type) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_null(array\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m-> 1936\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast array of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpa_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\u001b[38;5;241m.\u001b[39mcast(pa_type)\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't cast array of type timestamp[s] to null",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m issues_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m issues_dataset\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:2582\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2582\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2591\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2592\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2593\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2594\u001b[0m )\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/builder.py:1005\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/builder.py:1100\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1107\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/builder.py:1860\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1858\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1860\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1861\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1862\u001b[0m     ):\n\u001b[1;32m   1863\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1864\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/builder.py:2016\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[1;32m   2015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
    "issues_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d7cc1a-24f9-42e2-8da3-1b0a29034519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
      "        num_rows: 3651\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# DatasetDict 반환\n",
    "print(load_dataset(\"json\", data_files=data_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6507bf4c-e655-4c4d-8524-3966e0d8f5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'is_pull_request'],\n",
       "    num_rows: 7076\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_url\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# returns 'https://huggingface.co/datasets/Evan/spaCy-github-issues/resolve/main/spacy-issues.jsonl'\n",
    "#data_files = hf_hub_url(repo_id=\"hwang2006/huggingface-datasets-issues-2024-03-20\", filename=\"datasets-issues-with-comments.jsonl\", repo_type=\"dataset\")\n",
    "data_files = hf_hub_url(repo_id=\"hwang2006/datasets-github-issues-2024-08-23\", filename=\"datasets-issues-with-comments.jsonl\", repo_type=\"dataset\")\n",
    "# throws TypeError: Couldn't cast array of type\n",
    "#dset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
    "#dset = load_dataset(\"json\", data_files=data_files)\n",
    "# no problem with pandas - note this take a while as the file is >2GB\n",
    "df = pd.read_json(data_files, orient=\"records\", lines=True)\n",
    "#df.head()\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14cbbf2-dccb-41c9-9da6-ae86142d07c5",
   "metadata": {},
   "source": [
    "여기에서 load_dataset()에 기본 train 분할(split)을 지정했으므로 DatasetDict 대신 Dataset을 반환합니다. 작업의 첫 번째 순서는 pull requests를 필터링하는 것입니다. Pull requests는 사용자 쿼리(query)에 응답하는데 거의 사용되지 않고 검색 엔진에 노이즈를 발생시키기 때문입니다. 지금은 어느 정도 익숙해졌겠지만, Dataset.filter() 함수를 사용하여 데이터셋에서 이러한 행(rows)을 제거할 수 있습니다. 이와 동시에 사용자 쿼리에 대한 답변을 제공할 수 없는 주석이 없는 행들도 필터링해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ddfc54d-7e00-45ea-a175-13e80726a450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007e7fcae5194d35b6083ac29ea53576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7076 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'is_pull_request'],\n",
       "    num_rows: 5308\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (len(x[\"comments\"]) > 0)\n",
    ")\n",
    "issues_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6b7fe6-bb15-4ef8-9b29-5b34b192c034",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/datasets/pr_7119). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.',\n",
       "  '<details>\\n<summary>Show benchmarks</summary>\\n\\nPyArrow==8.0.0\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.005156 / 0.011353 (-0.006197) | 0.003365 / 0.011008 (-0.007643) | 0.063451 / 0.038508 (0.024943) | 0.029510 / 0.023109 (0.006401) | 0.244825 / 0.275898 (-0.031074) | 0.265157 / 0.323480 (-0.058323) | 0.004239 / 0.007986 (-0.003747) | 0.002732 / 0.004328 (-0.001596) | 0.050412 / 0.004250 (0.046162) | 0.043608 / 0.037052 (0.006556) | 0.256635 / 0.258489 (-0.001854) | 0.277472 / 0.293841 (-0.016369) | 0.029329 / 0.128546 (-0.099217) | 0.012318 / 0.075646 (-0.063329) | 0.204751 / 0.419271 (-0.214520) | 0.036468 / 0.043533 (-0.007065) | 0.246773 / 0.255139 (-0.008366) | 0.263932 / 0.283200 (-0.019268) | 0.017053 / 0.141683 (-0.124629) | 1.173249 / 1.452155 (-0.278905) | 1.234186 / 1.492716 (-0.258531) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.092398 / 0.018006 (0.074391) | 0.309473 / 0.000490 (0.308983) | 0.000220 / 0.000200 (0.000020) | 0.000050 / 0.000054 (-0.000004) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.018553 / 0.037411 (-0.018858) | 0.062546 / 0.014526 (0.048020) | 0.073943 / 0.176557 (-0.102613) | 0.120498 / 0.737135 (-0.616638) | 0.075185 / 0.296338 (-0.221153) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.296899 / 0.215209 (0.081690) | 2.919088 / 2.077655 (0.841433) | 1.533146 / 1.504120 (0.029026) | 1.395441 / 1.541195 (-0.145754) | 1.399089 / 1.468490 (-0.069401) | 0.742750 / 4.584777 (-3.842027) | 2.390317 / 3.745712 (-1.355395) | 2.883166 / 5.269862 (-2.386695) | 1.854003 / 4.565676 (-2.711674) | 0.077140 / 0.424275 (-0.347136) | 0.005176 / 0.007607 (-0.002432) | 0.349391 / 0.226044 (0.123347) | 3.466043 / 2.268929 (1.197114) | 1.870619 / 55.444624 (-53.574005) | 1.559173 / 6.876477 (-5.317303) | 1.605480 / 2.142072 (-0.536592) | 0.786753 / 4.805227 (-4.018474) | 0.134869 / 6.500664 (-6.365795) | 0.042176 / 0.075469 (-0.033293) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.954256 / 1.841788 (-0.887532) | 11.194758 / 8.074308 (3.120449) | 9.129670 / 10.191392 (-1.061722) | 0.138318 / 0.680424 (-0.542106) | 0.014299 / 0.534201 (-0.519902) | 0.303704 / 0.579283 (-0.275579) | 0.262513 / 0.434364 (-0.171851) | 0.346539 / 0.540337 (-0.193798) | 0.429524 / 1.386936 (-0.957412) |\\n\\n</details>\\nPyArrow==latest\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.005692 / 0.011353 (-0.005661) | 0.003423 / 0.011008 (-0.007586) | 0.050618 / 0.038508 (0.012110) | 0.031053 / 0.023109 (0.007944) | 0.275901 / 0.275898 (0.000003) | 0.294404 / 0.323480 (-0.029076) | 0.004303 / 0.007986 (-0.003682) | 0.002728 / 0.004328 (-0.001600) | 0.049757 / 0.004250 (0.045507) | 0.039997 / 0.037052 (0.002945) | 0.287291 / 0.258489 (0.028802) | 0.319186 / 0.293841 (0.025345) | 0.032558 / 0.128546 (-0.095988) | 0.012088 / 0.075646 (-0.063558) | 0.060746 / 0.419271 (-0.358525) | 0.034046 / 0.043533 (-0.009486) | 0.276170 / 0.255139 (0.021031) | 0.293673 / 0.283200 (0.010474) | 0.018018 / 0.141683 (-0.123665) | 1.158453 / 1.452155 (-0.293701) | 1.198599 / 1.492716 (-0.294118) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.093134 / 0.018006 (0.075127) | 0.304511 / 0.000490 (0.304021) | 0.000216 / 0.000200 (0.000016) | 0.000053 / 0.000054 (-0.000002) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.022991 / 0.037411 (-0.014421) | 0.077548 / 0.014526 (0.063022) | 0.087887 / 0.176557 (-0.088670) | 0.131786 / 0.737135 (-0.605349) | 0.088747 / 0.296338 (-0.207591) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.302811 / 0.215209 (0.087602) | 2.959276 / 2.077655 (0.881621) | 1.591348 / 1.504120 (0.087229) | 1.464731 / 1.541195 (-0.076464) | 1.474112 / 1.468490 (0.005622) | 0.741573 / 4.584777 (-3.843204) | 0.959229 / 3.745712 (-2.786483) | 2.895750 / 5.269862 (-2.374111) | 1.896051 / 4.565676 (-2.669625) | 0.079012 / 0.424275 (-0.345264) | 0.005494 / 0.007607 (-0.002113) | 0.355699 / 0.226044 (0.129655) | 3.524833 / 2.268929 (1.255905) | 1.972358 / 55.444624 (-53.472266) | 1.667249 / 6.876477 (-5.209228) | 1.658635 / 2.142072 (-0.483438) | 0.813184 / 4.805227 (-3.992044) | 0.134226 / 6.500664 (-6.366438) | 0.041087 / 0.075469 (-0.034382) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 1.038963 / 1.841788 (-0.802824) | 11.785835 / 8.074308 (3.711526) | 10.397027 / 10.191392 (0.205635) | 0.141748 / 0.680424 (-0.538676) | 0.014738 / 0.534201 (-0.519463) | 0.300056 / 0.579283 (-0.279227) | 0.127442 / 0.434364 (-0.306922) | 0.345013 / 0.540337 (-0.195324) | 0.449598 / 1.386936 (-0.937338) |\\n\\n</details>\\n</details>\\n\\n![](https://cml.dev/watermark.png#70bac27ef861b2b11f581a291a6b76adeee24f98 \"CML watermark\")\\n'],\n",
       " ['The docs for this PR live [here](https://moon-ci-docs.huggingface.co/docs/datasets/pr_7118). All of your documentation changes will be reflected on that endpoint. The docs are available until 30 days after the last update.',\n",
       "  '<details>\\n<summary>Show benchmarks</summary>\\n\\nPyArrow==8.0.0\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.005674 / 0.011353 (-0.005679) | 0.003919 / 0.011008 (-0.007089) | 0.062665 / 0.038508 (0.024157) | 0.031750 / 0.023109 (0.008641) | 0.234809 / 0.275898 (-0.041089) | 0.264454 / 0.323480 (-0.059026) | 0.004265 / 0.007986 (-0.003720) | 0.002757 / 0.004328 (-0.001572) | 0.048921 / 0.004250 (0.044671) | 0.050765 / 0.037052 (0.013713) | 0.246185 / 0.258489 (-0.012305) | 0.287011 / 0.293841 (-0.006829) | 0.030754 / 0.128546 (-0.097792) | 0.012368 / 0.075646 (-0.063278) | 0.203841 / 0.419271 (-0.215431) | 0.037579 / 0.043533 (-0.005953) | 0.238165 / 0.255139 (-0.016974) | 0.264375 / 0.283200 (-0.018824) | 0.018663 / 0.141683 (-0.123020) | 1.143897 / 1.452155 (-0.308258) | 1.218130 / 1.492716 (-0.274586) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.102112 / 0.018006 (0.084106) | 0.303214 / 0.000490 (0.302724) | 0.000232 / 0.000200 (0.000032) | 0.000043 / 0.000054 (-0.000011) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.019401 / 0.037411 (-0.018010) | 0.062444 / 0.014526 (0.047919) | 0.076497 / 0.176557 (-0.100060) | 0.122309 / 0.737135 (-0.614826) | 0.077178 / 0.296338 (-0.219160) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.282931 / 0.215209 (0.067722) | 2.783587 / 2.077655 (0.705932) | 1.464076 / 1.504120 (-0.040044) | 1.333912 / 1.541195 (-0.207282) | 1.367391 / 1.468490 (-0.101099) | 0.736702 / 4.584777 (-3.848075) | 2.413625 / 3.745712 (-1.332087) | 2.949549 / 5.269862 (-2.320313) | 1.910308 / 4.565676 (-2.655369) | 0.077419 / 0.424275 (-0.346856) | 0.005159 / 0.007607 (-0.002448) | 0.345595 / 0.226044 (0.119551) | 3.433205 / 2.268929 (1.164277) | 1.844443 / 55.444624 (-53.600181) | 1.527475 / 6.876477 (-5.349002) | 1.544315 / 2.142072 (-0.597758) | 0.803942 / 4.805227 (-4.001285) | 0.134131 / 6.500664 (-6.366533) | 0.042638 / 0.075469 (-0.032831) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.975158 / 1.841788 (-0.866629) | 11.726187 / 8.074308 (3.651879) | 9.403347 / 10.191392 (-0.788045) | 0.131583 / 0.680424 (-0.548840) | 0.014358 / 0.534201 (-0.519843) | 0.301360 / 0.579283 (-0.277923) | 0.266529 / 0.434364 (-0.167835) | 0.341669 / 0.540337 (-0.198668) | 0.425751 / 1.386936 (-0.961186) |\\n\\n</details>\\nPyArrow==latest\\n\\n<details>\\n<summary>Show updated benchmarks!</summary>\\n\\n### Benchmark: benchmark_array_xd.json\\n\\n| metric | read_batch_formatted_as_numpy after write_array2d | read_batch_formatted_as_numpy after write_flattened_sequence | read_batch_formatted_as_numpy after write_nested_sequence | read_batch_unformated after write_array2d | read_batch_unformated after write_flattened_sequence | read_batch_unformated after write_nested_sequence | read_col_formatted_as_numpy after write_array2d | read_col_formatted_as_numpy after write_flattened_sequence | read_col_formatted_as_numpy after write_nested_sequence | read_col_unformated after write_array2d | read_col_unformated after write_flattened_sequence | read_col_unformated after write_nested_sequence | read_formatted_as_numpy after write_array2d | read_formatted_as_numpy after write_flattened_sequence | read_formatted_as_numpy after write_nested_sequence | read_unformated after write_array2d | read_unformated after write_flattened_sequence | read_unformated after write_nested_sequence | write_array2d | write_flattened_sequence | write_nested_sequence |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.005911 / 0.011353 (-0.005442) | 0.004093 / 0.011008 (-0.006915) | 0.049936 / 0.038508 (0.011428) | 0.031828 / 0.023109 (0.008719) | 0.273874 / 0.275898 (-0.002025) | 0.296871 / 0.323480 (-0.026609) | 0.004470 / 0.007986 (-0.003516) | 0.002902 / 0.004328 (-0.001426) | 0.048848 / 0.004250 (0.044597) | 0.042320 / 0.037052 (0.005268) | 0.287957 / 0.258489 (0.029468) | 0.321033 / 0.293841 (0.027192) | 0.032996 / 0.128546 (-0.095550) | 0.012244 / 0.075646 (-0.063403) | 0.060493 / 0.419271 (-0.358779) | 0.034630 / 0.043533 (-0.008902) | 0.277254 / 0.255139 (0.022115) | 0.292822 / 0.283200 (0.009623) | 0.017966 / 0.141683 (-0.123717) | 1.167432 / 1.452155 (-0.284723) | 1.231837 / 1.492716 (-0.260880) |\\n\\n### Benchmark: benchmark_getitem\\\\_100B.json\\n\\n| metric | get_batch_of\\\\_1024\\\\_random_rows | get_batch_of\\\\_1024\\\\_rows | get_first_row | get_last_row |\\n|--------|---|---|---|---|\\n| new / old (diff) | 0.099970 / 0.018006 (0.081964) | 0.313240 / 0.000490 (0.312750) | 0.000217 / 0.000200 (0.000017) | 0.000043 / 0.000054 (-0.000011) |\\n\\n### Benchmark: benchmark_indices_mapping.json\\n\\n| metric | select | shard | shuffle | sort | train_test_split |\\n|--------|---|---|---|---|---|\\n| new / old (diff) | 0.022928 / 0.037411 (-0.014483) | 0.077058 / 0.014526 (0.062532) | 0.090147 / 0.176557 (-0.086409) | 0.129416 / 0.737135 (-0.607720) | 0.091021 / 0.296338 (-0.205318) |\\n\\n### Benchmark: benchmark_iterating.json\\n\\n| metric | read 5000 | read 50000 | read_batch 50000 10 | read_batch 50000 100 | read_batch 50000 1000 | read_formatted numpy 5000 | read_formatted pandas 5000 | read_formatted tensorflow 5000 | read_formatted torch 5000 | read_formatted_batch numpy 5000 10 | read_formatted_batch numpy 5000 1000 | shuffled read 5000 | shuffled read 50000 | shuffled read_batch 50000 10 | shuffled read_batch 50000 100 | shuffled read_batch 50000 1000 | shuffled read_formatted numpy 5000 | shuffled read_formatted_batch numpy 5000 10 | shuffled read_formatted_batch numpy 5000 1000 |\\n|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 0.300697 / 0.215209 (0.085488) | 2.944649 / 2.077655 (0.866995) | 1.609106 / 1.504120 (0.104986) | 1.483762 / 1.541195 (-0.057433) | 1.519433 / 1.468490 (0.050943) | 0.714129 / 4.584777 (-3.870648) | 0.991848 / 3.745712 (-2.753864) | 2.966340 / 5.269862 (-2.303521) | 1.905427 / 4.565676 (-2.660249) | 0.079041 / 0.424275 (-0.345234) | 0.005671 / 0.007607 (-0.001936) | 0.356037 / 0.226044 (0.129993) | 3.504599 / 2.268929 (1.235670) | 1.979207 / 55.444624 (-53.465417) | 1.695030 / 6.876477 (-5.181447) | 1.703978 / 2.142072 (-0.438095) | 0.800871 / 4.805227 (-4.004357) | 0.134414 / 6.500664 (-6.366250) | 0.041743 / 0.075469 (-0.033726) |\\n\\n### Benchmark: benchmark_map_filter.json\\n\\n| metric | filter | map fast-tokenizer batched | map identity | map identity batched | map no-op batched | map no-op batched numpy | map no-op batched pandas | map no-op batched pytorch | map no-op batched tensorflow |\\n|--------|---|---|---|---|---|---|---|---|---|\\n| new / old (diff) | 1.029879 / 1.841788 (-0.811909) | 12.132252 / 8.074308 (4.057944) | 10.596576 / 10.191392 (0.405184) | 0.132237 / 0.680424 (-0.548187) | 0.016239 / 0.534201 (-0.517962) | 0.301831 / 0.579283 (-0.277452) | 0.127966 / 0.434364 (-0.306398) | 0.341081 / 0.540337 (-0.199256) | 0.448996 / 1.386936 (-0.937940) |\\n\\n</details>\\n</details>\\n\\n![](https://cml.dev/watermark.png#0a0fa48a68c3502edfa50273b881f909e4e6e70c \"CML watermark\")\\n']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset[:2][\"comments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e905ee-5688-4baa-b7da-5ff9fd8a4868",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fe13e06a264e60aa0ac85a20669b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'is_pull_request'],\n",
       "    num_rows: 2424\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False)\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022285b7-0492-4241-a5e7-92fe86a8dd79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb751e5c2c3400b89cb76e3fb664fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'is_pull_request'],\n",
       "    num_rows: 2424\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")\n",
    "issues_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa2f2e-f639-4bb2-a2db-364fc6f0af43",
   "metadata": {},
   "source": [
    "데이터셋에 많은 열(columns)이 있음을 알 수 있으며 이들 대부분은 검색 대상이 아닙니다. 검색 관점에서 가장 유익한 열은 title, body 및 comments이며 html_url은 소스 문제에 대한 링크를 제공합니다. Dataset.remove_columns() 함수를 사용하여 그외의 나머지 열을 삭제해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191b8efa-fdef-4e85-8455-7da6f2300e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assignees', 'number', 'draft', 'pull_request', 'events_url', 'author_association', 'performed_via_github_app', 'assignee', 'timeline_url', 'locked', 'repository_url', 'node_id', 'active_lock_reason', 'updated_at', 'url', 'comments_url', 'closed_at', 'reactions', 'state', 'state_reason', 'created_at', 'is_pull_request', 'user', 'id', 'labels', 'labels_url', 'milestone'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2424\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "print(columns_to_remove)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bb825-c654-4512-8be1-c6bfd4abc7e6",
   "metadata": {},
   "source": [
    "임베딩을 생성하기 위해 각 이슈의 comments에 해당 이슈의 title과 body 내용을 추가할 것입니다. 이러한 필드들에는 종종 유용한 컨텍스트 정보가 포함되기 때문입니다. 현재 데이터셋의 comments 열(column)은 현재 각 이슈에 대한 여러 개의 comments 리스트이므로 각 행이 (html_url, title, body, comment) 튜플로 구성되도록 해당 열을 \"분해(explode, 단일 행의 특정 필드가 여러 항목을 포함하는 경우, 이를 확장하여 여러 개의 행으로 확장하는 기법)\"시켜야 합니다. Pandas에서는 DataFrame.explode() 함수를 사용하여 이 작업을 수행할 수 있습니다. 이 함수는 리스트 형태로 구성된 열(list-like column)의 각 요소에 대해 새로운 행을 생성하고 해당 열이 아닌 다른 모든 열 값을 최초 행의 값으로 복제합니다. 이 동작을 확인하기 위해 먼저 Pandas DataFrame 형식으로 전환해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f0906e-a077-4477-bf92-bb4c34376a72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba5eaf5e-852a-423e-b3ce-27ba30908dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"comments\"][5].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c51f3fe-228c-4fd2-9c80-57fd3f0d265a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Audio dataset load everything in RAM and is ve...</td>\n",
       "      <td>[Hi ! I think the issue comes from the fact th...</td>\n",
       "      <td>Hello, I'm working with an audio dataset. I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>datasets cannot handle nested json if features...</td>\n",
       "      <td>[Hi ! `Sequence` has a weird behavior for dict...</td>\n",
       "      <td>### Describe the bug\\n\\nI have a json named te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>module 'pyarrow.lib' has no attribute 'ListVie...</td>\n",
       "      <td>[https://github.com/neurafusionai/Hugging_Face...</td>\n",
       "      <td>### Describe the bug\\n\\nCode:\\r\\n`!pipuninstal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Stream dataset does not iterate if the batch s...</td>\n",
       "      <td>[That's expected behavior, it's also the same ...</td>\n",
       "      <td>### Describe the bug\\r\\n\\r\\nHi there,\\r\\n\\r\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>cudf-cu12 24.4.1, ibis-framework 8.0.0 require...</td>\n",
       "      <td>[@sayakpaul  please advice ]</td>\n",
       "      <td>### Describe the bug\\n\\n!pip install accelerat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Audio dataset load everything in RAM and is ve...   \n",
       "1  datasets cannot handle nested json if features...   \n",
       "2  module 'pyarrow.lib' has no attribute 'ListVie...   \n",
       "3  Stream dataset does not iterate if the batch s...   \n",
       "4  cudf-cu12 24.4.1, ibis-framework 8.0.0 require...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  [Hi ! I think the issue comes from the fact th...   \n",
       "1  [Hi ! `Sequence` has a weird behavior for dict...   \n",
       "2  [https://github.com/neurafusionai/Hugging_Face...   \n",
       "3  [That's expected behavior, it's also the same ...   \n",
       "4                       [@sayakpaul  please advice ]   \n",
       "\n",
       "                                                body  \n",
       "0  Hello, I'm working with an audio dataset. I wa...  \n",
       "1  ### Describe the bug\\n\\nI have a json named te...  \n",
       "2  ### Describe the bug\\n\\nCode:\\r\\n`!pipuninstal...  \n",
       "3  ### Describe the bug\\r\\n\\r\\nHi there,\\r\\n\\r\\nI...  \n",
       "4  ### Describe the bug\\n\\n!pip install accelerat...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06fe55f2-c9fa-4494-a3f1-da989b71e24d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Note that the CI before was using:\\r\\n- llvmlite: 0.43.0\\r\\n- numba: 0.60.0\\r\\n\\r\\nNow it tries to use:\\r\\n- llvmlite: 0.34.0\\r\\n- numba: 0.51.2',\n",
       "       'The issue is because numba-0.60.0 pins numpy<2.1 and `uv` tries to install latest numpy-2.1.0 with an old numba-0.51.0 version (and llvmlite-0.34.0). See discussion in their repo:\\r\\n- https://github.com/numba/numba/issues/9708\\r\\n\\r\\nLatest numpy-2.1.0 will be supported by the next numba-0.61.0 release in September.\\r\\n\\r\\nNote that our CI requires numba with the \"audio\" extra:\\r\\n- librosa > numba'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "667c6676-1b79-429e-b0fd-d76570bfa637",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Note that the CI before was using:\\r\\n- llvmlite: 0.43.0\\r\\n- numba: 0.60.0\\r\\n\\r\\nNow it tries to use:\\r\\n- llvmlite: 0.34.0\\r\\n- numba: 0.51.2',\n",
       " 'The issue is because numba-0.60.0 pins numpy<2.1 and `uv` tries to install latest numpy-2.1.0 with an old numba-0.51.0 version (and llvmlite-0.34.0). See discussion in their repo:\\r\\n- https://github.com/numba/numba/issues/9708\\r\\n\\r\\nLatest numpy-2.1.0 will be supported by the next numba-0.61.0 release in September.\\r\\n\\r\\nNote that our CI requires numba with the \"audio\" extra:\\r\\n- librosa > numba']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments\"][5].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a243c-fcff-4645-b5b5-d3bec2480d65",
   "metadata": {},
   "source": [
    "df를 분해(explode)할 때 이러한 각 comment에 대해 하나의 행이 새롭게 생성될 것으로 예상합니다. 다음과 같은 경우인지 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e19030a-fef4-4280-b295-333b46596e81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Audio dataset load everything in RAM and is ve...</td>\n",
       "      <td>Hi ! I think the issue comes from the fact tha...</td>\n",
       "      <td>Hello, I'm working with an audio dataset. I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>datasets cannot handle nested json if features...</td>\n",
       "      <td>Hi ! `Sequence` has a weird behavior for dicti...</td>\n",
       "      <td>### Describe the bug\\n\\nI have a json named te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>module 'pyarrow.lib' has no attribute 'ListVie...</td>\n",
       "      <td>https://github.com/neurafusionai/Hugging_Face/...</td>\n",
       "      <td>### Describe the bug\\n\\nCode:\\r\\n`!pipuninstal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Stream dataset does not iterate if the batch s...</td>\n",
       "      <td>That's expected behavior, it's also the same i...</td>\n",
       "      <td>### Describe the bug\\r\\n\\r\\nHi there,\\r\\n\\r\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>cudf-cu12 24.4.1, ibis-framework 8.0.0 require...</td>\n",
       "      <td>@sayakpaul  please advice</td>\n",
       "      <td>### Describe the bug\\n\\n!pip install accelerat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>CI is broken for numpy-2: Failed to fetch whee...</td>\n",
       "      <td>Note that the CI before was using:\\r\\n- llvmli...</td>\n",
       "      <td>Ci is broken with error `Failed to fetch wheel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>CI is broken for numpy-2: Failed to fetch whee...</td>\n",
       "      <td>The issue is because numba-0.60.0 pins numpy&lt;2...</td>\n",
       "      <td>Ci is broken with error `Failed to fetch wheel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>website broken: Create a new dataset repositor...</td>\n",
       "      <td>I don't reproduce, I was able to create a new ...</td>\n",
       "      <td>### Describe the bug\\r\\n\\r\\nThis issue is also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>website broken: Create a new dataset repositor...</td>\n",
       "      <td>I have just tried again.\\r\\n\\r\\nFirefox: The `...</td>\n",
       "      <td>### Describe the bug\\r\\n\\r\\nThis issue is also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>website broken: Create a new dataset repositor...</td>\n",
       "      <td>I have updated Firefox 129.0 (64 bit), and now...</td>\n",
       "      <td>### Describe the bug\\r\\n\\r\\nThis issue is also...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "5  https://github.com/huggingface/datasets/issues...   \n",
       "6  https://github.com/huggingface/datasets/issues...   \n",
       "7  https://github.com/huggingface/datasets/issues...   \n",
       "8  https://github.com/huggingface/datasets/issues...   \n",
       "9  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Audio dataset load everything in RAM and is ve...   \n",
       "1  datasets cannot handle nested json if features...   \n",
       "2  module 'pyarrow.lib' has no attribute 'ListVie...   \n",
       "3  Stream dataset does not iterate if the batch s...   \n",
       "4  cudf-cu12 24.4.1, ibis-framework 8.0.0 require...   \n",
       "5  CI is broken for numpy-2: Failed to fetch whee...   \n",
       "6  CI is broken for numpy-2: Failed to fetch whee...   \n",
       "7  website broken: Create a new dataset repositor...   \n",
       "8  website broken: Create a new dataset repositor...   \n",
       "9  website broken: Create a new dataset repositor...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  Hi ! I think the issue comes from the fact tha...   \n",
       "1  Hi ! `Sequence` has a weird behavior for dicti...   \n",
       "2  https://github.com/neurafusionai/Hugging_Face/...   \n",
       "3  That's expected behavior, it's also the same i...   \n",
       "4                         @sayakpaul  please advice    \n",
       "5  Note that the CI before was using:\\r\\n- llvmli...   \n",
       "6  The issue is because numba-0.60.0 pins numpy<2...   \n",
       "7  I don't reproduce, I was able to create a new ...   \n",
       "8  I have just tried again.\\r\\n\\r\\nFirefox: The `...   \n",
       "9  I have updated Firefox 129.0 (64 bit), and now...   \n",
       "\n",
       "                                                body  \n",
       "0  Hello, I'm working with an audio dataset. I wa...  \n",
       "1  ### Describe the bug\\n\\nI have a json named te...  \n",
       "2  ### Describe the bug\\n\\nCode:\\r\\n`!pipuninstal...  \n",
       "3  ### Describe the bug\\r\\n\\r\\nHi there,\\r\\n\\r\\nI...  \n",
       "4  ### Describe the bug\\n\\n!pip install accelerat...  \n",
       "5  Ci is broken with error `Failed to fetch wheel...  \n",
       "6  Ci is broken with error `Failed to fetch wheel...  \n",
       "7  ### Describe the bug\\r\\n\\r\\nThis issue is also...  \n",
       "8  ### Describe the bug\\r\\n\\r\\nThis issue is also...  \n",
       "9  ### Describe the bug\\r\\n\\r\\nThis issue is also...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52635258-fd3b-4bbb-93b6-8fbeb7df6e66",
   "metadata": {},
   "source": [
    "개별 comment가 포함된 comments 열과 함께 행이 복제된 것을 볼 수 있습니다! 이제 Pandas 작업을 마쳤으므로 DataFrame을 메모리에 로드하여 Dataset으로 빠르게 다시 전환할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0207781-3f2b-4f9b-8b47-0e6f3a62a6d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 9226\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203cabf2-fcf5-420e-aad3-c24ca8f7d109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df11c48efc464d7da1dcda4573bd0998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 9226\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b4817-ae4e-4a93-9ea8-6ff44a34c85f",
   "metadata": {},
   "source": [
    "이 신규 컬럼(column)을 사용하여 일반적으로 검색에 유용하지 않은 \"cc @lewtun\" 또는 \"Thanks!\"와 같은 내용을 포함하는 짧은 댓글을 필터링할 수 있습니다. 필터에 대해 선택할 정확한 숫자는 없지만 약 15단어가 좋은 시작인 것 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4baa140-246b-4389-b7b5-0ee7ef6008c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd619efc18654fc1b06538ba3d12468b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 6686\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15 and x[\"body\"] is not None)\n",
    "comments_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe8b1aa7-1a1e-4a4f-bf25-c15dbca1a20b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acc25bf80f34e429c9ff02943b1db28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"] \n",
    "        + \" \\n \" \n",
    "        + examples[\"body\"] \n",
    "        + \" \\n \" \n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e24a10cd-866a-4214-a0c4-ead69fd9510e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 6686\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22e21f0e-fa94-45ed-a37f-f330d2e94f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': ['https://github.com/huggingface/datasets/issues/7117',\n",
       "  'https://github.com/huggingface/datasets/issues/7116'],\n",
       " 'title': ['Audio dataset load everything in RAM and is very slow',\n",
       "  'datasets cannot handle nested json if features is given.'],\n",
       " 'comments': ['Hi ! I think the issue comes from the fact that you return `row` entirely, and therefore the dataset has to re-encode the audio data in `row`.\\r\\n\\r\\nCan you try this instead ?\\r\\n\\r\\n```python\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    return {\"transcribed\": True}\\r\\n```\\r\\n\\r\\nPS: no need to iter on the dataset to trigger the `map` function on a `Dataset` - `map` runs directly when it\\'s called (contrary to `IterableDataset` taht you can get when streaming, which are lazy)',\n",
       "  'Hi ! `Sequence` has a weird behavior for dictionaries (from tensorflow-datasets), use a regular list instead:\\r\\n\\r\\n```python\\r\\nds = datasets.load_dataset(\\'json\\', data_files=\"./temp.json\", features=datasets.Features({\\r\\n    \\'ref1\\': datasets.Value(\\'string\\'),\\r\\n    \\'ref2\\': datasets.Value(\\'string\\'),\\r\\n    \\'cuts\\': [{\\r\\n        \"cut1\": datasets.Value(\"uint16\"),\\r\\n        \"cut2\": datasets.Value(\"uint16\")\\r\\n    }]\\r\\n}))\\r\\n```'],\n",
       " 'body': ['Hello, I\\'m working with an audio dataset. I want to transcribe the audio that the dataset contain, and for that I use whisper. My issue is that the dataset load everything in the RAM when I map the dataset, obviously, when RAM usage is too high, the program crashes.\\r\\n\\r\\nTo fix this issue, I\\'m using writer_batch_size that I set to 10, but in this case, the mapping of the dataset is extremely slow.\\r\\nTo illustrate this, on 50 examples, with `writer_batch_size` set to 10, it takes 123.24 seconds to process the dataset, but without `writer_batch_size` set to 10, it takes about ten seconds to process the dataset, but then the process remains blocked (I assume that it is writing the dataset and therefore suffers from the same problem as `writer_batch_size`)\\r\\n\\r\\n### Steps to reproduce the bug\\r\\n\\r\\nHug ram usage but fast (but actually slow when saving the dataset):\\r\\n```py\\r\\nfrom datasets import load_dataset\\r\\nimport time\\r\\n\\r\\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\\r\\n\\r\\n\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    row[\"transcribed\"] = True\\r\\n    return row\\r\\n\\r\\n\\r\\ntime1 = time.time()\\r\\nds = ds.map(\\r\\n    transcribe_audio\\r\\n) \\r\\n\\r\\nfor row in ds:\\r\\n    pass  # do nothing, just iterate to trigger the map function\\r\\n\\r\\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\\r\\n```\\r\\n\\r\\nLow ram usage but very very slow:\\r\\n\\r\\n```py\\r\\nfrom datasets import load_dataset\\r\\nimport time\\r\\n\\r\\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\\r\\n\\r\\n\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    row[\"transcribed\"] = True\\r\\n    return row\\r\\n\\r\\n\\r\\ntime1 = time.time()\\r\\nds = ds.map(\\r\\n    transcribe_audio, writer_batch_size=10\\r\\n)  # set low writer_batch_size to avoid memory issues\\r\\n\\r\\nfor row in ds:\\r\\n    pass  # do nothing, just iterate to trigger the map function\\r\\n\\r\\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\\r\\n```\\r\\n\\r\\n### Expected behavior\\r\\n\\r\\nI think the processing should be much faster, on only 50 audio examples, the mapping takes several minutes while nothing is done (just loading the audio).\\r\\n\\r\\n### Environment info\\r\\n\\r\\n- `datasets` version: 2.21.0\\r\\n- Platform: Linux-6.10.5-arch1-1-x86_64-with-glibc2.40\\r\\n- Python version: 3.10.4\\r\\n- `huggingface_hub` version: 0.24.5\\r\\n- PyArrow version: 17.0.0\\r\\n- Pandas version: 1.5.3\\r\\n- `fsspec` version: 2024.6.1\\r\\n\\r\\n# Extra\\r\\n\\r\\nThe dataset has been generated by using audio folder, so I don\\'t think anything specific in my code is causing this problem.\\r\\n```py\\r\\nimport argparse\\r\\n\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nparser = argparse.ArgumentParser()\\r\\nparser.add_argument(\"--folder\", help=\"folder path\", default=\"/media/works/test/\")\\r\\nargs = parser.parse_args()\\r\\n\\r\\ndataset = load_dataset(\"audiofolder\", data_dir=args.folder)\\r\\n\\r\\n# push the dataset to hub\\r\\ndataset.push_to_hub(\"WaveGenAI/audios\")\\r\\n```\\r\\n\\r\\nAlso, it\\'s the combination of `audio = row[\"audio\"]` and `row[\"transcribed\"] = True` which causes problems, `row[\"transcribed\"] = True `alone does nothing and `audio = row[\"audio\"]` alone sometimes causes problems, sometimes not.',\n",
       "  '### Describe the bug\\n\\nI have a json named temp.json.\\r\\n```json\\r\\n{\"ref1\": \"ABC\", \"ref2\": \"DEF\", \"cuts\":[{\"cut1\": 3, \"cut2\": 5}]}\\r\\n```\\r\\nI want to load it.\\r\\n```python\\r\\nds = datasets.load_dataset(\\'json\\', data_files=\"./temp.json\", features=datasets.Features({\\r\\n    \\'ref1\\': datasets.Value(\\'string\\'),\\r\\n    \\'ref2\\': datasets.Value(\\'string\\'),\\r\\n    \\'cuts\\': datasets.Sequence({\\r\\n        \"cut1\": datasets.Value(\"uint16\"),\\r\\n        \"cut2\": datasets.Value(\"uint16\")\\r\\n    })\\r\\n}))\\r\\n```\\r\\nThe above code does not work. However, I can load it without giving features.\\r\\n```python\\r\\nds = datasets.load_dataset(\\'json\\', data_files=\"./temp.json\")\\r\\n```\\r\\nIs it possible to load integers as uint16 to save some memory?\\n\\n### Steps to reproduce the bug\\n\\nAs in the bug description.\\n\\n### Expected behavior\\n\\nThe data are loaded and integers are uint16.\\n\\n### Environment info\\n\\nCopy-and-paste the text below in your GitHub issue.\\r\\n\\r\\n- `datasets` version: 2.21.0\\r\\n- Platform: Linux-5.15.0-118-generic-x86_64-with-glibc2.35\\r\\n- Python version: 3.11.9\\r\\n- `huggingface_hub` version: 0.24.5\\r\\n- PyArrow version: 17.0.0\\r\\n- Pandas version: 2.2.2\\r\\n- `fsspec` version: 2024.5.0'],\n",
       " 'comment_length': [91, 35],\n",
       " 'text': ['Audio dataset load everything in RAM and is very slow \\n Hello, I\\'m working with an audio dataset. I want to transcribe the audio that the dataset contain, and for that I use whisper. My issue is that the dataset load everything in the RAM when I map the dataset, obviously, when RAM usage is too high, the program crashes.\\r\\n\\r\\nTo fix this issue, I\\'m using writer_batch_size that I set to 10, but in this case, the mapping of the dataset is extremely slow.\\r\\nTo illustrate this, on 50 examples, with `writer_batch_size` set to 10, it takes 123.24 seconds to process the dataset, but without `writer_batch_size` set to 10, it takes about ten seconds to process the dataset, but then the process remains blocked (I assume that it is writing the dataset and therefore suffers from the same problem as `writer_batch_size`)\\r\\n\\r\\n### Steps to reproduce the bug\\r\\n\\r\\nHug ram usage but fast (but actually slow when saving the dataset):\\r\\n```py\\r\\nfrom datasets import load_dataset\\r\\nimport time\\r\\n\\r\\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\\r\\n\\r\\n\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    row[\"transcribed\"] = True\\r\\n    return row\\r\\n\\r\\n\\r\\ntime1 = time.time()\\r\\nds = ds.map(\\r\\n    transcribe_audio\\r\\n) \\r\\n\\r\\nfor row in ds:\\r\\n    pass  # do nothing, just iterate to trigger the map function\\r\\n\\r\\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\\r\\n```\\r\\n\\r\\nLow ram usage but very very slow:\\r\\n\\r\\n```py\\r\\nfrom datasets import load_dataset\\r\\nimport time\\r\\n\\r\\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\\r\\n\\r\\n\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    row[\"transcribed\"] = True\\r\\n    return row\\r\\n\\r\\n\\r\\ntime1 = time.time()\\r\\nds = ds.map(\\r\\n    transcribe_audio, writer_batch_size=10\\r\\n)  # set low writer_batch_size to avoid memory issues\\r\\n\\r\\nfor row in ds:\\r\\n    pass  # do nothing, just iterate to trigger the map function\\r\\n\\r\\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\\r\\n```\\r\\n\\r\\n### Expected behavior\\r\\n\\r\\nI think the processing should be much faster, on only 50 audio examples, the mapping takes several minutes while nothing is done (just loading the audio).\\r\\n\\r\\n### Environment info\\r\\n\\r\\n- `datasets` version: 2.21.0\\r\\n- Platform: Linux-6.10.5-arch1-1-x86_64-with-glibc2.40\\r\\n- Python version: 3.10.4\\r\\n- `huggingface_hub` version: 0.24.5\\r\\n- PyArrow version: 17.0.0\\r\\n- Pandas version: 1.5.3\\r\\n- `fsspec` version: 2024.6.1\\r\\n\\r\\n# Extra\\r\\n\\r\\nThe dataset has been generated by using audio folder, so I don\\'t think anything specific in my code is causing this problem.\\r\\n```py\\r\\nimport argparse\\r\\n\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nparser = argparse.ArgumentParser()\\r\\nparser.add_argument(\"--folder\", help=\"folder path\", default=\"/media/works/test/\")\\r\\nargs = parser.parse_args()\\r\\n\\r\\ndataset = load_dataset(\"audiofolder\", data_dir=args.folder)\\r\\n\\r\\n# push the dataset to hub\\r\\ndataset.push_to_hub(\"WaveGenAI/audios\")\\r\\n```\\r\\n\\r\\nAlso, it\\'s the combination of `audio = row[\"audio\"]` and `row[\"transcribed\"] = True` which causes problems, `row[\"transcribed\"] = True `alone does nothing and `audio = row[\"audio\"]` alone sometimes causes problems, sometimes not. \\n Hi ! I think the issue comes from the fact that you return `row` entirely, and therefore the dataset has to re-encode the audio data in `row`.\\r\\n\\r\\nCan you try this instead ?\\r\\n\\r\\n```python\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    return {\"transcribed\": True}\\r\\n```\\r\\n\\r\\nPS: no need to iter on the dataset to trigger the `map` function on a `Dataset` - `map` runs directly when it\\'s called (contrary to `IterableDataset` taht you can get when streaming, which are lazy)',\n",
       "  'datasets cannot handle nested json if features is given. \\n ### Describe the bug\\n\\nI have a json named temp.json.\\r\\n```json\\r\\n{\"ref1\": \"ABC\", \"ref2\": \"DEF\", \"cuts\":[{\"cut1\": 3, \"cut2\": 5}]}\\r\\n```\\r\\nI want to load it.\\r\\n```python\\r\\nds = datasets.load_dataset(\\'json\\', data_files=\"./temp.json\", features=datasets.Features({\\r\\n    \\'ref1\\': datasets.Value(\\'string\\'),\\r\\n    \\'ref2\\': datasets.Value(\\'string\\'),\\r\\n    \\'cuts\\': datasets.Sequence({\\r\\n        \"cut1\": datasets.Value(\"uint16\"),\\r\\n        \"cut2\": datasets.Value(\"uint16\")\\r\\n    })\\r\\n}))\\r\\n```\\r\\nThe above code does not work. However, I can load it without giving features.\\r\\n```python\\r\\nds = datasets.load_dataset(\\'json\\', data_files=\"./temp.json\")\\r\\n```\\r\\nIs it possible to load integers as uint16 to save some memory?\\n\\n### Steps to reproduce the bug\\n\\nAs in the bug description.\\n\\n### Expected behavior\\n\\nThe data are loaded and integers are uint16.\\n\\n### Environment info\\n\\nCopy-and-paste the text below in your GitHub issue.\\r\\n\\r\\n- `datasets` version: 2.21.0\\r\\n- Platform: Linux-5.15.0-118-generic-x86_64-with-glibc2.35\\r\\n- Python version: 3.11.9\\r\\n- `huggingface_hub` version: 0.24.5\\r\\n- PyArrow version: 17.0.0\\r\\n- Pandas version: 2.2.2\\r\\n- `fsspec` version: 2024.5.0 \\n Hi ! `Sequence` has a weird behavior for dictionaries (from tensorflow-datasets), use a regular list instead:\\r\\n\\r\\n```python\\r\\nds = datasets.load_dataset(\\'json\\', data_files=\"./temp.json\", features=datasets.Features({\\r\\n    \\'ref1\\': datasets.Value(\\'string\\'),\\r\\n    \\'ref2\\': datasets.Value(\\'string\\'),\\r\\n    \\'cuts\\': [{\\r\\n        \"cut1\": datasets.Value(\"uint16\"),\\r\\n        \"cut2\": datasets.Value(\"uint16\")\\r\\n    }]\\r\\n}))\\r\\n```']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb051fc-305d-47a3-bf2f-6bc938e660c0",
   "metadata": {},
   "source": [
    "#### 텍스트 임베딩 생성\n",
    "---\n",
    "2장에서 AutoModel 클래스를 사용하여 토큰 임베딩을 얻을 수 있음을 보았습니다. 우리가 해야 할 일은 모델을 로드할 적절한 체크포인트(checkpoint)를 선택하는 것입니다. 다행히 임베딩을 만드는데 특화된 sentence-transformers라는 라이브러리가 있습니다. 라이브러리 문서에 설명된 대로 지금 우리가 만들려고 하는 코드는 경우는 비대칭 의미 검색(asymmetric semantic search) 의 한 예입니다. 이슈 comments과 같이 더 긴 문서에서 답을 찾고자 하는 짧은 쿼리가 있기 때문입니다. 이 문서 내의 모델 개요 테이블(model overview table)에서 multi-qa-mpnet-base-dot-v1 체크포인트가 의미 검색에 가장 좋은 성능을 나타내고 있다고 하므로 우리는 이를 애플리케이션에 사용할 것입니다. 동일한 체크포인트를 사용하여 토크나이저도 로드합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7482b067-de5f-44ad-8794-d007898fa138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentence_transformers in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from sentence_transformers) (4.39.2)\n",
      "Requirement already satisfied: tqdm in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: numpy in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from sentence_transformers) (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from sentence_transformers) (0.20.3)\n",
      "Requirement already satisfied: Pillow in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from sentence_transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: sympy in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a58c3cc-f1d5-4843-ad25-50af5b2c7c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.5472, 0.6330]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "query_embedding = model.encode(\"How big is London\")\n",
    "passage_embedding = model.encode([\n",
    "    \"London has 9,787,426 inhabitants at the 2011 census\",\n",
    "    \"London is known for its finacial district\",\n",
    "])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fce4e087-097f-4cb8-8b10-efb997c24b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4659, 0.6142, 0.2697]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"multi-qa-mpnet-base-cos-v1\")\n",
    "\n",
    "query_embedding = model.encode(\"How big is London\")\n",
    "passage_embeddings = model.encode([\n",
    "    \"London is known for its finacial district\",\n",
    "    \"London has 9,787,426 inhabitants at the 2011 census\",\n",
    "    \"The United Kingdom is the fourth largest exporter of goods in the world\",\n",
    "])\n",
    "\n",
    "similarity = model.similarity(query_embedding, passage_embeddings)\n",
    "print(similarity)\n",
    "# => tensor([[0.4659, 0.6142, 0.2697]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92717d9e-afa2-41df-a382-48d19457d28a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0a6bf76-59c7-4576-915e-f16dddac4dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ced7721a-1cd2-4a71-a376-bf3095401b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Audio dataset load everything in RAM and is very slow \\n Hello, I\\'m working with an audio dataset. I want to transcribe the audio that the dataset contain, and for that I use whisper. My issue is that the dataset load everything in the RAM when I map the dataset, obviously, when RAM usage is too high, the program crashes.\\r\\n\\r\\nTo fix this issue, I\\'m using writer_batch_size that I set to 10, but in this case, the mapping of the dataset is extremely slow.\\r\\nTo illustrate this, on 50 examples, with `writer_batch_size` set to 10, it takes 123.24 seconds to process the dataset, but without `writer_batch_size` set to 10, it takes about ten seconds to process the dataset, but then the process remains blocked (I assume that it is writing the dataset and therefore suffers from the same problem as `writer_batch_size`)\\r\\n\\r\\n### Steps to reproduce the bug\\r\\n\\r\\nHug ram usage but fast (but actually slow when saving the dataset):\\r\\n```py\\r\\nfrom datasets import load_dataset\\r\\nimport time\\r\\n\\r\\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\\r\\n\\r\\n\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    row[\"transcribed\"] = True\\r\\n    return row\\r\\n\\r\\n\\r\\ntime1 = time.time()\\r\\nds = ds.map(\\r\\n    transcribe_audio\\r\\n) \\r\\n\\r\\nfor row in ds:\\r\\n    pass  # do nothing, just iterate to trigger the map function\\r\\n\\r\\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\\r\\n```\\r\\n\\r\\nLow ram usage but very very slow:\\r\\n\\r\\n```py\\r\\nfrom datasets import load_dataset\\r\\nimport time\\r\\n\\r\\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\\r\\n\\r\\n\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    row[\"transcribed\"] = True\\r\\n    return row\\r\\n\\r\\n\\r\\ntime1 = time.time()\\r\\nds = ds.map(\\r\\n    transcribe_audio, writer_batch_size=10\\r\\n)  # set low writer_batch_size to avoid memory issues\\r\\n\\r\\nfor row in ds:\\r\\n    pass  # do nothing, just iterate to trigger the map function\\r\\n\\r\\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\\r\\n```\\r\\n\\r\\n### Expected behavior\\r\\n\\r\\nI think the processing should be much faster, on only 50 audio examples, the mapping takes several minutes while nothing is done (just loading the audio).\\r\\n\\r\\n### Environment info\\r\\n\\r\\n- `datasets` version: 2.21.0\\r\\n- Platform: Linux-6.10.5-arch1-1-x86_64-with-glibc2.40\\r\\n- Python version: 3.10.4\\r\\n- `huggingface_hub` version: 0.24.5\\r\\n- PyArrow version: 17.0.0\\r\\n- Pandas version: 1.5.3\\r\\n- `fsspec` version: 2024.6.1\\r\\n\\r\\n# Extra\\r\\n\\r\\nThe dataset has been generated by using audio folder, so I don\\'t think anything specific in my code is causing this problem.\\r\\n```py\\r\\nimport argparse\\r\\n\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nparser = argparse.ArgumentParser()\\r\\nparser.add_argument(\"--folder\", help=\"folder path\", default=\"/media/works/test/\")\\r\\nargs = parser.parse_args()\\r\\n\\r\\ndataset = load_dataset(\"audiofolder\", data_dir=args.folder)\\r\\n\\r\\n# push the dataset to hub\\r\\ndataset.push_to_hub(\"WaveGenAI/audios\")\\r\\n```\\r\\n\\r\\nAlso, it\\'s the combination of `audio = row[\"audio\"]` and `row[\"transcribed\"] = True` which causes problems, `row[\"transcribed\"] = True `alone does nothing and `audio = row[\"audio\"]` alone sometimes causes problems, sometimes not. \\n Hi ! I think the issue comes from the fact that you return `row` entirely, and therefore the dataset has to re-encode the audio data in `row`.\\r\\n\\r\\nCan you try this instead ?\\r\\n\\r\\n```python\\r\\n# map the dataset\\r\\ndef transcribe_audio(row):\\r\\n    audio = row[\"audio\"]  # get the audio but do nothing with it\\r\\n    return {\"transcribed\": True}\\r\\n```\\r\\n\\r\\nPS: no need to iter on the dataset to trigger the `map` function on a `Dataset` - `map` runs directly when it\\'s called (contrary to `IterableDataset` taht you can get when streaming, which are lazy)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = comments_dataset[\"text\"][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77570284-a262-482d-874e-cf6c67022e80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetTokenizerFast(name_or_path='sentence-transformers/multi-qa-mpnet-base-dot-v1', vocab_size=30527, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t104: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30526: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23493190-ce41-4547-b9a3-1af811ed16d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62275966-4173-42f0-96d0-9da2e8042977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'data', '##set', '[unused131]', '</s>']\n",
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  5750,  2955, 13466,  7174,  2677,  2003,  8227,  2002,  2007,\n",
       "          2204,  4034,  7596,  1014,  1049,  1009,  1053,  2555,  2011,  2023,\n",
       "          5750,  2955, 13466,  1016,  1049,  2219,  2004,  9103, 26779, 20759,\n",
       "          2000,  5750,  2012,  2000,  2955, 13466,  5387,  1014,  2002,  2009,\n",
       "          2012,  1049,  2228,  7208,  1016,  2030,  3281,  2007,  2012,  2000,\n",
       "          2955, 13466,  7174,  2677,  2003,  2000,  8227,  2047,  1049,  4953,\n",
       "          2000,  2955, 13466,  1014,  5529,  1014,  2047,  8227,  8196,  2007,\n",
       "          2209,  2156,  1014,  2000,  2569, 19123,  1016,  2004,  8085,  2027,\n",
       "          3281,  1014,  1049,  1009,  1053,  2482,  3217,  1039, 14112,  1039,\n",
       "          2950,  2012,  1049,  2279,  2004,  2188,  1014,  2025,  2003,  2027,\n",
       "          2557,  1014,  2000, 12379,  2001,  2000,  2955, 13466,  2007,  5190,\n",
       "          4034,  1016,  2004, 19145,  2027,  1014,  2010,  2757,  4977,  1014,\n",
       "          2011,  1040,  3217,  1039, 14112,  1039,  2950,  1040,  2279,  2004,\n",
       "          2188,  1014,  2013,  3142, 13142,  1016,  2488,  3827,  2004,  2836,\n",
       "          2000,  2955, 13466,  1014,  2025,  2306,  1040,  3217,  1039, 14112,\n",
       "          1039,  2950,  1040,  2279,  2004,  2188,  1014,  2013,  3142,  2059,\n",
       "          2706,  3827,  2004,  2836,  2000,  2955, 13466,  1014,  2025,  2063,\n",
       "          2000,  2836,  3468,  8538,  1010,  1049,  7872,  2012,  2013,  2007,\n",
       "          3019,  2000,  2955, 13466,  2002,  3572, 17571,  2017,  2000,  2172,\n",
       "          3295,  2008,  1040,  3217,  1039, 14112,  1039,  2950,  1040,  1011,\n",
       "          1005,  1005,  1005,  4088,  2004, 21380,  2000, 11833,  8553,  8227,\n",
       "          8196,  2025,  3439,  1010,  2025,  2945,  4034,  2047,  7498,  2000,\n",
       "          2955, 13466,  1011,  1028,  1040,  1040,  1040,  1056,  2104,  2017,\n",
       "          2955, 13466,  2019, 12328,  7174,  1039,  2955, 13466, 12328,  2055,\n",
       "         16237,  1031,  7174,  1039,  2955, 13466,  1010,  1004,  4404,  6918,\n",
       "          4890,  1017,  5750,  2019,  2479,  1004,  1014,  3979,  1031,  1004,\n",
       "          3349,  1035,  1028,  2757,  1037,  1004,  1011,  1005,  4953,  2000,\n",
       "          2955, 13466, 13370,  9103, 26779, 20759,  1039,  5750,  1010,  5220,\n",
       "          1011,  1028,  5750,  1031,  5220,  1035,  1004,  5750,  1004,  1037,\n",
       "          1005,  2135,  2000,  5750,  2025,  2083,  2502,  2011,  2013,  5220,\n",
       "          1035,  1004, 26227,  1004,  1037,  1031,  2999,  2713,  5220,  2055,\n",
       "          2491,  1031,  2055,  1016,  2055,  1010,  1011, 16237,  1031, 16237,\n",
       "          1016,  4953,  1010,  9103, 26779, 20759,  1039,  5750,  1011,  2009,\n",
       "          5220,  2003, 16237,  1028,  3417,  1005,  2083,  2502,  1014,  2078,\n",
       "          2013, 22143,  2004,  9499,  2000,  4953,  3857,  6144,  1010,  1046,\n",
       "          1004,  2055,  2583,  1028,  1067,  2055,  1016,  2055,  1010,  1011,\n",
       "          1015,  2055,  2491,  1028,  1016,  1020,  2550,  1069,  3827,  1004,\n",
       "          1011,  1040,  1040,  1040,  2663,  8227,  8196,  2025,  2204,  2204,\n",
       "          4034,  1028,  1040,  1040,  1040,  1056,  2104,  2017,  2955, 13466,\n",
       "          2019, 12328,  7174,  1039,  2955, 13466, 12328,  2055, 16237,  1031,\n",
       "          7174,  1039,  2955, 13466,  1010,  1004,  4404,  6918,  4890,  1017,\n",
       "          5750,  2019,  2479,  1004,  1014,  3979,  1031,  1004,  3349,  1035,\n",
       "          1028,  2757,  1037,  1004,  1011,  1005,  4953,  2000,  2955, 13466,\n",
       "         13370,  9103, 26779, 20759,  1039,  5750,  1010,  5220,  1011,  1028,\n",
       "          5750,  1031,  5220,  1035,  1004,  5750,  1004,  1037,  1005,  2135,\n",
       "          2000,  5750,  2025,  2083,  2502,  2011,  2013,  5220,  1035,  1004,\n",
       "         26227,  1004,  1037,  1031,  2999,  2713,  5220,  2055,  2491,  1031,\n",
       "          2055,  1016,  2055,  1010,  1011, 16237,  1031, 16237,  1016,  4953,\n",
       "          1010,  9103, 26779, 20759,  1039,  5750,  1014,  3217,  1039, 14112,\n",
       "          1039,  2950,  1031,  2188,  1011,  1005,  2279,  2663,  3217,  1039,\n",
       "         14112,  1039,  2950,  2004,  4472,  3642,  3318,  2009,  5220,  2003,\n",
       "         16237,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "print(tokenizer.convert_ids_to_tokens([0,  2955, 13466, 140, 2]))\n",
    "print(encoded_input[\"input_ids\"].shape)\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bea5c791-80f4-4686-b2e7-2a54608e30a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1af3505-7719-41ce-82d4-7012aa4d8913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a6fe5ae-2657-4597-b919-2e05c8858746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4712,  0.0537, -0.0201,  ..., -0.2617,  0.0721, -0.4141],\n",
       "         [-0.2391,  0.3009, -0.0784,  ..., -0.0998,  0.1447, -0.2866],\n",
       "         [-0.3046,  0.6578, -0.2158,  ..., -0.2800,  0.0501, -0.3406],\n",
       "         ...,\n",
       "         [-0.0807, -1.0307, -0.1704,  ..., -0.0302,  0.2726, -0.2690],\n",
       "         [-0.4099, -0.4894, -0.1271,  ...,  0.2389,  0.1461, -0.3746],\n",
       "         [-0.4886,  0.0078, -0.0563,  ..., -0.2398,  0.0625, -0.4470]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "633b1238-61c0-4c08-a58f-31c70b413032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7125e-01,  5.3745e-02, -2.0133e-02,  3.1866e-01,  2.0124e-01,\n",
       "          2.1911e-03,  1.4834e-01,  2.3799e-01,  4.5511e-02,  2.7738e-01,\n",
       "          2.2210e-02,  8.1672e-02, -3.1484e-01, -1.5764e-01, -4.1450e-02,\n",
       "          1.5755e-01,  1.7000e-01,  8.8720e-02, -5.0003e-02, -6.3025e-02,\n",
       "         -2.9285e-01, -1.9287e-01, -1.3279e-01, -1.2677e-01, -1.1428e-01,\n",
       "         -2.0726e-01,  3.8567e-01,  2.1749e-02, -2.7896e-01, -4.7208e-02,\n",
       "          1.0354e-01,  2.6212e-01, -5.1871e-02,  6.1097e-01, -1.2220e-04,\n",
       "          1.0630e-01,  3.1088e-01, -7.4219e-03, -5.6068e-02,  2.6801e-01,\n",
       "         -4.0436e-01,  5.8559e-02, -1.8065e-01, -6.4449e-02,  2.1116e-01,\n",
       "          1.8019e-01,  2.9448e-02, -5.3549e-01,  2.8248e-01,  1.3918e-01,\n",
       "          7.5814e-02,  3.7746e-02, -1.9992e-01,  2.1791e-01,  6.5224e-01,\n",
       "         -7.6405e-02, -1.1174e-01,  6.2100e-02,  4.5188e-01, -1.1831e-01,\n",
       "         -1.7057e-02,  3.5958e-01, -4.2470e-01,  7.6583e-02,  2.3431e-01,\n",
       "         -1.4895e-01,  2.0228e-01, -3.3888e-01,  2.0632e-01,  3.0901e-01,\n",
       "          2.8596e-01, -3.5070e-01, -6.9088e-02, -1.9419e-01, -2.3491e-01,\n",
       "         -3.6479e-01, -5.5043e-02,  4.2439e-02, -4.9756e-01, -1.1071e-01,\n",
       "         -4.7674e-01,  2.6037e-03,  2.3470e-01,  5.3303e-02,  2.4193e-01,\n",
       "          3.5505e-02, -1.1440e-02,  8.7356e-02,  3.6023e-01, -2.2578e-01,\n",
       "          2.5395e-01, -2.4775e-01, -1.5221e-01,  2.8737e-01, -5.7578e-01,\n",
       "         -1.6435e-02,  6.9540e-02, -2.5614e-01,  2.3805e-01, -3.6372e-01,\n",
       "          4.8600e-02,  2.7373e-01,  1.7107e-01,  9.6492e-02,  1.9205e-01,\n",
       "          1.0501e-01, -6.0807e-03,  3.7493e-02,  3.3559e-01, -2.7535e-01,\n",
       "          1.9434e-01,  4.1707e-02, -2.1758e-01, -1.6974e-01,  4.8475e-01,\n",
       "         -9.1656e-02, -2.9595e-02, -1.0681e-01, -2.8457e-01, -1.3275e-01,\n",
       "         -1.1797e-01, -6.6613e-02,  9.1387e-02,  4.6203e-01,  2.1592e-01,\n",
       "         -1.4969e-02,  5.5363e-02,  9.4509e-02, -2.1889e-01,  2.5967e-01,\n",
       "         -9.0473e-02, -2.4717e-01, -1.1929e-01,  3.1104e-01,  6.7516e-02,\n",
       "          2.6922e-01,  1.0539e-01,  1.6337e-03, -8.4337e-03,  7.2924e-02,\n",
       "          1.7100e-01, -2.7638e-01, -5.1694e-02,  3.0277e-01, -1.5411e-01,\n",
       "          3.4355e-01, -7.8405e-02, -4.7578e-02, -1.6613e-01,  3.7407e-01,\n",
       "         -2.0528e-01, -2.1562e-01,  3.6560e-01,  4.9105e-02,  7.8653e-02,\n",
       "          1.4068e-01, -6.7309e-01,  2.0621e-01,  4.2807e-01, -1.2327e-01,\n",
       "         -7.1625e-02, -3.7495e-01, -4.9033e-01, -6.2114e-02,  4.9529e-01,\n",
       "          3.7505e-01, -3.6776e-01, -1.2665e-01,  7.9523e-03, -2.1384e-01,\n",
       "          3.6856e-01,  5.8959e-01, -2.7436e-02,  1.7763e-01,  2.4324e-02,\n",
       "          2.2122e-02,  3.2956e-01, -3.8662e-01, -6.4860e-01,  2.3673e-01,\n",
       "         -2.2446e-01,  9.9884e-02,  2.6188e-01,  3.7190e-01,  2.7165e-01,\n",
       "         -5.3377e-02,  4.9849e-01,  2.7720e-01, -3.5486e-02,  3.4923e-01,\n",
       "         -4.7067e-02, -1.3923e-01,  8.6938e-02,  1.6257e-01, -2.0673e-01,\n",
       "         -4.4565e-02, -7.2563e-02,  1.7928e-01,  3.7897e-01, -1.6183e-01,\n",
       "          1.3669e-01,  1.5615e-01,  1.4333e-01, -1.9723e-01,  5.1585e-02,\n",
       "         -1.4472e-01,  7.0170e-02,  1.1067e-01,  1.9607e-02,  5.3205e-02,\n",
       "         -2.4772e-02, -1.6011e-01,  1.2614e-01,  1.0795e-01, -2.4820e-01,\n",
       "          1.6749e-01, -5.6137e-02, -9.9806e-02,  3.1713e-02,  2.9272e-02,\n",
       "          1.8718e-01, -8.3459e-02, -1.9061e-01, -1.0516e-01, -3.1951e-01,\n",
       "         -3.0310e-01,  2.1098e-01, -2.1173e-01,  1.7480e-01,  1.7275e-01,\n",
       "          3.9162e-02,  1.0949e-01, -4.7422e-02,  4.1394e-01,  8.4863e-02,\n",
       "          2.1406e-01, -5.4041e-01,  2.8089e-01,  9.4676e-02, -5.0146e-02,\n",
       "          3.8917e-01, -1.8895e-02,  2.0148e-01, -3.8517e-01, -4.4936e-02,\n",
       "         -2.8032e-02,  3.2817e-01,  5.7144e-01,  1.2439e-01, -2.9961e-01,\n",
       "         -1.2272e-01,  1.4901e-01, -2.5412e-02,  1.0136e-01,  9.0361e-02,\n",
       "         -2.3959e-01,  2.4783e-01,  1.1037e-01,  2.5395e-01, -2.1224e-01,\n",
       "          3.5117e-01, -2.5251e-01,  2.0413e-01,  1.2296e-01, -5.3738e-01,\n",
       "         -2.1717e-02,  8.4674e-02,  2.6102e-01,  2.2034e-01,  2.1595e-01,\n",
       "          3.7098e-02, -1.7899e-01,  3.3774e-01, -1.5740e-01,  1.7530e-01,\n",
       "          1.4934e-01,  2.2013e-01, -8.0428e-02, -1.2517e-03, -2.4463e-01,\n",
       "         -3.9350e-01,  1.9069e-01, -5.2754e-03,  1.9518e-01, -1.2699e-01,\n",
       "         -1.6751e-01, -8.0347e-02, -8.7190e-02, -1.5408e-01,  2.3782e-01,\n",
       "         -3.4249e-03, -1.0097e-02, -7.6997e-02,  1.1048e-01,  5.7420e-02,\n",
       "          4.1497e-02,  4.1630e-02, -1.0526e-01,  1.5302e-01, -3.1119e-01,\n",
       "          7.5557e-02, -1.4608e-01, -3.2550e-02, -4.1809e-03,  2.4077e-01,\n",
       "          3.2305e-02,  2.6354e-01,  1.1436e-01, -2.5381e-01,  1.9382e-01,\n",
       "          1.1396e-01, -7.1110e-02, -1.0397e-01,  1.8273e-01, -2.7161e-01,\n",
       "          2.8467e-01,  1.3193e-01, -2.5992e-01,  1.0949e-01,  6.1606e-02,\n",
       "         -2.1990e-01,  2.2387e-01, -5.6863e-02, -2.2809e-01,  9.6002e-02,\n",
       "         -1.6663e-01, -2.8985e-01, -3.6794e-01,  2.7809e-01, -1.6257e-01,\n",
       "          6.7454e-02,  2.0391e-01,  1.8013e-01,  2.2548e-02, -1.5938e-01,\n",
       "          1.7656e-01,  6.5657e-02, -2.4716e-01,  2.6909e-01,  1.4470e-01,\n",
       "         -3.6071e-02, -2.4012e-01,  2.2198e-02, -1.2630e-01,  4.4410e-01,\n",
       "         -4.9293e-01,  5.0854e-01, -5.2491e-01, -2.4036e-01,  3.4890e-01,\n",
       "          1.3327e-01,  2.9251e-01,  1.9304e-01, -2.0697e-02,  5.5929e-02,\n",
       "         -3.9422e-01, -1.9764e-01, -1.8927e-01,  8.8050e-02,  1.2767e-01,\n",
       "          8.5155e-01, -8.7748e-02,  7.7715e-01,  2.7131e-01,  5.8415e-02,\n",
       "         -2.8503e-02,  3.2889e-02,  3.5101e-02, -9.3338e-02, -1.3971e-01,\n",
       "          4.1149e-01, -1.5571e-02, -1.5333e-01,  3.6898e-05,  1.5652e-01,\n",
       "         -3.2329e-01,  1.2421e-01, -2.4437e-01,  9.1550e-02, -2.6748e-01,\n",
       "          2.4957e-01, -2.2484e-01,  1.6599e-01,  1.2259e-02,  1.6501e-01,\n",
       "         -1.0646e-01, -3.7097e-01, -6.5049e-02, -2.1904e-01,  1.6344e-01,\n",
       "         -5.6956e-02, -6.8571e-01,  8.3444e-02, -8.3597e-01,  1.5267e-01,\n",
       "         -6.4180e-02,  5.4888e-01, -1.0771e-01,  1.9871e-03,  7.3701e-02,\n",
       "          1.0818e-01,  5.7706e-01, -7.4306e-02, -3.1676e-03,  2.9517e-02,\n",
       "          1.1284e-01, -3.5956e-01,  1.2185e-01, -2.1163e-01,  2.5521e-01,\n",
       "         -3.3006e-02,  3.7162e-01, -3.0075e-02,  3.6004e-02, -6.5683e-03,\n",
       "          2.7823e-01, -2.6222e-01, -1.6076e-01, -1.9325e-01, -2.9203e-01,\n",
       "         -1.6296e-01,  7.7904e-02, -1.1179e-01,  1.8177e-01,  1.1876e-01,\n",
       "         -2.8045e-01,  3.5476e-01,  1.4673e-01,  5.2268e-01,  1.3025e-01,\n",
       "          1.6679e-01,  2.2171e-01,  3.7666e-01,  4.9809e-02, -1.5706e-01,\n",
       "          4.3143e-01,  1.7340e-01,  4.0768e-02,  1.1655e-01, -6.1619e-02,\n",
       "         -3.5147e-02,  5.5126e-01,  2.2332e-01, -8.4149e-02,  2.7869e-01,\n",
       "          3.0250e-01,  2.0193e-01, -6.1409e-01, -9.9228e-02,  5.8577e-01,\n",
       "          7.6265e-03, -3.3295e-01, -6.2435e-01,  5.5195e-01,  2.5711e-01,\n",
       "         -1.6890e-01,  2.6472e-01, -3.7481e-01, -2.6661e-01,  5.4415e-01,\n",
       "          1.3305e-01,  8.2500e-01, -4.8597e-01,  2.1926e-01, -1.7018e-01,\n",
       "          7.9852e-02, -1.5232e-01, -4.5737e-01,  1.0968e-01,  1.9173e-01,\n",
       "         -6.4289e-02,  1.8918e-01, -1.6102e-01, -1.1737e-01,  1.2766e-01,\n",
       "         -1.1120e-01,  1.8101e-01,  9.8430e-03, -2.6283e-01, -1.7189e-01,\n",
       "         -3.4207e-02, -1.3986e-01, -4.1788e-01,  2.8021e-01,  7.7069e-02,\n",
       "          2.4429e-01, -2.2581e-01,  7.0261e-02,  9.9584e-02,  2.5352e-02,\n",
       "         -1.7933e-01, -3.4752e-01, -7.2853e-02, -5.3387e-01, -1.4367e-01,\n",
       "         -5.0142e-01, -1.7184e-01,  2.8118e-01,  1.4642e-01, -2.7567e-01,\n",
       "          4.5690e-01, -4.0892e-02,  2.1362e-01,  2.9841e-01,  4.2050e-01,\n",
       "          2.3890e-01, -1.1801e-02, -1.0136e-01,  1.4890e-01, -1.3833e-01,\n",
       "          1.3259e-01, -5.8557e-02, -1.5084e-01, -3.0393e-01, -7.3163e-03,\n",
       "          4.6396e-01,  1.4175e-02, -2.9680e-01,  1.3135e-01, -7.6738e-02,\n",
       "          2.6018e-02,  6.7399e-02,  1.6846e-01, -1.4152e-01,  4.9123e-01,\n",
       "         -4.1215e-01, -2.5111e-01,  5.1458e-02,  3.5660e-01,  3.3099e-01,\n",
       "          6.7715e-02,  7.2834e-01,  3.9860e-02, -1.0083e-01, -1.6754e-01,\n",
       "          2.0600e-01,  3.4883e-01,  7.5312e-02,  2.1061e-02,  8.1217e-02,\n",
       "         -1.6498e-01, -2.1647e-01, -4.5280e-02, -1.9409e-01, -3.7046e-01,\n",
       "         -2.5952e-01, -2.1672e-01, -3.4051e-01,  2.2740e-01,  1.5851e-01,\n",
       "          2.9561e-01, -3.2800e-01, -2.1888e-01, -1.9966e-01,  3.4185e-01,\n",
       "         -1.9813e-01, -1.6120e-01, -1.2779e-01,  2.7771e-01,  6.1073e-02,\n",
       "          3.2092e-01,  2.4546e-01, -1.0282e-01, -5.2513e-03,  1.0031e-01,\n",
       "         -1.5301e-01, -1.1873e-01,  1.0153e-01,  2.1052e-01, -5.6258e-02,\n",
       "         -2.4153e-01,  1.1819e-01, -4.3847e-01, -1.0593e-01, -3.6621e-01,\n",
       "          3.6605e-01,  2.1673e-01, -4.6953e-02, -2.5349e-01,  2.5090e-01,\n",
       "          1.2044e-03,  6.7046e-02,  1.4791e-01, -2.7676e-01, -6.7846e-02,\n",
       "         -7.4163e-02,  9.8607e-02,  3.5053e-01, -1.7143e-01,  1.3043e-02,\n",
       "          5.2093e-02,  3.0081e-01, -1.1159e-01,  2.7307e-01, -2.1339e-01,\n",
       "         -1.4179e-01,  2.8475e-01,  2.4622e-01,  3.1150e-01, -1.7268e-01,\n",
       "          6.0872e-02,  3.5120e-02,  1.3933e-01,  3.7518e-02, -2.3873e-01,\n",
       "          6.2738e-03, -2.9551e-01,  1.3252e-01,  4.1578e-02, -2.7099e-01,\n",
       "          4.0278e-02, -1.3102e-01, -9.5914e-02,  8.2674e-02, -3.3402e-01,\n",
       "         -3.3218e-03,  2.2498e-01,  2.2506e-01, -1.2558e-01,  1.6399e-01,\n",
       "          3.8268e-02,  2.6617e-01,  6.4851e-01,  1.9233e-01,  4.2872e-01,\n",
       "          9.1137e-02,  9.1491e-02, -3.1285e-01, -2.7651e-01, -1.4907e-01,\n",
       "          4.8299e-01, -2.6838e-01,  3.2199e-01,  9.7210e-02,  3.0571e-01,\n",
       "         -6.1221e-02,  5.3663e-02,  4.1101e-02, -1.4678e-01, -1.3250e-01,\n",
       "         -2.6907e-01,  6.5699e-02, -6.9711e-03, -1.0763e-01,  5.9203e-02,\n",
       "         -1.2054e-01, -9.5452e-02,  1.0708e-01, -4.7506e-02, -4.1057e-02,\n",
       "          1.1470e-01, -2.4370e-01,  1.4631e-01, -1.5572e-02, -3.6551e-01,\n",
       "          1.5980e-01, -6.7155e-02,  1.0561e-01,  7.8203e-02,  6.2150e-01,\n",
       "          3.4862e-01,  4.1833e-01, -6.3155e-02,  4.4053e-01,  2.0476e-01,\n",
       "         -1.9690e-01, -2.3455e-01,  3.6743e-01,  1.1417e-01,  3.9426e-01,\n",
       "          7.9983e-03,  3.4405e-02, -1.2047e-01, -3.0026e-01,  7.3469e-02,\n",
       "         -4.2489e-03, -7.9745e-02,  1.2789e-01, -2.4482e-01, -3.8424e-01,\n",
       "         -1.8521e-01, -9.8852e-02, -2.8018e-01,  3.2920e-01,  6.5422e-01,\n",
       "         -3.7987e-01,  9.3540e-02, -2.0215e-01,  1.6526e-02, -1.3378e-01,\n",
       "          4.4886e-01,  1.2113e-01,  2.2945e-01, -3.3962e-01, -9.2680e-02,\n",
       "         -5.2427e-01, -2.1561e-01, -6.0094e-01,  1.7970e-01, -8.5399e-02,\n",
       "          1.5981e-01,  3.9864e-01,  1.4060e-01, -8.1468e-02, -2.8041e-01,\n",
       "         -5.6978e-01,  5.5119e-02, -1.3473e-02, -5.1881e-02, -1.5276e-01,\n",
       "          8.6012e-02,  6.2859e-02, -6.8819e-01,  4.3336e-01, -5.3594e-02,\n",
       "         -1.0283e-01, -2.0706e-01,  2.3134e-01,  6.8060e-02,  6.1545e-02,\n",
       "          4.5581e-01,  1.4023e-01,  3.2843e-01, -1.8525e-01, -4.3606e-02,\n",
       "         -6.5935e-02,  2.1570e-02, -2.3144e-01, -4.3881e-01,  5.9417e-02,\n",
       "          2.9455e-01, -2.0112e-01, -1.2688e-01, -1.2173e-01,  1.6256e-01,\n",
       "         -2.0755e-01, -2.4842e-01, -5.5378e-02, -1.0143e-01, -3.3988e-02,\n",
       "          1.6633e-01,  9.2777e-02, -5.2642e-02, -1.3235e-02, -8.0082e-02,\n",
       "         -2.0553e-01, -2.0415e-01,  2.3565e-01, -2.9005e-01, -1.3614e-01,\n",
       "         -7.9058e-02,  2.0760e-01, -2.3837e-01,  1.4499e-01, -5.1283e-01,\n",
       "          2.3429e-01,  2.5097e-01, -5.5225e-02, -2.8579e-01,  1.5092e-01,\n",
       "         -4.0048e-02, -1.2192e-01, -3.4930e-02,  3.5064e-01, -1.4039e-01,\n",
       "         -2.6166e-01,  7.2085e-02, -4.1411e-01]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.last_hidden_state[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "558283cb-3b79-4cf2-84c1-763c1bb12dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "cls_pool= model_output.last_hidden_state[:, 0]\n",
    "print(cls_pool.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd839792-ef32-4248-b016-6b5b5c548a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7640bf9b-6622-46c2-9b2c-3617c349a5a2",
   "metadata": {},
   "source": [
    "다음으로, 문서들을 토큰화하고, GPU에 텐서(tensors)를 배치하고, 이를 모델에 공급하고, 마지막으로 출력에 CLS 풀링을 적용하는 핼퍼 함수를 만듭니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96c31bfb-a7df-405f-8552-a4fbce2e4502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0930b82-962c-433c-a30a-b60246e761f5",
   "metadata": {},
   "source": [
    "말뭉치의 첫 번째 텍스트 항목을 입력하고 출력 모양을 검사하여 함수 작동을 테스트할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b8a0fd6-0bb1-4234-96af-fea38a8898e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac93b106-609a-4e64-836f-ec820a30ecc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b7276-2667-4812-89d6-31586a41a92c",
   "metadata": {},
   "source": [
    "말뭉치의 첫 번째 항목을 768차원 벡터로 변환했습니다! Dataset.map()을 사용하여 말뭉치의 각 행에 get_embeddings() 함수를 적용할 수 있으므로 다음과 같이 새 임베딩 열을 생성해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f2298dc-609d-45cb-9e63-e0fb51e80051",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832a6e74dc6d4ee3a9ccc4fd4bb78f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c279742c-9d9a-4fa7-bde3-92992cb3e1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 6686\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58009e90-ee83-4b60-a188-d1a1b5b67e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(embeddings_dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7614dbd4-ed93-4112-bafd-15d6d11968da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings_dataset[\"embeddings\"][0]))\n",
    "#print(embeddings_dataset[\"embeddings\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe8dc1-891c-46f0-b7f5-26f1f0418ea9",
   "metadata": {},
   "source": [
    "임베딩을 NumPy 배열로 변환했음에 주목하세요. 그 이유는 FAISS로 데이터셋을 인덱싱하려고 할 때 🤗Datasets에 이 형식이 필요하기 때문입니다. 이는 다음에서 설명하겠습니다.\n",
    "\n",
    "#### 효율적인 시맨틱 검색을 위한 FAISS 사용\n",
    "---\n",
    "이제 임베딩 데이터셋이 있으므로 이를 검색할 방법이 필요합니다. 이를 위해 FAISS 인덱스 라고 하는, 🤗Datasets 내에서의 특별한 자료 구조를 사용합니다. FAISS(Facebook AI Similarity Search)는 임베딩 벡터를 빠르게 검색하고 클러스터링하는 효율적인 알고리즘을 제공하는 라이브러리입니다.\n",
    "\n",
    "FAISS의 기본 아이디어는 입력 임베딩과 유사한 임베딩을 찾을 수 있는 인덱스(index) 라는 특수 데이터 구조를 만드는 것입니다. 🤗Datasets에서 FAISS 인덱스를 만드는 것은 간단합니다. Dataset.add_faiss_index() 함수를 사용하고 인덱스할 데이터셋의 열을 지정합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b1c21f7-7edb-422b-a1e7-aa25083284fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 6686\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b73a24a-1fe4-4f28-8a20-3158c4e154f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52ee7afc-5014-4534-9f39-27f47100aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af2c3fe2-ae76-42c9-8375-7dbd2c232ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7b2d36ee964b1e9949255f49c90a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 6686\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf3345c9-f7f6-44d5-84ca-c7328e1c2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install elasticsearch==7.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1274c658-fe92-40dd-9c6b-895b2d049136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#embeddings_dataset.add_elasticsearch_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f5e63-f88a-4df7-86bd-7d889da4406d",
   "metadata": {},
   "source": [
    "이제 Dataset.get_nearest_examples() 함수로 근접 이웃 검색(nearest neighbor lookup)을 수행하여 이 인덱스에 대한 쿼리를 수행할 수 있습니다. 먼저 다음과 같이 질문을 삽입하여 이를 테스트해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "798c63a2-e1ab-484a-aff1-db62d3971a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c216ae-12f8-441f-939f-d89a02c99ffc",
   "metadata": {},
   "source": [
    "문서와 마찬가지로 이제 해당 쿼리를 나타내는 768차원 벡터를 만들었습니다. 이 벡터를 전체 코퍼스와 비교하여 가장 유사한 임베딩을 찾을 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0000d5ea-08ee-4cca-a93b-237edc103c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\"embeddings\", question_embedding, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8413fd5f-d741-497d-9705-9a361e2ea43a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([22.406656, 22.894001, 24.148985, 24.55554 , 25.505016],\n",
      "      dtype=float32)\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(scores)\n",
    "print(type(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a56960f6-a684-4c40-a7a5-03fcedde63eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "#samples_df[\"scores\"] = scores\n",
    "#samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef82fe20-d9ac-4c4f-8eac-135a9cfa9ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>here is my way to load a dataset offline, but ...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>47</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4902576208114624, 0.22889626026153564, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>&gt; here is my way to load a dataset offline, bu...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>76</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4992600083351135, 0.2269977331161499, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>I opened a PR that allows to reload modules th...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>179</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4716480076313019, 0.29022741317749023, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>The local dataset builders (csv, text , json a...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>38</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4490852653980255, 0.20950627326965332, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>Requiring online connection is a deal breaker ...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>57</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4731806516647339, 0.24578382074832916, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                       title  \\\n",
       "0  Discussion using datasets in offline mode   \n",
       "1  Discussion using datasets in offline mode   \n",
       "2  Discussion using datasets in offline mode   \n",
       "3  Discussion using datasets in offline mode   \n",
       "4  Discussion using datasets in offline mode   \n",
       "\n",
       "                                            comments  \\\n",
       "0  here is my way to load a dataset offline, but ...   \n",
       "1  > here is my way to load a dataset offline, bu...   \n",
       "2  I opened a PR that allows to reload modules th...   \n",
       "3  The local dataset builders (csv, text , json a...   \n",
       "4  Requiring online connection is a deal breaker ...   \n",
       "\n",
       "                                                body  comment_length  \\\n",
       "0  `datasets.load_dataset(\"csv\", ...)` breaks if ...              47   \n",
       "1  `datasets.load_dataset(\"csv\", ...)` breaks if ...              76   \n",
       "2  `datasets.load_dataset(\"csv\", ...)` breaks if ...             179   \n",
       "3  `datasets.load_dataset(\"csv\", ...)` breaks if ...              38   \n",
       "4  `datasets.load_dataset(\"csv\", ...)` breaks if ...              57   \n",
       "\n",
       "                                                text  \\\n",
       "0  Discussion using datasets in offline mode \\n `...   \n",
       "1  Discussion using datasets in offline mode \\n `...   \n",
       "2  Discussion using datasets in offline mode \\n `...   \n",
       "3  Discussion using datasets in offline mode \\n `...   \n",
       "4  Discussion using datasets in offline mode \\n `...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.4902576208114624, 0.22889626026153564, -0....  \n",
       "1  [-0.4992600083351135, 0.2269977331161499, -0.0...  \n",
       "2  [-0.4716480076313019, 0.29022741317749023, -0....  \n",
       "3  [-0.4490852653980255, 0.20950627326965332, -0....  \n",
       "4  [-0.4731806516647339, 0.24578382074832916, -0....  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9a1b699f-d24b-4b8e-b564-c66473aa2fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>here is my way to load a dataset offline, but ...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>47</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4902576208114624, 0.22889626026153564, -0....</td>\n",
       "      <td>22.406656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>&gt; here is my way to load a dataset offline, bu...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>76</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4992600083351135, 0.2269977331161499, -0.0...</td>\n",
       "      <td>22.894001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>I opened a PR that allows to reload modules th...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>179</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4716480076313019, 0.29022741317749023, -0....</td>\n",
       "      <td>24.148985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>The local dataset builders (csv, text , json a...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>38</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4490852653980255, 0.20950627326965332, -0....</td>\n",
       "      <td>24.555540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>Requiring online connection is a deal breaker ...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>57</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4731806516647339, 0.24578382074832916, -0....</td>\n",
       "      <td>25.505016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                       title  \\\n",
       "0  Discussion using datasets in offline mode   \n",
       "1  Discussion using datasets in offline mode   \n",
       "2  Discussion using datasets in offline mode   \n",
       "3  Discussion using datasets in offline mode   \n",
       "4  Discussion using datasets in offline mode   \n",
       "\n",
       "                                            comments  \\\n",
       "0  here is my way to load a dataset offline, but ...   \n",
       "1  > here is my way to load a dataset offline, bu...   \n",
       "2  I opened a PR that allows to reload modules th...   \n",
       "3  The local dataset builders (csv, text , json a...   \n",
       "4  Requiring online connection is a deal breaker ...   \n",
       "\n",
       "                                                body  comment_length  \\\n",
       "0  `datasets.load_dataset(\"csv\", ...)` breaks if ...              47   \n",
       "1  `datasets.load_dataset(\"csv\", ...)` breaks if ...              76   \n",
       "2  `datasets.load_dataset(\"csv\", ...)` breaks if ...             179   \n",
       "3  `datasets.load_dataset(\"csv\", ...)` breaks if ...              38   \n",
       "4  `datasets.load_dataset(\"csv\", ...)` breaks if ...              57   \n",
       "\n",
       "                                                text  \\\n",
       "0  Discussion using datasets in offline mode \\n `...   \n",
       "1  Discussion using datasets in offline mode \\n `...   \n",
       "2  Discussion using datasets in offline mode \\n `...   \n",
       "3  Discussion using datasets in offline mode \\n `...   \n",
       "4  Discussion using datasets in offline mode \\n `...   \n",
       "\n",
       "                                          embeddings     scores  \n",
       "0  [-0.4902576208114624, 0.22889626026153564, -0....  22.406656  \n",
       "1  [-0.4992600083351135, 0.2269977331161499, -0.0...  22.894001  \n",
       "2  [-0.4716480076313019, 0.29022741317749023, -0....  24.148985  \n",
       "3  [-0.4490852653980255, 0.20950627326965332, -0....  24.555540  \n",
       "4  [-0.4731806516647339, 0.24578382074832916, -0....  25.505016  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_df[\"scores\"] = scores\n",
    "samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "82e41e54-6c10-4f4d-8e7e-72edc4d83024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_length</th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>Requiring online connection is a deal breaker ...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>57</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4731806516647339, 0.24578382074832916, -0....</td>\n",
       "      <td>25.505016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>The local dataset builders (csv, text , json a...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>38</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4490852653980255, 0.20950627326965332, -0....</td>\n",
       "      <td>24.555540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>I opened a PR that allows to reload modules th...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>179</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4716480076313019, 0.29022741317749023, -0....</td>\n",
       "      <td>24.148985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>&gt; here is my way to load a dataset offline, bu...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>76</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4992600083351135, 0.2269977331161499, -0.0...</td>\n",
       "      <td>22.894001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>here is my way to load a dataset offline, but ...</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>47</td>\n",
       "      <td>Discussion using datasets in offline mode \\n `...</td>\n",
       "      <td>[-0.4902576208114624, 0.22889626026153564, -0....</td>\n",
       "      <td>22.406656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                       title  \\\n",
       "4  Discussion using datasets in offline mode   \n",
       "3  Discussion using datasets in offline mode   \n",
       "2  Discussion using datasets in offline mode   \n",
       "1  Discussion using datasets in offline mode   \n",
       "0  Discussion using datasets in offline mode   \n",
       "\n",
       "                                            comments  \\\n",
       "4  Requiring online connection is a deal breaker ...   \n",
       "3  The local dataset builders (csv, text , json a...   \n",
       "2  I opened a PR that allows to reload modules th...   \n",
       "1  > here is my way to load a dataset offline, bu...   \n",
       "0  here is my way to load a dataset offline, but ...   \n",
       "\n",
       "                                                body  comment_length  \\\n",
       "4  `datasets.load_dataset(\"csv\", ...)` breaks if ...              57   \n",
       "3  `datasets.load_dataset(\"csv\", ...)` breaks if ...              38   \n",
       "2  `datasets.load_dataset(\"csv\", ...)` breaks if ...             179   \n",
       "1  `datasets.load_dataset(\"csv\", ...)` breaks if ...              76   \n",
       "0  `datasets.load_dataset(\"csv\", ...)` breaks if ...              47   \n",
       "\n",
       "                                                text  \\\n",
       "4  Discussion using datasets in offline mode \\n `...   \n",
       "3  Discussion using datasets in offline mode \\n `...   \n",
       "2  Discussion using datasets in offline mode \\n `...   \n",
       "1  Discussion using datasets in offline mode \\n `...   \n",
       "0  Discussion using datasets in offline mode \\n `...   \n",
       "\n",
       "                                          embeddings     scores  \n",
       "4  [-0.4731806516647339, 0.24578382074832916, -0....  25.505016  \n",
       "3  [-0.4490852653980255, 0.20950627326965332, -0....  24.555540  \n",
       "2  [-0.4716480076313019, 0.29022741317749023, -0....  24.148985  \n",
       "1  [-0.4992600083351135, 0.2269977331161499, -0.0...  22.894001  \n",
       "0  [-0.4902576208114624, 0.22889626026153564, -0....  22.406656  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52574b0e-4c64-45be-9dee-ebb5da496c33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
      "\n",
      "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "SCORE: 25.505016326904297\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
      "You can now use them offline\n",
      "```python\n",
      "datasets = load_dataset('text', data_files=data_files)\n",
      "```\n",
      "\n",
      "We'll do a new release soon\n",
      "SCORE: 24.555540084838867\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
      "\n",
      "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n",
      "\n",
      "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
      "\n",
      "----------\n",
      "\n",
      "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "\n",
      "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
      "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
      "```python\n",
      "load_dataset(\"./my_dataset\")\n",
      "```\n",
      "and the dataset script will generate your dataset once and for all.\n",
      "\n",
      "----------\n",
      "\n",
      "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
      "cf #1724 \n",
      "SCORE: 24.148984909057617\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
      "> \n",
      "> 1. (online machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_dataset(...)\n",
      "> \n",
      "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> 2. copy the dir from online to the offline machine\n",
      "> \n",
      "> 3. (offline machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> \n",
      "> \n",
      "> HTH.\n",
      "\n",
      "\n",
      "SCORE: 22.894001007080078\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
      "1. (online machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_dataset(...)\n",
      "data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "```\n",
      "2. copy the dir from online to the offline machine\n",
      "3. (offline machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "```\n",
      "\n",
      "HTH.\n",
      "SCORE: 22.40665626525879\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d0e4fdb4-774a-4bb2-b81c-fe0b69edeac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\"embeddings\", question_embedding, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66dd9a94-5d4c-4ee5-8df5-3494307833da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75c80142-8e08-42e2-ad0e-6499ca44a387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: Hi, thanks for the suggestion. It's not possible at the moment. The viewer is part of the Hub codebase and only works on public datasets. Also, it relies on [Datasets Server](https://github.com/huggingface/datasets-server/), which prepares the data and provides an API to access the rows, size, etc.\n",
      "\n",
      "If you're interested in hosting your data as a private dataset on the Hub, you might want to look at https://github.com/huggingface/datasets-server/issues/39.\n",
      "SCORE: 32.6430778503418\n",
      "TITLE: Offline dataset viewer\n",
      "URL: https://github.com/huggingface/datasets/issues/6139\n",
      "==================================================\n",
      "\n",
      "COMMENT: Yes currently you need an internet connection because the lib tries to check for the etag of the dataset script online to see if you don't have it locally already.\n",
      "\n",
      "If we add a way to store the etag/hash locally after the first download, it would allow users to first download the dataset with an internet connection, and still have it working without an internet connection.\n",
      "\n",
      "I'll let you know when we add this feature.\n",
      "SCORE: 31.704299926757812\n",
      "TITLE: Downloaded datasets are not usable offline\n",
      "URL: https://github.com/huggingface/datasets/issues/761\n",
      "==================================================\n",
      "\n",
      "COMMENT: It looks like a bug in `fsspec`, can you try updating `fsspec` (and maybe `datasets` as well) ?\n",
      "SCORE: 30.441139221191406\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: I think it would be very cool. I'm currently working on a cluster from Compute Canada, and I have internet access only when I'm not in the nodes where I run the scripts. So I was expecting to be able to use the wmt14 dataset until I realized I needed internet connection even if I downloaded the data already. I'm going to try option 2 you mention for now though! Thanks ;)\n",
      "SCORE: 26.60210418701172\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: @albertvillanova \n",
      "datasets version： 2.10.1\n",
      "I load_dataset and save_to_disk sucessfully on windows10, and I copy the dataset dir\n",
      "into a ubuntu system, and when I load_from_disk(dir),  something weird happens:\n",
      "\n",
      "\n",
      "```\n",
      "load_from_disk('/LLM/data/wiki')\n",
      "  File \"/usr/local/miniconda3/lib/python3.8/site-packages/datasets/load.py\", line 1874, in load_from_disk\n",
      "    return DatasetDict.load_from_disk(dataset_path, keep_in_memory=keep_in_memory, storage_options=storage_options)\n",
      "  File \"/usr/local/miniconda3/lib/python3.8/site-packages/datasets/dataset_dict.py\", line 1309, in load_from_disk\n",
      "    dataset_dict[k] = Dataset.load_from_disk(\n",
      "  File \"/usr/local/miniconda3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1543, in load_from_disk\n",
      "    fs_token_paths = fsspec.get_fs_token_paths(dataset_path, storage_options=storage_options)\n",
      "  File \"/usr/local/miniconda3/lib/python3.8/site-packages/fsspec/core.py\", line 610, in get_fs_token_paths\n",
      "    chain = _un_chain(urlpath0, storage_options or {})\n",
      "  File \"/usr/local/miniconda3/lib/python3.8/site-packages/fsspec/core.py\", line 325, in _un_chain\n",
      "    cls = get_filesystem_class(protocol)\n",
      "  File \"/usr/local/miniconda3/lib/python3.8/site-packages/fsspec/registry.py\", line 232, in get_filesystem_class\n",
      "    raise ValueError(f\"Protocol not known: {protocol}\")\n",
      "ValueError: Protocol not known: /LLM/data/wiki\n",
      "```\n",
      "It seems that something went wrong on the arrow file?\n",
      "How can I solve this , since currently I can not save_to_disk on ubuntu system\n",
      "SCORE: 25.807376861572266\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
      "\n",
      "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "SCORE: 25.505016326904297\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
      "You can now use them offline\n",
      "```python\n",
      "datasets = load_dataset('text', data_files=data_files)\n",
      "```\n",
      "\n",
      "We'll do a new release soon\n",
      "SCORE: 24.555540084838867\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
      "\n",
      "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n",
      "\n",
      "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
      "\n",
      "----------\n",
      "\n",
      "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "\n",
      "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
      "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
      "```python\n",
      "load_dataset(\"./my_dataset\")\n",
      "```\n",
      "and the dataset script will generate your dataset once and for all.\n",
      "\n",
      "----------\n",
      "\n",
      "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
      "cf #1724 \n",
      "SCORE: 24.148984909057617\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
      "> \n",
      "> 1. (online machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_dataset(...)\n",
      "> \n",
      "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> 2. copy the dir from online to the offline machine\n",
      "> \n",
      "> 3. (offline machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> \n",
      "> \n",
      "> HTH.\n",
      "\n",
      "\n",
      "SCORE: 22.894001007080078\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
      "1. (online machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_dataset(...)\n",
      "data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "```\n",
      "2. copy the dir from online to the offline machine\n",
      "3. (offline machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "```\n",
      "\n",
      "HTH.\n",
      "SCORE: 22.40665626525879\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c1d97dd-8ea7-45e2-b40c-2142f61d2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "#pubmed_dataset = load_dataset(\"Shaier/pubmed\") # Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3fe10956-f2a5-48dc-8ef4-056b55b81950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age\n",
      "0    Alice   25\n",
      "1      Bob   30\n",
      "2  Charlie   35\n",
      "3    David   40\n",
      "      Name  Age  Salary\n",
      "0    Alice   25   50000\n",
      "1      Bob   30   60000\n",
      "2  Charlie   35   70000\n",
      "3    David   40   80000\n"
     ]
    }
   ],
   "source": [
    "#https://docs.kanaries.net/ko/topics/Pandas/pandas-add-column\n",
    "\n",
    "import pandas as pd\n",
    " \n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 40]\n",
    "}\n",
    " \n",
    "df = pd.DataFrame(data)\n",
    "print(df) \n",
    "\n",
    "#df['Salary'] = Nan\n",
    "#print(df)\n",
    "\n",
    "df['Salary'] = [50000, 60000, 70000, 80000]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23332dc-f368-4c0c-932c-c6dc6fc56d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
