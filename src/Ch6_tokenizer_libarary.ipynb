{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca4a0bb-fe31-4fe1-9f2e-45ecb71bb13e",
   "metadata": {},
   "source": [
    "### 1. 기존 토크나이저에서 새로운 토크나이저 학습\n",
    "\n",
    "#### 말뭉치 모으기\n",
    "🤗Transformers에는 기존에 존재하는 것들과 동일한 특성을 가진 새로운 토크나이저를 학습하는데 사용할 수 있는 매우 간단한 API가 있습니다. 바로 AutoTokenizer.train_new_from_iterator()가 그것입니다. 이를 실제로 실행해 보기 위해 GPT-2를 처음부터 영어가 아닌 다른 언어로 학습하고 싶다고 가정해 보겠습니다. 첫 번째 작업은 해당 언어로 표현된 대규모의 데이터를 수집하여 학습 말뭉치로 구성하는 것입니다. 모든 사람이 이해할 수 있는 예시를 제공하기 위해 여기서는 러시아어나 중국어와 같은 언어가 아니라 특수한 영어 텍스트로 볼 수 있는 Python 소스코드 집합을 사용합니다.\n",
    "\n",
    "🤗Datasets 라이브러리는 Python 소스코드를 모으는데 도움을 줄 수 있습니다. 간단하게 load_dataset() 함수를 사용하여 CodeSearchNet 데이터셋을 다운로드하고 캐시합니다. 이 데이터셋은 CodeSearchNet 챌린지를 위해 생성되었으며 여러 프로그래밍 언어로 된 GitHub의 오픈소스 라이브러리에서 수백만 개의 함수를 포함하고 있습니다. 이 예시에서는 이 데이터셋의 Python 부분을 로드합니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67eecb8-7289-4abe-927f-bbde127bfc17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "# 로드하는데 몇 분이 소요될 수 있습니다. 커피나 차를 준비하세요.\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59e123c-7624-4745-a7b5-218a1a901052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 412178\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 22176\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 23107\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afcfb753-841a-476b-a3ad-2a8ee6dff7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19da7073-0e2b-43cc-9521-e09693dcfd98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'func_code_string': 'def findArgs(args, prefixes):\\n'\n",
      "                     '\\t\\t\"\"\"\\n'\n",
      "                     '\\t\\tExtracts the list of arguments that start with any '\n",
      "                     'of the specified prefix values\\n'\n",
      "                     '\\t\\t\"\"\"\\n'\n",
      "                     '\\t\\treturn list([\\n'\n",
      "                     '\\t\\t\\targ for arg in args\\n'\n",
      "                     '\\t\\t\\tif len([p for p in prefixes if '\n",
      "                     'arg.lower().startswith(p.lower())]) > 0\\n'\n",
      "                     '\\t\\t])',\n",
      " 'func_code_tokens': ['def',\n",
      "                      'findArgs',\n",
      "                      '(',\n",
      "                      'args',\n",
      "                      ',',\n",
      "                      'prefixes',\n",
      "                      ')',\n",
      "                      ':',\n",
      "                      'return',\n",
      "                      'list',\n",
      "                      '(',\n",
      "                      '[',\n",
      "                      'arg',\n",
      "                      'for',\n",
      "                      'arg',\n",
      "                      'in',\n",
      "                      'args',\n",
      "                      'if',\n",
      "                      'len',\n",
      "                      '(',\n",
      "                      '[',\n",
      "                      'p',\n",
      "                      'for',\n",
      "                      'p',\n",
      "                      'in',\n",
      "                      'prefixes',\n",
      "                      'if',\n",
      "                      'arg',\n",
      "                      '.',\n",
      "                      'lower',\n",
      "                      '(',\n",
      "                      ')',\n",
      "                      '.',\n",
      "                      'startswith',\n",
      "                      '(',\n",
      "                      'p',\n",
      "                      '.',\n",
      "                      'lower',\n",
      "                      '(',\n",
      "                      ')',\n",
      "                      ')',\n",
      "                      ']',\n",
      "                      ')',\n",
      "                      '>',\n",
      "                      '0',\n",
      "                      ']',\n",
      "                      ')'],\n",
      " 'func_code_url': 'https://github.com/adamrehn/ue4cli/blob/f1c34502c96059e36757b7433da7e98760a75a6f/ue4cli/Utility.py#L86-L93',\n",
      " 'func_documentation_string': 'Extracts the list of arguments that start with '\n",
      "                              'any of the specified prefix values',\n",
      " 'func_documentation_tokens': ['Extracts',\n",
      "                               'the',\n",
      "                               'list',\n",
      "                               'of',\n",
      "                               'arguments',\n",
      "                               'that',\n",
      "                               'start',\n",
      "                               'with',\n",
      "                               'any',\n",
      "                               'of',\n",
      "                               'the',\n",
      "                               'specified',\n",
      "                               'prefix',\n",
      "                               'values'],\n",
      " 'func_name': 'Utility.findArgs',\n",
      " 'func_path_in_repository': 'ue4cli/Utility.py',\n",
      " 'language': 'python',\n",
      " 'repository_name': 'adamrehn/ue4cli',\n",
      " 'split_name': 'train',\n",
      " 'whole_func_string': 'def findArgs(args, prefixes):\\n'\n",
      "                      '\\t\\t\"\"\"\\n'\n",
      "                      '\\t\\tExtracts the list of arguments that start with any '\n",
      "                      'of the specified prefix values\\n'\n",
      "                      '\\t\\t\"\"\"\\n'\n",
      "                      '\\t\\treturn list([\\n'\n",
      "                      '\\t\\t\\targ for arg in args\\n'\n",
      "                      '\\t\\t\\tif len([p for p in prefixes if '\n",
      "                      'arg.lower().startswith(p.lower())]) > 0\\n'\n",
      "                      '\\t\\t])'}\n"
     ]
    }
   ],
   "source": [
    "pprint(raw_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a57525-c1e4-49f4-9aec-a903dafe73e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def check_result(running, recurse=False, highstate=None):\n",
      "    '''\n",
      "    Check the total return value of the run and determine if the running\n",
      "    dict has any issues\n",
      "    '''\n",
      "    if not isinstance(running, dict):\n",
      "        return False\n",
      "\n",
      "    if not running:\n",
      "        return False\n",
      "\n",
      "    ret = True\n",
      "    for state_id, state_result in six.iteritems(running):\n",
      "        expected_type = dict\n",
      "        # The __extend__ state is a list\n",
      "        if \"__extend__\" == state_id:\n",
      "            expected_type = list\n",
      "        if not recurse and not isinstance(state_result, expected_type):\n",
      "            ret = False\n",
      "        if ret and isinstance(state_result, dict):\n",
      "            result = state_result.get('result', _empty)\n",
      "            if result is False:\n",
      "                ret = False\n",
      "            # only override return value if we are not already failed\n",
      "            elif result is _empty and isinstance(state_result, dict) and ret:\n",
      "                ret = check_result(\n",
      "                    state_result, recurse=True, highstate=highstate)\n",
      "        # if we detect a fail, check for onfail requisites\n",
      "        if not ret:\n",
      "            # ret can be None in case of no onfail reqs, recast it to bool\n",
      "            ret = bool(check_onfail_requisites(state_id, state_result,\n",
      "                                               running, highstate))\n",
      "        # return as soon as we got a failure\n",
      "        if not ret:\n",
      "            break\n",
      "    return ret\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb1121b-581f-4e98-a881-8d27a53df646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 182.55 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Process.memory_info는 바이트 단위로 표시되므로 이를 메가바이트로 변환합니다.\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "# 여기에서 rss 속성은 프로세스가 RAM에서 차지하는 메모리 비율인 resident set size(상주 세트 크기)를 나타냅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4723e74a-eec2-4708-af15-53930c95752a",
   "metadata": {},
   "source": [
    "\n",
    "가장 먼저 해야 할 일은 데이터셋을 텍스트 리스트의 이터레이터(iterator)로 변환하는 것입니다. 예를 들어, 텍스트 **리스트의 리스트**로 구성할 수 있습니다. 텍스트 리스트를 사용하면 개별 텍스트를 하나씩 처리하는 대신 텍스트 배치(batches)에 대한 학습을 통해서 토크나이저가 더 빨라질 수 있으며, 모든 것을 한 번에 메모리에 로딩하지 않으려면 이 리스트를 이터레이터(iterator)로 변환되어야 합니다. 말뭉치의 규모가 크다면 🤗Datasets는 RAM에 모든 것을 로드하지 않고 데이터셋의 요소를 디스크에 저장한다는 사실을 활용할 수 있습니다.\n",
    "\n",
    "다음을 수행하면 각각 1,000개의 텍스트 리스트가 생성되지만 모든 것이 메모리에 로드됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1046e8f-477e-4fb2-af78-60e42aab9840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터셋의 규모가 작지 않다면, 다음 라인의 주석을 제거하지 마세요.\n",
    "#training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)]\n",
    "#print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "# RAM used: 2142.84 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ce18cf-0372-4d19-b234-3b451733428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(training_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28c77028-a973-4f82-b746-e470f17a2849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2879e1f-f832-45bd-9482-1efaf404e3c2",
   "metadata": {},
   "source": [
    "Python 제너레이터(generator)를 사용하면 실제로 필요할 때까지 Python이 메모리에 아무 것도 로드하지 않도록 할 수 있습니다. 이러한 생성기를 만들려면 꺽쇠괄호(brackets)를 소괄호(parentheses)로 바꾸기만 하면 됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187d54b5-7511-46ce-a745-47b75939fd64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 182.80 MB\n"
     ]
    }
   ],
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")\n",
    "\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "# RAM used: 167.87 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5564ed8-e678-4ad3-9706-4c354ee34234",
   "metadata": {},
   "source": [
    "위 코드는 데이터셋의 요소를 가져오지 않습니다. Python의 for 루프에서 사용할 수 있는 객체를 생성할 뿐입니다. 텍스트는 필요할 때만 메모리로 로드되며(즉, 해당 텍스트 집합이 필요한 for-loop 단계에 있을 때만) 한 번에 1,000개의 텍스트만 로드됩니다. 이렇게 하면 거대한 데이터셋을 처리하더라도 메모리를 완전히 소진하지 않습니다.\n",
    "\n",
    "파이썬 제너레이터(generator) 객체의 문제점은 단 한번만 사용할 수 있다는 것입니다. 따라서, 아래 코드의 결과는 우리가 예상한 것처럼 10개의 숫자 리스트를 두번 출력하는 것이 아니라:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "268a6cef-4461-4e6a-8417-b0a0f5d083ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "gen = (i for i in range(10))\n",
    "print(list(gen))\n",
    "print(list(gen))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621607e4-e274-4dcf-ae86-7eb3e612010b",
   "metadata": {},
   "source": [
    "첫번째 print문은 10개의 숫자를 출력하지만, 그 다음은 비어있는 리스트를 출력하고 있습니다. 이것이 바로 제너레이터(generator)를 반환하는 함수를 정의하는 이유입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcba76ee-317e-4df7-8230-96d6ba94d4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "training_corpus = get_training_corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "020b0585-3ad0-451e-b3c5-682e8bc078ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = next(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6132ce82-5ca0-4e0a-8ca5-8fc6a5b6e13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb202b0-b271-48e0-b4fa-e4e42532977c",
   "metadata": {},
   "source": [
    "yield 문을 사용하여 for 루프 내에서 제너레이터(generator)를 정의할 수도 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ae97e6a-d4bb-47c5-8e8e-ebfcb5287aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "        \n",
    "        \n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60299c9c-45be-4144-88a0-7aea786f877b",
   "metadata": {},
   "source": [
    "이전과 동일한 제너레이터를 생성하지만 리스트 내포(list comprehension)에서 할 수 있는 것보다 더 복잡한 로직을 사용할 수 있습니다.\n",
    "\n",
    "#### 새로운 토크나이저 학습\n",
    "이제 텍스트 배치(batch)의 이터레이터(iterator) 형태로 말뭉치를 구성했으므로 새로운 토크나이저를 학습할 준비가 되었습니다. 학습을 위해서 먼저 모델과 일치시키려는 토크나이저를 로드해야 합니다. 여기서는 GPT-2를 사용하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5699fc15-72b6-4d62-8d9e-966e37345148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db6fd3a1-4d7e-44af-ad93-ab8ab29e2426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b411e70-011e-4550-9cfa-b9464fdc3fb9",
   "metadata": {},
   "source": [
    "새로운 토크나이저를 학습하지만, 완전히 처음부터 시작하지 않도록 하는 것이 좋습니다. 이렇게 하면 토큰화 알고리즘이나 사용하려는 특수 토큰(special tokens)에 대해 아무 것도 신경쓰거나 지정할 필요가 없습니다. 우리의 새로운 토크나이저는 GPT-2와 정확히 동일할 것이며, 우리 말뭉치를 이용한 학습을 통해 vocabulary만 변경됩니다.\n",
    "\n",
    "먼저 이 토크나이저가 예제 함수(example function)를 처리하는 방법을 살펴보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8631069-fd46-4f24-915c-4e96569b9ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'n',\n",
       " 'umbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġb',\n",
       " '):',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ\"\"\"',\n",
       " 'Add',\n",
       " 'Ġthe',\n",
       " 'Ġtwo',\n",
       " 'Ġnumbers',\n",
       " 'Ġ`',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ġand',\n",
       " 'Ġ`',\n",
       " 'b',\n",
       " '`',\n",
       " '.\"',\n",
       " '\"\"',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġreturn',\n",
       " 'Ġa',\n",
       " 'Ġ+',\n",
       " 'Ġb']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5d2844e-6db8-4a24-a57e-24eb64c8bde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5d005-c438-4fd3-8f80-fb5deb02945e",
   "metadata": {},
   "source": [
    "이 토크나이저는 각각 공백과 줄바꿈을 나타내는 Ċ 및 Ġ와 같은 몇가지 특수 기호를 포함하고 있습니다. 결과에서 볼 수 있듯이, 이는 효율적이지 않습니다. 여러 개의 공백이 나타날 때 토크나이저는 이를 그룹화하여 하나의 토큰으로 표현할 수도 있는데, 여기서는 각 공백을 개별 토큰으로 표현하고 있습니다. 소스코드에서 4개 또는 8개의 공백 그룹이 나타나는 것은 매우 일반적입니다. 또한 _ 문자가 있는 단어가 익숙하지 않은지, 함수명을 약간 이상하게 분할합니다.\n",
    "\n",
    "새로운 토크나이저를 학습하고 이러한 문제를 해결하는지 봅시다. 이를 위해 우리는 train_new_from_iterator() 메서드를 사용할 것입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46412452-df55-477b-83fa-5bdd87bdda1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 3min 17s, sys: 12 s, total: 3min 29s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37fc3c3-90e4-4143-8f0e-dd93781c2776",
   "metadata": {},
   "source": [
    "이 명령은 말뭉치가 매우 큰 경우 시간이 걸릴 수 있지만 1.6GB 텍스트 데이터셋의 경우에는 매우 빠릅니다(12코어가 있는 AMD Ryzen 9 3900X CPU에서 1분 16초).\n",
    "\n",
    "AutoTokenizer.train_new_from_iterator()는 사용 중인 토크나이저가 \"빠른(fast)\" 토크나이저인 경우에만 작동합니다. 다음 섹션에서 볼 수 있듯이 🤗Transformers 라이브러리에는 두 가지 유형의 토크나이저가 포함되어 있습니다. 한 유형은 순수하게 Python으로 작성되어 있고 다른 유형(빠른 토크나이저)은 🤗Tokenizers 라이브러리의 도움을 받아서 Rust 프로그래밍 언어로 작성된 토크나이저입니다. Python은 데이터 과학 및 딥러닝 응용 프로그램에 가장 자주 사용되는 언어이지만 빠른 병렬 처리가 필요한 경우 다른 언어로 작성해야 합니다. 예를 들어, 모델 계산(model computation)의 핵심인 행렬 곱셈(matrix multiplication)은 GPU에 최적화된 C 라이브러리인 CUDA로 작성되어 있습니다.\n",
    "\n",
    "순수한 Python으로 새로운 토크나이저를 학습하는 것은 엄청나게 느릴 것입니다. 이는 우리가 🤗Tokenizers 라이브러리를 개발한 이유입니다. GPU에 로드된 입력 배치(input batch)에서 모델을 실행하기 위해 CUDA 언어를 배울 필요가 없었던 것처럼 빠른 토크나이저를 사용하기 위해 Rust를 배울 필요가 없습니다. 🤗Tokenizers 라이브러리는 내부적으로 Rust의 일부 코드를 호출하는 많은 메서드에 대한 Python 바인딩을 제공합니다. 예를 들어, 새 토크나이저의 학습을 병렬화하거나 3장에서 보았듯이 입력 배치(batch)의 토큰화를 병렬화합니다.\n",
    "\n",
    "대부분의 트랜스포머(Transformer) 모델에는 사용 가능한 \"빠른(fast)\" 토크나이저가 있으며(여기에서 확인할 수 있는 몇 가지 예외가 있음) AutoTokenizer API는 사용 가능한 경우 항상 빠른 토크나이저를 선택합니다. 다음 섹션에서는 토큰 분류(token classification) 및 질의 응답(question answering)과 같은 작업에 정말 유용한 빠른 토크나이저의 다른 몇 가지 특수 기능을 살펴보겠습니다. 그러나 이에 대해 알아보기 전에 위 예제에서 새로운 토크나이저를 사용해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "171c157c-4e3b-4812-86fb-c0ffce177d36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'numbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġb',\n",
       " '):',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġ\"\"\"',\n",
       " 'Add',\n",
       " 'Ġthe',\n",
       " 'Ġtwo',\n",
       " 'Ġnumbers',\n",
       " 'Ġ`',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ġand',\n",
       " 'Ġ`',\n",
       " 'b',\n",
       " '`.\"\"\"',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġreturn',\n",
       " 'Ġa',\n",
       " 'Ġ+',\n",
       " 'Ġb']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce090018-bdc0-49ff-bc6c-12186dfb7eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=52000, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c7852-b689-455c-9de0-b084687ce912",
   "metadata": {},
   "source": [
    "위 결과에서 공백(space)과 줄바꿈(newline)을 나타내는 특수 기호 Ċ 및 Ġ를 다시 볼 수 있지만, 새롭게 학습된 토크나이저는 Python 함수(function) 코퍼스에 매우 특화된 일부 토큰을 학습했음을 알 수 있습니다. 예를 들어, 들여쓰기를 나타내는 ĊĠĠĠ 토큰과 독스트링을 시작하는 세 개의 따옴표를 나타내는 Ġ\"\"\" 토큰이 있습니다. 토크나이저는 _ 문자를 중심으로 함수명도 올바르게 분할합니다. 이는 매우 간결한(compact) 표현입니다. 이에 비해, 동일한 예제에서 일반적인 영어 토크나이저를 사용하면 더 긴 문장(혹은 토큰 시퀀스)을 얻을 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e75f462a-10d0-4219-a9fc-7ba8b504d13c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d51f6ca6-7256-4308-bc10-eef8e6eac4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'ĠLinear',\n",
       " 'Layer',\n",
       " '():',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġdef',\n",
       " 'Ġ__',\n",
       " 'init',\n",
       " '__(',\n",
       " 'self',\n",
       " ',',\n",
       " 'Ġinput',\n",
       " '_',\n",
       " 'size',\n",
       " ',',\n",
       " 'Ġoutput',\n",
       " '_',\n",
       " 'size',\n",
       " '):',\n",
       " 'ĊĠĠĠĠĠĠĠ',\n",
       " 'Ġself',\n",
       " '.',\n",
       " 'weight',\n",
       " 'Ġ=',\n",
       " 'Ġtorch',\n",
       " '.',\n",
       " 'randn',\n",
       " '(',\n",
       " 'input',\n",
       " '_',\n",
       " 'size',\n",
       " ',',\n",
       " 'Ġoutput',\n",
       " '_',\n",
       " 'size',\n",
       " ')',\n",
       " 'ĊĠĠĠĠĠĠĠ',\n",
       " 'Ġself',\n",
       " '.',\n",
       " 'bias',\n",
       " 'Ġ=',\n",
       " 'Ġtorch',\n",
       " '.',\n",
       " 'zeros',\n",
       " '(',\n",
       " 'output',\n",
       " '_',\n",
       " 'size',\n",
       " ')',\n",
       " 'ĊĠĊĠĠĠ',\n",
       " 'Ġdef',\n",
       " 'Ġ__',\n",
       " 'call',\n",
       " '__(',\n",
       " 'self',\n",
       " ',',\n",
       " 'Ġx',\n",
       " '):',\n",
       " 'ĊĠĠĠĠĠĠĠ',\n",
       " 'Ġreturn',\n",
       " 'Ġx',\n",
       " 'Ġ@',\n",
       " 'Ġself',\n",
       " '.',\n",
       " 'weights',\n",
       " 'Ġ+',\n",
       " 'Ġself',\n",
       " '.',\n",
       " 'bias',\n",
       " 'ĊĠĠĠĠ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    " \n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "tokenizer.tokenize(example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc94b1c-0271-47a6-aac4-d50df362c085",
   "metadata": {},
   "source": [
    "들여쓰기에 해당하는 토큰 외에도 여기에서는 이중 들여쓰기에 대한 토큰(ĊĠĠĠĠĠĠĠ)을 볼 수 있습니다. class, init, call, self, return과 같은 특수한 Python 단어는 각각 하나의 토큰으로 토큰화되며 _ 및 . 으로 분할되는 것을 볼 수 있습니다. 토크나이저는 camel-cased name도 올바르게 분할합니다. LinearLayer는 [\"ĠLinear\", \"Layer\"]로 토큰화됩니다.\n",
    "\n",
    "#### 학습된 토크나이저 저장\n",
    "이제 나중에 사용할 수 있도록 새 토크나이저를 저장해야 합니다. 모델과 마찬가지로 이 작업은 save_pretrained() 메서드로 수행됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a4765cd-84d8-48d1-a525-26df27f5307b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer/tokenizer_config.json',\n",
       " 'code-search-net-tokenizer/special_tokens_map.json',\n",
       " 'code-search-net-tokenizer/vocab.json',\n",
       " 'code-search-net-tokenizer/merges.txt',\n",
       " 'code-search-net-tokenizer/added_tokens.json',\n",
       " 'code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f92af5a-d9c7-4372-a2e1-e7509d48238b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e5ecd288ab4267a0e46af1aabc977c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a97be9bd-3477-427d-a8a8-cfed0a78d669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hwang2006/code-search-net-tokenizer/commit/e8ab7fb55d2ac92ff1a090844b54cbf622e74753', commit_message='Upload tokenizer', commit_description='', oid='e8ab7fb55d2ac92ff1a090844b54cbf622e74753', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\", use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e43ca73e-5e25-4565-9b3c-4c09c329f311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 당신이 직접 이 섹션에서 학습한 토크나이저를 사용하기 위해서,\n",
    "# 아래의 \"spasis\"를 당신의 실제 네임스페이스로 변경하십시오.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hwang2006/code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44798d-d7a4-4544-a3d9-70c9ffa63b34",
   "metadata": {},
   "source": [
    "### 2. \"빠른(fast)\" 토크나이저의 특별한 능력\n",
    "#### 배치 인코딩 (Batch encoding)\n",
    "토크나이저의 출력은 단순한 Python 딕셔너리가 아닙니다. 우리가 얻는 것은 실제로 특별한 BatchEncoding 객체입니다. 이것은 딕셔너리의 하위 클래스이지만(이것이 이전에 우리가 문제없이 해당 결과를 색인화할 수 있었던 이유입니다), 빠른 토크나이저에서 주로 사용하는 추가 메서드가 있습니다.\n",
    "\n",
    "병렬화(parallelization) 기능 외에도, 빠른 토크나이저의 주요 기능은 최종 토큰이 원본 텍스트에서 어디에 위치하는지 범위(span)를 항상 추적한다는 것입니다. 이를 오프셋 매핑(offset mapping) 이라고 합니다. 이것은 차례대로 각 단어를 생성된 토큰에 매핑하거나 원본 텍스트의 각 문자를 내부 토큰에 매핑하거나 그 반대로 매핑하는 것과 같은 기능들입니다.\n",
    "\n",
    "예를 살펴보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e644d22e-d6a8-47f7-aa44-99ed00c8cc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1422, 1271, 1110, 156, 7777, 2497, 1394, 1105, 146, 1250, 1120, 20164, 10932, 10289, 1107, 6010, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(encoding)\n",
    "print(type(encoding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71616453-7efe-448a-b337-e267101064c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be1b69-c7e3-4c45-bda3-1e3f137cb8bb",
   "metadata": {},
   "source": [
    "빠른 토크나이저를 가지고 우리가 무엇을 할 수 있는지 봅시다. 첫째, 토큰 아이디를 다시 토큰으로 변환하지 않고도 토큰에 액세스할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee87061a-7229-46bc-bb91-f12b9337be5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'S',\n",
       " '##yl',\n",
       " '##va',\n",
       " '##in',\n",
       " 'and',\n",
       " 'I',\n",
       " 'work',\n",
       " 'at',\n",
       " 'Hu',\n",
       " '##gging',\n",
       " 'Face',\n",
       " 'in',\n",
       " 'Brooklyn',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3183b4a-9676-4257-9c52-26fafaf35c60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6847e2fb-2892-4386-b849-df6bc4993514",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sylvain'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.word_to_chars(3)\n",
    "print(start, end)\n",
    "example[start:end]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1c1a9-1772-4d3d-9a18-5448774f1972",
   "metadata": {},
   "source": [
    "#### token-classification 파이프라인의 내부 동작\n",
    "1장에서 우리는 🤗Transformers의 pipeline() 함수를 사용하여, 텍스트의 어느 부분이 사람(person), 위치(location) 또는 조직(organization)과 같은 엔터티(entities)에 해당하는지 식별하는 작업인 NER을 처음으로 살펴봤습니다. 그런 다음 2장에서 파이프라인이 원시 텍스트를 대상으로 예측하는데 필요한 세 단계 즉, 토큰화(tokenization), 모델을 통한 입력 전달, 후처리(post-processing)를 어떻게 그룹화하는지를 보았습니다. token-classification 파이프라인의 처음 두 단계는 다른 파이프라인과 동일하지만 후처리(post-processing)는 조금 더 복잡합니다. 한번 살펴봅시다!\n",
    "\n",
    "#### 파이프라인으로 기본 실행 결과 도출하기\n",
    "먼저, 수작업으로 비교할 결과를 얻을 수 있도록 token-classification 파이프라인을 구현해 보겠습니다. 사용되는 모델은 dbmdz/bert-large-cased-finetuned-conll03-english입니다. 이 모델은 문장에 대해 NER를 수행합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d456e6f2-2916-47b5-8988-c620166de362",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99938285,\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99815494,\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99590707,\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99923277,\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9738931,\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.976115,\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9887976,\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9932106,\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7372d723-130c-4ecb-9492-b48448419ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2aa6a808-ce6e-4a5d-8520-f35c45548627",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9819008,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"average\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce5b6d-e45b-4e5d-ba5d-a0c1d8e527d2",
   "metadata": {},
   "source": [
    "aggregation_strategy를 위와 같이 지정하면 토큰들이 하나로 합쳐진 엔터티에 대해 새롭게 계산된 스코어를 제시합니다. \"simple\"의 경우 스코어는 해당 개체명 내의 각 토큰에 대한 스코어의 평균입니다. 예를 들어, \"Sylvain\"의 스코어는 이전 예에서 S, ##yl, ##va 및 ##in 토큰에 대해 계산된 스코어의 평균입니다. 사용 가능한 다른 지정자는 다음과 같습니다:\n",
    "\n",
    "\"first\", 여기서 각 개체명의 스코어는 해당 개체명의 첫 번째 토큰의 스코어입니다(따라서 \"Sylvain\"의 경우 토큰 S의 점수인 0.993828이 됨).\n",
    "\n",
    "\"max\", 여기서 각 엔터티의 스코어는 해당 엔터티내의 토큰들 중의 최대값 스코어입니다(\"Hugging Face\"의 경우 \"Face\"의 점수는 0.98879766이 됨).\n",
    "\n",
    "\"average\", 여기서 각 항목의 스코어는 해당 항목을 구성하는 단어(토큰이 아닙니다) 스코어의 평균입니다(따라서 \"Sylvain\"의 경우 \"simple\" 지정자와 차이가 없지만 \"Hugging Face\"의 점수는 0.9819이며 \"Hugging\"은 0.975이고 \"Face\"는 0.98879입니다).\n",
    "\n",
    "이제 pipeline() 함수를 사용하지 않고 이러한 결과를 얻는 방법을 살펴보겠습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1de42d0-8034-4113-a4f2-158c47eadc85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"ner\", grouped_entities=True)\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa66f26a-6fe4-44d4-99e8-3c83759974b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7147c12f-ee8b-41f3-a6c4-5af2452fa033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n",
      "torch.Size([1, 19, 9])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6d50610-7778-4d0e-b434-01da51679d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86bc3c-2eb3-4acd-abd0-51362730053c",
   "metadata": {},
   "source": [
    "19개의 토큰으로 구성된 1개의 시퀀스가 있는 배치(batch)가 있고 모델에는 9개의 서로 다른 레이블이 존재하므로 모델의 출력은 1 x 19 x 9의 모양을 갖습니다. text-classification 파이프라인과 마찬가지로 softmax 함수를 사용하여 해당 logits을 확률로 변환하고 argmax를 사용하여 예측 결과를 얻을 수 있습니다(softmax는 순서를 변경하지 않기 때문에 logits에 대해서 argmax를 취할 수 있습니다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7eb82774-6b1c-430c-8cad-9052100ac5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9994322657585144, 1.6470283298986033e-05, 3.4266999136889353e-05, 1.6042311472119763e-05, 8.250683458754793e-05, 2.1382273189374246e-05, 0.00015649090346414596, 1.965209776244592e-05, 0.00022089220874477178], [0.9989631175994873, 1.8515736883273348e-05, 5.240452446741983e-05, 1.253474511031527e-05, 0.0004347366339061409, 3.087432560278103e-05, 0.00031468752422370017, 2.78607003565412e-05, 0.00014510865730699152], [0.999708354473114, 8.30812678032089e-06, 2.8745640520355664e-05, 5.650358161801705e-06, 8.69486466399394e-05, 9.783458153833635e-06, 6.786145240766928e-05, 1.1793980775109958e-05, 7.241900311782956e-05], [0.9998350143432617, 5.645536475640256e-06, 1.3955165741208475e-05, 4.3133732106070966e-06, 4.017691026092507e-05, 8.123070074361749e-06, 5.6484961532987654e-05, 8.99163478607079e-06, 2.7239138944423757e-05], [0.00018333422485738993, 2.5156617994070984e-05, 4.8462032282259315e-05, 1.4900553651386872e-05, 0.9993828535079956, 1.99977403099183e-05, 0.00011153621017001569, 1.0790749911393505e-05, 0.00020288894302211702], [0.0006440270808525383, 7.437894237227738e-05, 0.00013196618237998337, 3.471966556389816e-05, 0.9981548190116882, 3.382968498044647e-05, 0.000543818052392453, 1.9978177078883164e-05, 0.0003624462988227606], [0.0016408422961831093, 9.469492943026125e-05, 0.00027364378911443055, 4.440645716385916e-05, 0.995907187461853, 5.1262213673908263e-05, 0.0012787937885150313, 3.283548358012922e-05, 0.0006763280252926052], [0.00022901801276020706, 2.518332257750444e-05, 5.7899465900845826e-05, 9.95701066131005e-06, 0.9992327690124512, 1.7655056581133977e-05, 0.00023448324645869434, 1.2356568731775042e-05, 0.0001806502405088395], [0.999804675579071, 5.465042249852559e-06, 1.2950029486091807e-05, 4.972443548467709e-06, 2.350327486055903e-05, 1.2930740012961905e-05, 9.509934898233041e-05, 8.891779543773737e-06, 3.146939707221463e-05], [0.9995046854019165, 1.4611860024160706e-05, 2.96468806482153e-05, 8.223405529861338e-06, 0.000160165160195902, 2.0456785932765342e-05, 0.0001753710093908012, 1.9349117792444304e-05, 6.74397379043512e-05], [0.9996776580810547, 7.596950126753654e-06, 1.700691063888371e-05, 3.7776110275444807e-06, 6.39606878394261e-05, 1.229785630130209e-05, 0.00018241394718643278, 7.648396604054142e-06, 2.766460966086015e-05], [0.999434769153595, 1.1278339115960989e-05, 2.8862770705018193e-05, 6.246603334147949e-06, 8.598288695793599e-05, 2.298940671607852e-05, 0.0003494526317808777, 1.3841339750797488e-05, 4.654669464798644e-05], [0.018156303092837334, 6.24591630185023e-05, 0.00026932242326438427, 4.766620259033516e-05, 0.0061346301808953285, 0.00023967465676832944, 0.9738931059837341, 7.093795284163207e-05, 0.001125914161093533], [0.014645998366177082, 0.0002047977759502828, 0.002236028900370002, 9.357065573567525e-05, 0.003731631673872471, 0.0005988667835481465, 0.9761149883270264, 0.00017609857604838908, 0.0021980972960591316], [0.0031715729273855686, 8.892173354979604e-05, 0.0015001465799286962, 7.653019565623254e-05, 0.0033575661946088076, 0.0004643830470740795, 0.9887974858283997, 0.00010871348058572039, 0.0024345973506569862], [0.9995326995849609, 6.553959792654496e-06, 2.831611709552817e-05, 6.235935870790854e-06, 3.73719994968269e-05, 2.035711077041924e-05, 0.00028727450990118086, 1.5032612282084301e-05, 6.614286394324154e-05], [0.0006589225959032774, 6.67124695610255e-05, 0.0002244396455353126, 4.190392428427003e-05, 0.00046020327135920525, 9.038783173309639e-05, 0.005088846664875746, 0.00015803218411747366, 0.99321049451828], [0.9994322657585144, 1.6470645277877338e-05, 3.426755574764684e-05, 1.6042418792494573e-05, 8.250888640759513e-05, 2.1382315026130527e-05, 0.00015649314445909113, 1.9652265109471045e-05, 0.00022089347476139665], [0.9994322657585144, 1.6470283298986033e-05, 3.426703187869862e-05, 1.6042327843024395e-05, 8.250691462308168e-05, 2.1382315026130527e-05, 0.00015649104898329824, 1.965213414223399e-05, 0.0002208926307503134]]\n",
      "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(probabilities)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee0cea07-b3ab-4e4e-b815-e00ebb57155f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d99fc521-5205-4cd1-98a9-d0fb74a661df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.9943e-01, 1.6470e-05, 3.4267e-05, 1.6042e-05, 8.2507e-05,\n",
       "          2.1382e-05, 1.5649e-04, 1.9652e-05, 2.2089e-04],\n",
       "         [9.9896e-01, 1.8516e-05, 5.2405e-05, 1.2535e-05, 4.3474e-04,\n",
       "          3.0874e-05, 3.1469e-04, 2.7861e-05, 1.4511e-04],\n",
       "         [9.9971e-01, 8.3081e-06, 2.8746e-05, 5.6504e-06, 8.6949e-05,\n",
       "          9.7835e-06, 6.7861e-05, 1.1794e-05, 7.2419e-05],\n",
       "         [9.9984e-01, 5.6455e-06, 1.3955e-05, 4.3134e-06, 4.0177e-05,\n",
       "          8.1231e-06, 5.6485e-05, 8.9916e-06, 2.7239e-05],\n",
       "         [1.8333e-04, 2.5157e-05, 4.8462e-05, 1.4901e-05, 9.9938e-01,\n",
       "          1.9998e-05, 1.1154e-04, 1.0791e-05, 2.0289e-04],\n",
       "         [6.4403e-04, 7.4379e-05, 1.3197e-04, 3.4720e-05, 9.9815e-01,\n",
       "          3.3830e-05, 5.4382e-04, 1.9978e-05, 3.6245e-04],\n",
       "         [1.6408e-03, 9.4695e-05, 2.7364e-04, 4.4406e-05, 9.9591e-01,\n",
       "          5.1262e-05, 1.2788e-03, 3.2835e-05, 6.7633e-04],\n",
       "         [2.2902e-04, 2.5183e-05, 5.7899e-05, 9.9570e-06, 9.9923e-01,\n",
       "          1.7655e-05, 2.3448e-04, 1.2357e-05, 1.8065e-04],\n",
       "         [9.9980e-01, 5.4650e-06, 1.2950e-05, 4.9724e-06, 2.3503e-05,\n",
       "          1.2931e-05, 9.5099e-05, 8.8918e-06, 3.1469e-05],\n",
       "         [9.9950e-01, 1.4612e-05, 2.9647e-05, 8.2234e-06, 1.6017e-04,\n",
       "          2.0457e-05, 1.7537e-04, 1.9349e-05, 6.7440e-05],\n",
       "         [9.9968e-01, 7.5970e-06, 1.7007e-05, 3.7776e-06, 6.3961e-05,\n",
       "          1.2298e-05, 1.8241e-04, 7.6484e-06, 2.7665e-05],\n",
       "         [9.9943e-01, 1.1278e-05, 2.8863e-05, 6.2466e-06, 8.5983e-05,\n",
       "          2.2989e-05, 3.4945e-04, 1.3841e-05, 4.6547e-05],\n",
       "         [1.8156e-02, 6.2459e-05, 2.6932e-04, 4.7666e-05, 6.1346e-03,\n",
       "          2.3967e-04, 9.7389e-01, 7.0938e-05, 1.1259e-03],\n",
       "         [1.4646e-02, 2.0480e-04, 2.2360e-03, 9.3571e-05, 3.7316e-03,\n",
       "          5.9887e-04, 9.7611e-01, 1.7610e-04, 2.1981e-03],\n",
       "         [3.1716e-03, 8.8922e-05, 1.5001e-03, 7.6530e-05, 3.3576e-03,\n",
       "          4.6438e-04, 9.8880e-01, 1.0871e-04, 2.4346e-03],\n",
       "         [9.9953e-01, 6.5540e-06, 2.8316e-05, 6.2359e-06, 3.7372e-05,\n",
       "          2.0357e-05, 2.8727e-04, 1.5033e-05, 6.6143e-05],\n",
       "         [6.5892e-04, 6.6712e-05, 2.2444e-04, 4.1904e-05, 4.6020e-04,\n",
       "          9.0388e-05, 5.0888e-03, 1.5803e-04, 9.9321e-01],\n",
       "         [9.9943e-01, 1.6471e-05, 3.4268e-05, 1.6042e-05, 8.2509e-05,\n",
       "          2.1382e-05, 1.5649e-04, 1.9652e-05, 2.2089e-04],\n",
       "         [9.9943e-01, 1.6470e-05, 3.4267e-05, 1.6042e-05, 8.2507e-05,\n",
       "          2.1382e-05, 1.5649e-04, 1.9652e-05, 2.2089e-04]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(outputs.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4916c79f-3ffa-4c1f-81c5-984ac9a8cf74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
       "  \"_num_labels\": 9,\n",
       "  \"architectures\": [\n",
       "    \"BertForTokenClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-MISC\",\n",
       "    \"2\": \"I-MISC\",\n",
       "    \"3\": \"B-PER\",\n",
       "    \"4\": \"I-PER\",\n",
       "    \"5\": \"B-ORG\",\n",
       "    \"6\": \"I-ORG\",\n",
       "    \"7\": \"B-LOC\",\n",
       "    \"8\": \"I-LOC\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"label2id\": {\n",
       "    \"B-LOC\": 7,\n",
       "    \"B-MISC\": 1,\n",
       "    \"B-ORG\": 5,\n",
       "    \"B-PER\": 3,\n",
       "    \"I-LOC\": 8,\n",
       "    \"I-MISC\": 2,\n",
       "    \"I-ORG\": 6,\n",
       "    \"I-PER\": 4,\n",
       "    \"O\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.39.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d97cfb41-c83a-44ba-911f-edac9f877f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f650cfca-265f-40a8-8c61-dd77b7eae2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a46dbc4c-0b4b-449a-b94f-a152839d2dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'},\n",
      " {'entity': 'I-PER', 'score': 0.9981548190116882, 'word': '##yl'},\n",
      " {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'},\n",
      " {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'},\n",
      " {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'},\n",
      " {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'},\n",
      " {'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face'},\n",
      " {'entity': 'I-LOC', 'score': 0.99321049451828, 'word': 'Brooklyn'}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "# predictions: [0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n",
    "# probabilities.shape = [19, 9]\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        results.append(\n",
    "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
    "        )\n",
    "\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b61f7e9-d511-48d6-9af9-340fc7b8884e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 10),\n",
       " (11, 12),\n",
       " (12, 14),\n",
       " (14, 16),\n",
       " (16, 18),\n",
       " (19, 22),\n",
       " (23, 24),\n",
       " (25, 29),\n",
       " (30, 32),\n",
       " (33, 35),\n",
       " (35, 40),\n",
       " (41, 45),\n",
       " (46, 48),\n",
       " (49, 57),\n",
       " (57, 58),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "inputs_with_offsets[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce2193c6-d8f9-4315-865b-55fcd533bca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yl'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[12:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d9ef623-d78e-4ace-a60a-6da5e82e3774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': 12,\n",
      "  'entity': 'I-PER',\n",
      "  'score': 0.9993828535079956,\n",
      "  'start': 11,\n",
      "  'word': 'S'},\n",
      " {'end': 14,\n",
      "  'entity': 'I-PER',\n",
      "  'score': 0.9981548190116882,\n",
      "  'start': 12,\n",
      "  'word': '##yl'},\n",
      " {'end': 16,\n",
      "  'entity': 'I-PER',\n",
      "  'score': 0.995907187461853,\n",
      "  'start': 14,\n",
      "  'word': '##va'},\n",
      " {'end': 18,\n",
      "  'entity': 'I-PER',\n",
      "  'score': 0.9992327690124512,\n",
      "  'start': 16,\n",
      "  'word': '##in'},\n",
      " {'end': 35,\n",
      "  'entity': 'I-ORG',\n",
      "  'score': 0.9738931059837341,\n",
      "  'start': 33,\n",
      "  'word': 'Hu'},\n",
      " {'end': 40,\n",
      "  'entity': 'I-ORG',\n",
      "  'score': 0.9761149883270264,\n",
      "  'start': 35,\n",
      "  'word': '##gging'},\n",
      " {'end': 45,\n",
      "  'entity': 'I-ORG',\n",
      "  'score': 0.9887974858283997,\n",
      "  'start': 41,\n",
      "  'word': 'Face'},\n",
      " {'end': 57,\n",
      "  'entity': 'I-LOC',\n",
      "  'score': 0.99321049451828,\n",
      "  'start': 49,\n",
      "  'word': 'Brooklyn'}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "# prediction : [0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n",
    "for idx, pred in enumerate(predictions): \n",
    "    label = model.config.id2label[pred]\n",
    "    if label != 'O':\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0c7f4e7-0173-4b29-b570-c70777c7c551",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[33:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c268bfd2-4af4-423e-bb4e-e3f6e37faa77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': 18,\n",
      "  'entity_group': 'PER',\n",
      "  'score': 0.998169407248497,\n",
      "  'start': 11,\n",
      "  'word': 'Sylvain'},\n",
      " {'end': 45,\n",
      "  'entity_group': 'ORG',\n",
      "  'score': 0.9796018600463867,\n",
      "  'start': 33,\n",
      "  'word': 'Hugging Face'},\n",
      " {'end': 57,\n",
      "  'entity_group': 'LOC',\n",
      "  'score': 0.99321049451828,\n",
      "  'start': 49,\n",
      "  'word': 'Brooklyn'}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e0768-1c96-4ff5-8fa2-47067fa8d0dd",
   "metadata": {},
   "source": [
    "### 3. QA 파이프라인에서의 \"빠른(fast)\" 토크나이저\n",
    "#### question-answering 파이프라인 사용하기\n",
    "1장에서 보았듯이 우리는 질문에 대한 답을 얻기 위해 다음과 같은 question-answering 파이프라인을 사용할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3816ca4-d6b6-499a-ab95-b478f7043e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9802603125572205,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "question_answerer(question=question, context=context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01308437-4e97-44e7-b1d5-32ace7957cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9714871048927307,\n",
       " 'start': 1892,\n",
       " 'end': 1919,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context = \"\"\"\n",
    "🤗 Transformers: State of the Art NLP\n",
    "\n",
    "🤗 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question_answerer(question=question, context=long_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e302d4f0-9ca6-400c-9674-53526fb96719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x2b02c825cac0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "379553f4-7e7b-4813-a4b9-f53a85dd1b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e63eaa5-ba17-44c4-b9b6-6a0ff20d821d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[  101,  5979,  1996,  3776,  9818,  1171,   100, 25267,   136,   102,\n",
      "           100, 25267,  1110,  5534,  1118,  1103,  1210,  1211,  1927,  1996,\n",
      "          3776,  9818,   783, 13612,   117,   153,  1183,  1942,  1766,  1732,\n",
      "           117,  1105,  5157, 21484,  2271,  6737,   783,  1114,   170,  2343,\n",
      "          1306,  2008,  9111,  1206,  1172,   119,  1135,   112,   188, 21546,\n",
      "          1106,  2669,  1240,  3584,  1114,  1141,  1196, 10745,  1172,  1111,\n",
      "          1107, 16792,  1114,  1103,  1168,   119,   102]])}\n"
     ]
    }
   ],
   "source": [
    "pprint(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1430f7eb-5479-43c9-a018-7d21232f55c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "['[CLS]', 'Which', 'deep', 'learning', 'libraries', 'back', '[UNK]', 'Transformers', '?', '[SEP]', '[UNK]', 'Transformers', 'is', 'backed', 'by', 'the', 'three', 'most', 'popular', 'deep', 'learning', 'libraries', '—', 'Jax', ',', 'P', '##y', '##T', '##or', '##ch', ',', 'and', 'Ten', '##sor', '##F', '##low', '—', 'with', 'a', 'sea', '##m', '##less', 'integration', 'between', 'them', '.', 'It', \"'\", 's', 'straightforward', 'to', 'train', 'your', 'models', 'with', 'one', 'before', 'loading', 'them', 'for', 'in', '##ference', 'with', 'the', 'other', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs.tokens()))\n",
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "961acae1-9a75-45bb-ac4a-9aa9640747f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-4.4952, -6.4454, -4.7115, -7.0968, -7.0726, -7.4981, -5.5397, -4.1368,\n",
       "         -5.9199, -5.4193, -1.5920, -1.0857, -5.0981, -2.9331, -3.4070,  2.2467,\n",
       "          5.1563, -1.3602, -2.2209, -0.9686, -4.8112, -2.2527,  1.4383, 10.1211,\n",
       "         -1.5311,  2.2685, -1.8951, -2.2108, -4.2142, -2.5571, -2.3252, -2.6046,\n",
       "          1.7047, -1.9867, -1.7211, -0.5415, -2.0239, -4.4246, -5.1012, -4.4966,\n",
       "         -7.8940, -6.7200, -4.6759, -6.3278, -4.8339, -5.1839, -3.3724, -7.4120,\n",
       "         -8.1542, -4.4871, -7.4659, -4.3293, -4.2293, -3.1903, -7.9467, -5.2665,\n",
       "         -7.5902, -5.0570, -7.4476, -7.9083, -6.5951, -7.4061, -8.8821, -7.6749,\n",
       "         -6.9879, -7.0466, -5.4193]], grad_fn=<CloneBackward0>), end_logits=tensor([[-2.3958e+00, -7.0978e+00, -7.0745e+00, -6.3676e+00, -5.9532e+00,\n",
       "         -7.9585e+00, -7.1869e+00, -3.6494e+00, -6.9677e+00, -5.1421e+00,\n",
       "         -3.1757e+00, -1.1649e+00, -7.0748e+00, -5.2875e+00, -6.8611e+00,\n",
       "         -5.1769e+00,  3.7892e+00, -4.4408e+00, -7.6688e-01, -3.9180e+00,\n",
       "         -2.1634e+00,  1.8116e+00, -1.4678e+00,  2.0508e+00,  1.5437e-03,\n",
       "         -1.5531e+00, -6.9469e-01, -1.3466e+00, -1.6879e+00,  4.0826e+00,\n",
       "          1.1467e+00, -3.7881e-01,  6.0774e-01,  1.2281e+00,  5.8202e-01,\n",
       "          1.0657e+01,  5.8794e+00, -5.7342e+00, -7.0719e+00, -6.8077e+00,\n",
       "         -7.1513e+00, -5.3228e+00, -3.4305e+00, -4.2575e+00,  2.2268e+00,\n",
       "         -4.1297e-01, -6.8944e+00, -7.9381e+00, -8.3298e+00, -5.6078e+00,\n",
       "         -8.9589e+00, -5.5772e+00, -5.7309e+00, -1.9592e+00, -7.8078e+00,\n",
       "         -2.3823e+00, -7.2457e+00, -6.1642e+00, -4.2830e+00, -8.0948e+00,\n",
       "         -8.0364e+00, -4.5566e+00, -7.6585e+00, -7.3241e+00, -2.2402e+00,\n",
       "         -1.8462e+00, -5.1420e+00]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a0294cc8-d350-43a8-9213-321713bb0c87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67]) torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16bff6da-9c78-419f-b3d5-fefea3a85c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None,\n",
      "                             start_logits=tensor([[-4.4952, -6.4454, -4.7115, -7.0968, -7.0726, -7.4981, -5.5397, -4.1368,\n",
      "         -5.9199, -5.4193, -1.5920, -1.0857, -5.0981, -2.9331, -3.4070,  2.2467,\n",
      "          5.1563, -1.3602, -2.2209, -0.9686, -4.8112, -2.2527,  1.4383, 10.1211,\n",
      "         -1.5311,  2.2685, -1.8951, -2.2108, -4.2142, -2.5571, -2.3252, -2.6046,\n",
      "          1.7047, -1.9867, -1.7211, -0.5415, -2.0239, -4.4246, -5.1012, -4.4966,\n",
      "         -7.8940, -6.7200, -4.6759, -6.3278, -4.8339, -5.1839, -3.3724, -7.4120,\n",
      "         -8.1542, -4.4871, -7.4659, -4.3293, -4.2293, -3.1903, -7.9467, -5.2665,\n",
      "         -7.5902, -5.0570, -7.4476, -7.9083, -6.5951, -7.4061, -8.8821, -7.6749,\n",
      "         -6.9879, -7.0466, -5.4193]], grad_fn=<CloneBackward0>),\n",
      "                             end_logits=tensor([[-2.3958e+00, -7.0978e+00, -7.0745e+00, -6.3676e+00, -5.9532e+00,\n",
      "         -7.9585e+00, -7.1869e+00, -3.6494e+00, -6.9677e+00, -5.1421e+00,\n",
      "         -3.1757e+00, -1.1649e+00, -7.0748e+00, -5.2875e+00, -6.8611e+00,\n",
      "         -5.1769e+00,  3.7892e+00, -4.4408e+00, -7.6688e-01, -3.9180e+00,\n",
      "         -2.1634e+00,  1.8116e+00, -1.4678e+00,  2.0508e+00,  1.5437e-03,\n",
      "         -1.5531e+00, -6.9469e-01, -1.3466e+00, -1.6879e+00,  4.0826e+00,\n",
      "          1.1467e+00, -3.7881e-01,  6.0774e-01,  1.2281e+00,  5.8202e-01,\n",
      "          1.0657e+01,  5.8794e+00, -5.7342e+00, -7.0719e+00, -6.8077e+00,\n",
      "         -7.1513e+00, -5.3228e+00, -3.4305e+00, -4.2575e+00,  2.2268e+00,\n",
      "         -4.1297e-01, -6.8944e+00, -7.9381e+00, -8.3298e+00, -5.6078e+00,\n",
      "         -8.9589e+00, -5.5772e+00, -5.7309e+00, -1.9592e+00, -7.8078e+00,\n",
      "         -2.3823e+00, -7.2457e+00, -6.1642e+00, -4.2830e+00, -8.0948e+00,\n",
      "         -8.0364e+00, -4.5566e+00, -7.6585e+00, -7.3241e+00, -2.2402e+00,\n",
      "         -1.8462e+00, -5.1420e+00]], grad_fn=<CloneBackward0>),\n",
      "                             hidden_states=None,\n",
      "                             attentions=None)\n"
     ]
    }
   ],
   "source": [
    "pprint(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "802f77e4-e0e4-439a-bb97-48888d8745bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23])\n",
      "torch.Size([1])\n",
      "<class 'torch.Tensor'>\n",
      "tensor(35)\n",
      "torch.Size([])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(start_logits.argmax(dim=-1))\n",
    "print(start_logits.argmax(dim=-1).shape)\n",
    "print(type(start_logits.argmax(dim=-1)))\n",
    "print(end_logits.argmax(dim=-1)[0])\n",
    "print(end_logits.argmax(dim=-1)[0].shape)\n",
    "print(type(end_logits.argmax(dim=-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf13a535-884c-471c-bf14-a652f0040dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Which', 'deep', 'learning', 'libraries', 'back', '[UNK]', 'Transformers', '?', '[SEP]', '[UNK]', 'Transformers', 'is', 'backed', 'by', 'the', 'three', 'most', 'popular', 'deep', 'learning', 'libraries', '—', 'Jax', ',', 'P', '##y', '##T', '##or', '##ch', ',', 'and', 'Ten', '##sor', '##F', '##low', '—', 'with', 'a', 'sea', '##m', '##less', 'integration', 'between', 'them', '.', 'It', \"'\", 's', 'straightforward', 'to', 'train', 'your', 'models', 'with', 'one', 'before', 'loading', 'them', 'for', 'in', '##ference', 'with', 'the', 'other', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7add8c57-d1cc-4ca5-abb4-96bd0fdc984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Which', 'deep']\n"
     ]
    }
   ],
   "source": [
    "print(inputs.tokens()[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "538736a0-c192-40ab-aa81-7f4a1ab85a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Which', 'deep']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt = (inputs.tokens())[0:3]\n",
    "ttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1be42a41-8a4f-43b0-9e8b-1fe806875976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax\n",
      "##low\n",
      "['Jax', ',', 'P', '##y', '##T', '##or', '##ch', ',', 'and', 'Ten', '##sor', '##F', '##low']\n"
     ]
    }
   ],
   "source": [
    "print((inputs.tokens())[23])\n",
    "print((inputs.tokens())[35])\n",
    "selected_elements = (inputs.tokens())[23:36]\n",
    "print(selected_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01098e93-3477-434c-a644-8fbc635429f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.sequence_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03f83abd-ea69-4283-9ac8-fa96a2edffff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5979,  1996,  3776,  9818,  1171,   100, 25267,   136,   102,\n",
       "           100, 25267,  1110,  5534,  1118,  1103,  1210,  1211,  1927,  1996,\n",
       "          3776,  9818,   783, 13612,   117,   153,  1183,  1942,  1766,  1732,\n",
       "           117,  1105,  5157, 21484,  2271,  6737,   783,  1114,   170,  2343,\n",
       "          1306,  2008,  9111,  1206,  1172,   119,  1135,   112,   188, 21546,\n",
       "          1106,  2669,  1240,  3584,  1114,  1141,  1196, 10745,  1172,  1111,\n",
       "          1107, 16792,  1114,  1103,  1168,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f095df4-3062-44b1-a2c4-47afaff69c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]\n",
      "[False, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "mask = [i != 1 for i in inputs.sequence_ids()]\n",
    "print(mask)\n",
    "mask[0] = False\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d6092de6-ecdd-45d5-96fc-1aab57b8ecb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True])\n",
      "torch.Size([1, 67])\n",
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.tensor(mask))\n",
    "print(torch.tensor(mask)[None].shape) \n",
    "print(torch.tensor([mask]))\n",
    "#print(torch.tensor([1, 2, 3], [3, 4, 5])[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0a92e2d-6aa9-4418-903a-07cff2b7c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.tensor([1, 2, 3], [3, 4, 5])[None])\n",
    "# TypeError: tensor() takes 1 positional argument but 2 were given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23d90484-be43-430c-a709-87127218b874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([[1, 2, 3], [3, 4, 5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea8c42ca-bff4-4382-a14e-349d0ce76e59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.4952, -6.4454, -4.7115, -7.0968, -7.0726, -7.4981, -5.5397, -4.1368,\n",
       "         -5.9199, -5.4193, -1.5920, -1.0857, -5.0981, -2.9331, -3.4070,  2.2467,\n",
       "          5.1563, -1.3602, -2.2209, -0.9686, -4.8112, -2.2527,  1.4383, 10.1211,\n",
       "         -1.5311,  2.2685, -1.8951, -2.2108, -4.2142, -2.5571, -2.3252, -2.6046,\n",
       "          1.7047, -1.9867, -1.7211, -0.5415, -2.0239, -4.4246, -5.1012, -4.4966,\n",
       "         -7.8940, -6.7200, -4.6759, -6.3278, -4.8339, -5.1839, -3.3724, -7.4120,\n",
       "         -8.1542, -4.4871, -7.4659, -4.3293, -4.2293, -3.1903, -7.9467, -5.2665,\n",
       "         -7.5902, -5.0570, -7.4476, -7.9083, -6.5951, -7.4061, -8.8821, -7.6749,\n",
       "         -6.9879, -7.0466, -5.4193]], grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "460e6ba0-8695-4eec-b9c0-e67a362bbbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# 컨텍스트 토큰들을 제외하고는 모두 마스킹한다.\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# [CLS] 토큰은 마스킹하지 않는다.\n",
    "mask[0] = False\n",
    "# adds another dimension using [None] to make it a 2D tensor.\n",
    "mask = torch.tensor(mask)[None] # torch.tensor([mask]) \n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "45f4c6c7-f957-41cd-b6c2-28651a722b59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.4952e+00, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.5920e+00, -1.0857e+00, -5.0981e+00, -2.9331e+00, -3.4070e+00,\n",
       "          2.2467e+00,  5.1563e+00, -1.3602e+00, -2.2209e+00, -9.6861e-01,\n",
       "         -4.8112e+00, -2.2527e+00,  1.4383e+00,  1.0121e+01, -1.5311e+00,\n",
       "          2.2685e+00, -1.8951e+00, -2.2108e+00, -4.2142e+00, -2.5571e+00,\n",
       "         -2.3252e+00, -2.6046e+00,  1.7047e+00, -1.9867e+00, -1.7211e+00,\n",
       "         -5.4148e-01, -2.0239e+00, -4.4246e+00, -5.1012e+00, -4.4966e+00,\n",
       "         -7.8940e+00, -6.7200e+00, -4.6759e+00, -6.3278e+00, -4.8339e+00,\n",
       "         -5.1839e+00, -3.3724e+00, -7.4120e+00, -8.1542e+00, -4.4871e+00,\n",
       "         -7.4659e+00, -4.3293e+00, -4.2293e+00, -3.1903e+00, -7.9467e+00,\n",
       "         -5.2665e+00, -7.5902e+00, -5.0570e+00, -7.4476e+00, -7.9083e+00,\n",
       "         -6.5951e+00, -7.4061e+00, -8.8821e+00, -7.6749e+00, -6.9879e+00,\n",
       "         -7.0466e+00, -1.0000e+04]], grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d477eb9b-92a5-408b-831e-dfc9df28c02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.4531e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1185e-06, 1.3470e-05,\n",
       "        2.4368e-07, 2.1236e-06, 1.3220e-06, 3.7722e-04, 6.9219e-03, 1.0237e-05,\n",
       "        4.3289e-06, 1.5143e-05, 3.2463e-07, 4.1933e-06, 1.6808e-04, 9.9179e-01,\n",
       "        8.6288e-06, 3.8557e-04, 5.9956e-06, 4.3725e-06, 5.8977e-07, 3.0929e-06,\n",
       "        3.8998e-06, 2.9493e-06, 2.1940e-04, 5.4713e-06, 7.1354e-06, 2.3212e-05,\n",
       "        5.2711e-06, 4.7788e-07, 2.4291e-07, 4.4467e-07, 1.4879e-08, 4.8133e-08,\n",
       "        3.7169e-07, 7.1242e-08, 3.1735e-07, 2.2365e-07, 1.3685e-06, 2.4093e-08,\n",
       "        1.1470e-08, 4.4891e-07, 2.2828e-08, 5.2562e-07, 5.8092e-07, 1.6419e-06,\n",
       "        1.4114e-08, 2.0591e-07, 2.0161e-08, 2.5390e-07, 2.3251e-08, 1.4667e-08,\n",
       "        5.4533e-08, 2.4235e-08, 5.5390e-09, 1.8524e-08, 3.6818e-08, 3.4721e-08,\n",
       "        0.0000e+00], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(start_logits[0], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dc7319f1-a57c-4ef0-b450-17c300ca82c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aaffa2fb-e268-4b7e-b99d-3d875054ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67])\n",
      "tensor([4.4531e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1185e-06, 1.3470e-05,\n",
      "        2.4368e-07, 2.1236e-06, 1.3220e-06, 3.7722e-04, 6.9219e-03, 1.0237e-05,\n",
      "        4.3289e-06, 1.5143e-05, 3.2463e-07, 4.1933e-06, 1.6808e-04, 9.9179e-01,\n",
      "        8.6288e-06, 3.8557e-04, 5.9956e-06, 4.3725e-06, 5.8977e-07, 3.0929e-06,\n",
      "        3.8998e-06, 2.9493e-06, 2.1940e-04, 5.4713e-06, 7.1354e-06, 2.3212e-05,\n",
      "        5.2711e-06, 4.7788e-07, 2.4291e-07, 4.4467e-07, 1.4879e-08, 4.8133e-08,\n",
      "        3.7169e-07, 7.1242e-08, 3.1735e-07, 2.2365e-07, 1.3685e-06, 2.4093e-08,\n",
      "        1.1470e-08, 4.4891e-07, 2.2828e-08, 5.2562e-07, 5.8092e-07, 1.6419e-06,\n",
      "        1.4114e-08, 2.0591e-07, 2.0161e-08, 2.5390e-07, 2.3251e-08, 1.4667e-08,\n",
      "        5.4533e-08, 2.4235e-08, 5.5390e-09, 1.8524e-08, 3.6818e-08, 3.4721e-08,\n",
      "        0.0000e+00], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(start_probabilities.shape)\n",
    "print(start_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "500b0265-6c9e-48c7-93ba-8de1040883a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "\n",
    "b = a.view(a.shape[0], -1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7e580ad4-e69a-4687-bf90-8671af433f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 1])\n",
      "torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "print(start_probabilities[:, None].shape)\n",
    "print(end_probabilities[None, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "128fcab9-c381-4cac-91cd-0f66ede23a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 1])\n",
      "torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "print(start_probabilities.view(start_probabilities.shape[0],-1).shape)\n",
    "print(end_probabilities.view(-1, end_probabilities.shape[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dcf3f150-aaae-4835-99a1-068ff776b9c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = start_probabilities[:, None] * end_probabilities[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cd5b171e-16b2-41e5-a8fc-ddd0cc471d87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([67, 67])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "77053198-3c8b-426b-b377-da16ef40fa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [0., 1., 1.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 3).triu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7e3ef6dc-9027-4f08-aa94-e97e1b8f7f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = torch.triu(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9a0a05bd-aad1-4695-b251-b46a86011650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([67, 67])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "54f54af7-ca96-4ce6-9bdc-f0d88c0f746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1576)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1576"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(scores.argmax())\n",
    "scores.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "41c86879-a452-42e6-af3a-54c0f500113f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([67, 67])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "749e3650-e542-4407-b877-1cb204212033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9803, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "print(scores[start_index, end_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1a73cf98-2479-4993-9fae-440e2bd9164e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "print(scores.argmax().item() // 67)\n",
    "print(scores.argmax().item() % 67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "673cb160-8b40-475a-9532-cf4d0e9c5178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9803, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[23,35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f10ebc1a-d448-4d7f-83ff-e613ffc992c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "44f3d9c1-2300-4b8b-9522-28ae8a3a2bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Jax, PyTorch, and TensorFlow', 'start': 78, 'end': 106, 'score': tensor(0.9803, grad_fn=<SelectBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index]\n",
    "}\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8cd8a91c-18a2-4968-9ff4-6ace0b41b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax\n",
      "##low\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Which',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'libraries',\n",
       " 'back',\n",
       " '[UNK]',\n",
       " 'Transformers',\n",
       " '?',\n",
       " '[SEP]',\n",
       " '[UNK]',\n",
       " 'Transformers',\n",
       " 'is',\n",
       " 'backed',\n",
       " 'by',\n",
       " 'the',\n",
       " 'three',\n",
       " 'most',\n",
       " 'popular',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'libraries',\n",
       " '—',\n",
       " 'Jax',\n",
       " ',',\n",
       " 'P',\n",
       " '##y',\n",
       " '##T',\n",
       " '##or',\n",
       " '##ch',\n",
       " ',',\n",
       " 'and',\n",
       " 'Ten',\n",
       " '##sor',\n",
       " '##F',\n",
       " '##low',\n",
       " '—',\n",
       " 'with',\n",
       " 'a',\n",
       " 'sea',\n",
       " '##m',\n",
       " '##less',\n",
       " 'integration',\n",
       " 'between',\n",
       " 'them',\n",
       " '.',\n",
       " 'It',\n",
       " \"'\",\n",
       " 's',\n",
       " 'straightforward',\n",
       " 'to',\n",
       " 'train',\n",
       " 'your',\n",
       " 'models',\n",
       " 'with',\n",
       " 'one',\n",
       " 'before',\n",
       " 'loading',\n",
       " 'them',\n",
       " 'for',\n",
       " 'in',\n",
       " '##ference',\n",
       " 'with',\n",
       " 'the',\n",
       " 'other',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs_with_offsets.tokens()[start_index]) #23\n",
    "print(inputs_with_offsets.tokens()[end_index]) #35\n",
    "inputs_with_offsets.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84888759-9482-440e-af3a-512b2a25a633",
   "metadata": {},
   "source": [
    "#### Try it out with top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "219578c5-0319-4b8f-be25-8f9bcc57914a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9802603125572205,\n",
       "  'start': 78,\n",
       "  'end': 106,\n",
       "  'answer': 'Jax, PyTorch, and TensorFlow'},\n",
       " {'score': 0.008247792720794678,\n",
       "  'start': 78,\n",
       "  'end': 108,\n",
       "  'answer': 'Jax, PyTorch, and TensorFlow —'},\n",
       " {'score': 0.0013677021488547325,\n",
       "  'start': 78,\n",
       "  'end': 90,\n",
       "  'answer': 'Jax, PyTorch'},\n",
       " {'score': 0.00038108628359623253,\n",
       "  'start': 83,\n",
       "  'end': 106,\n",
       "  'answer': 'PyTorch, and TensorFlow'},\n",
       " {'score': 0.000216845452087, 'start': 96, 'end': 106, 'answer': 'TensorFlow'}]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "question_answerer(question=question, context=context, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8ae25281-c103-41b4-9b48-66672b99c4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([9.8026e-01, 8.2478e-03, 6.8414e-03, 1.3677e-03, 3.8109e-04],\n",
       "       grad_fn=<TopkBackward0>),\n",
       "indices=tensor([1576, 1577, 1107, 1570, 1710]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(scores.flatten(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "58762095-40d1-4acf-8c34-d66225eab621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'answer': 'Jax, PyTorch, and TensorFlow',\n",
      "  'end': 106,\n",
      "  'score': 0.9802601933479309,\n",
      "  'start': 78},\n",
      " {'answer': 'Jax, PyTorch, and TensorFlow —',\n",
      "  'end': 108,\n",
      "  'score': 0.008247792720794678,\n",
      "  'start': 78},\n",
      " {'answer': 'three most popular deep learning libraries — Jax, PyTorch, and '\n",
      "            'TensorFlow',\n",
      "  'end': 106,\n",
      "  'score': 0.006841439288109541,\n",
      "  'start': 33},\n",
      " {'answer': 'Jax, PyTorch',\n",
      "  'end': 90,\n",
      "  'score': 0.0013677021488547325,\n",
      "  'start': 78},\n",
      " {'answer': 'PyTorch, and TensorFlow',\n",
      "  'end': 106,\n",
      "  'score': 0.0003810862544924021,\n",
      "  'start': 83}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "value_5, index_5 = torch.topk(scores.flatten(), 5)\n",
    "top5_indexes = [((i//scores.shape[1]).item(), (i%scores.shape[1]).item()) for i in index_5]\n",
    "\n",
    "results = []\n",
    "for s_i, e_i in top5_indexes:\n",
    "     start_char, _ = offsets[s_i]\n",
    "     _, end_char = offsets[e_i]\n",
    "     result = {\n",
    "         \"answer\": context[start_char:end_char],\n",
    "         \"start\": start_char,\n",
    "         \"end\": end_char,\n",
    "         \"score\": scores[s_i, e_i].item()\n",
    "     }\n",
    "     results.append(result)\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787af9a3-07d0-48ea-8369-3eef84f025b0",
   "metadata": {},
   "source": [
    "#### 길이가 긴 컨텍스트 다루기\n",
    "위에서 예제로 사용한 질문 및 길이가 긴 컨텍스트를 토큰화 해보면 question-answering 파이프라인에서 사용된 최대 길이(384)보다 더 많은 토큰들이 출력됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "be993665-f132-4fe9-b70e-fdccb8e2ad57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, long_context)\n",
    "print(len(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bc2a3305-7c50-4054-9177-5cd9ae5ae4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-cased-distilled-squad', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "944e8e5e-2738-4e48-9ada-cc6e15ab2f63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internal [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8bdca9f8-3362-466c-9b43-6a366ab63dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "058062bf-98ce-448e-b693-f095f6c6a547",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 1188, 5650, 1110, 1136, 1315, 1263, 102],\n",
      "               [101, 1315, 1263, 1133, 1195, 1132, 1280, 102],\n",
      "               [101, 1132, 1280, 1106, 3325, 1122, 4050, 102],\n",
      "               [101, 1122, 4050, 119, 102]],\n",
      " 'overflow_to_sample_mapping': [0, 0, 0, 0]}\n",
      "[CLS] This sentence is not too long [SEP]\n",
      "[CLS] too long but we are going [SEP]\n",
      "[CLS] are going to split it anyway [SEP]\n",
      "[CLS] it anyway. [SEP]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
    "inputs = tokenizer(\n",
    "    sentence, truncation=True, return_overflowing_tokens=True, max_length=8, stride=2\n",
    ")\n",
    "pprint(inputs)\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7bc9959c-cd00-414c-a898-84bb1f077cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f221edf6-5ce5-4d27-bb94-eb1971948977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "577937c0-f192-411c-b3d9-1d5f30742714",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "{'input_ids': [[101, 1188, 5650, 1110, 1136, 102], [101, 1110, 1136, 1315, 1263, 102], [101, 1315, 1263, 1133, 1195, 102], [101, 1133, 1195, 1132, 1280, 102], [101, 1132, 1280, 1106, 3325, 102], [101, 1106, 3325, 1122, 4050, 102], [101, 1122, 4050, 119, 102], [101, 1188, 5650, 1110, 7681, 102], [101, 1110, 7681, 1133, 1209, 102], [101, 1133, 1209, 1253, 1243, 102], [101, 1253, 1243, 3325, 119, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"This sentence is not too long but we are going to split it anyway.\",\n",
    "    \"This sentence is shorter but will still get split.\",\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")\n",
    "\n",
    "print(inputs[\"overflow_to_sample_mapping\"])\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bd63bdb6-d3a6-45ef-92a1-a5204e7e14ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 0, 0, 0, None]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.sequence_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "84c9c04f-a936-4a09-9ae7-5650c1906c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to [SEP]\n",
      "\n",
      "\n",
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch [SEP]\n",
      "\n",
      "\n",
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP]ined models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internals are exposed as consistently as possible. - Model files can be used independently of the library for quick experiments. [UNK] Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with [SEP]\n",
      "\n",
      "\n",
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] independently of the library for quick experiments. [UNK] Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=60,\n",
    "    max_length=200,\n",
    "    padding=\"longest\",\n",
    "    #truncation=True,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "efa23798-6173-460f-8212-37b79e814dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0]\n",
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to [SEP]\n",
      "\n",
      "\n",
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch [SEP]\n",
      "\n",
      "\n",
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP]ined models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internals are exposed as consistently as possible. - Model files can be used independently of the library for quick experiments. [UNK] Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with [SEP]\n",
      "\n",
      "\n",
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] independently of the library for quick experiments. [UNK] Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=60,\n",
    "    max_length=200,\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    #truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(inputs[\"overflow_to_sample_mapping\"])\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "22cca719-52c9-4db1-865b-911ffcd1696e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e81971f7-44ff-4c60-aa2e-3d157ec98774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs.sequence_ids())\n",
    "len(inputs.sequence_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "71d3b145-ba0a-4554-9f27-2357e503312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 5979, 1996, 3776, 9818, 1171, 100, 25267, 136, 102, 100, 25267, 131, 1426, 1104, 1103, 2051, 21239, 2101, 100, 25267, 2790, 4674, 1104, 3073, 4487, 9044, 3584, 1106, 3870, 8249, 1113, 6685, 1216, 1112, 5393, 117, 1869, 16026, 117, 2304, 10937, 117, 7584, 7317, 2734, 117, 5179, 117, 3087, 3964, 1105, 1167, 1107, 1166, 1620, 3483, 119, 2098, 6457, 1110, 1106, 1294, 5910, 118, 2652, 21239, 2101, 5477, 1106, 1329, 1111, 2490, 119, 100, 25267, 2790, 20480, 1116, 1106, 1976, 9133, 1105, 1329, 1343, 3073, 4487, 9044, 3584, 1113, 170, 1549, 3087, 117, 2503, 118, 9253, 1172, 1113, 1240, 1319, 2233, 27948, 1105, 1173, 2934, 1172, 1114, 1103, 1661, 1113, 1412, 2235, 10960, 119, 1335, 1103, 1269, 1159, 117, 1296, 185, 25669, 8613, 13196, 13682, 1126, 4220, 1110, 3106, 2484, 20717, 1673, 1105, 1169, 1129, 5847, 1106, 9396, 3613, 1844, 7857, 119, 2009, 1431, 146, 1329, 11303, 1468, 136, 122, 119, 12167, 118, 1106, 118, 1329, 1352, 118, 1104, 118, 1103, 118, 1893, 3584, 131, 118, 1693, 2099, 1113, 21239, 2591, 1105, 21239, 2349, 8249, 119, 118, 8274, 9391, 1106, 3990, 1111, 24937, 1105, 16681, 119, 118, 17751, 4795, 118, 4749, 11108, 5266, 1114, 1198, 1210, 3553, 1106, 3858, 119, 118, 138, 13943, 20480, 1111, 1606, 1155, 1412, 3073, 4487, 9044, 3584, 119, 118, 5738, 3254, 22662, 4692, 117, 2964, 6302, 2555, 10988, 131, 123, 119, 26982, 1169, 2934, 3972, 3584, 1939, 1104, 1579, 1231, 4487, 16534, 119, 118, 153, 19366, 3121, 2116, 1468, 1169, 4851, 3254, 22662, 1159, 1105, 1707, 4692, 119, 118, 2091, 10947, 1116, 1104, 4220, 1116, 1114, 1166, 1275, 117, 1288, 3073, 4487, 9044, 3584, 117, 1199, 1107, 1167, 1190, 1620, 3483, 119, 124, 119, 22964, 6787, 1103, 1268, 8297, 1111, 1451, 1226, 1104, 170, 2235, 112, 188, 7218, 131, 118, 9791, 1352, 118, 1104, 118, 1103, 118, 1893, 3584, 1107, 124, 2442, 1104, 3463, 119, 118, 15729, 170, 1423, 2235, 1206, 157, 2271, 1477, 119, 121, 120, 153, 1183, 1942, 1766, 1732, 8297, 1116, 1120, 1209, 119, 118, 3017, 1306, 8709, 3368, 1103, 1268, 8297, 1111, 2013, 117, 10540, 1105, 1707, 119, 125, 119, 142, 20158, 8156, 3708, 170, 2235, 1137, 1126, 1859, 1106, 1240, 2993, 131, 118, 1284, 2194, 5136, 1111, 1296, 4220, 1106, 23577, 1103, 2686, 1502, 1118, 1157, 1560, 5752, 119, 118, 6747, 4422, 102], [101, 5979, 1996, 3776, 9818, 1171, 100, 25267, 136, 102, 2091, 10947, 1116, 1104, 4220, 1116, 1114, 1166, 1275, 117, 1288, 3073, 4487, 9044, 3584, 117, 1199, 1107, 1167, 1190, 1620, 3483, 119, 124, 119, 22964, 6787, 1103, 1268, 8297, 1111, 1451, 1226, 1104, 170, 2235, 112, 188, 7218, 131, 118, 9791, 1352, 118, 1104, 118, 1103, 118, 1893, 3584, 1107, 124, 2442, 1104, 3463, 119, 118, 15729, 170, 1423, 2235, 1206, 157, 2271, 1477, 119, 121, 120, 153, 1183, 1942, 1766, 1732, 8297, 1116, 1120, 1209, 119, 118, 3017, 1306, 8709, 3368, 1103, 1268, 8297, 1111, 2013, 117, 10540, 1105, 1707, 119, 125, 119, 142, 20158, 8156, 3708, 170, 2235, 1137, 1126, 1859, 1106, 1240, 2993, 131, 118, 1284, 2194, 5136, 1111, 1296, 4220, 1106, 23577, 1103, 2686, 1502, 1118, 1157, 1560, 5752, 119, 118, 6747, 4422, 1116, 1132, 5490, 1112, 10887, 1112, 1936, 119, 118, 6747, 7004, 1169, 1129, 1215, 8942, 1104, 1103, 3340, 1111, 3613, 7857, 119, 100, 25267, 1110, 5534, 1118, 1103, 1210, 1211, 1927, 1996, 3776, 9818, 783, 13612, 117, 153, 1183, 1942, 1766, 1732, 1105, 5157, 21484, 2271, 6737, 783, 1114, 170, 2343, 1306, 2008, 9111, 1206, 1172, 119, 1135, 112, 188, 21546, 1106, 2669, 1240, 3584, 1114, 1141, 1196, 10745, 1172, 1111, 1107, 16792, 1114, 1103, 1168, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 5), (6, 10), (11, 19), (20, 29), (30, 34), (35, 36), (37, 49), (49, 50), (0, 0), (1, 2), (3, 15), (15, 16), (17, 22), (23, 25), (26, 29), (30, 33), (34, 36), (36, 37), (39, 40), (41, 53), (54, 62), (63, 72), (73, 75), (76, 79), (79, 82), (82, 86), (87, 93), (94, 96), (97, 104), (105, 110), (111, 113), (114, 119), (120, 124), (125, 127), (128, 142), (142, 143), (144, 155), (156, 166), (166, 167), (168, 176), (177, 186), (186, 187), (188, 191), (191, 194), (194, 201), (201, 202), (203, 214), (214, 215), (216, 220), (221, 231), (232, 235), (236, 240), (241, 243), (244, 248), (249, 252), (253, 262), (262, 263), (264, 267), (268, 271), (272, 274), (275, 277), (278, 282), (283, 290), (290, 291), (291, 295), (296, 298), (298, 299), (300, 306), (307, 309), (310, 313), (314, 317), (318, 326), (326, 327), (329, 330), (331, 343), (344, 352), (353, 356), (356, 357), (358, 360), (361, 368), (369, 377), (378, 381), (382, 385), (386, 391), (392, 395), (395, 398), (398, 402), (403, 409), (410, 412), (413, 414), (415, 420), (421, 425), (425, 426), (427, 431), (431, 432), (432, 436), (437, 441), (442, 444), (445, 449), (450, 453), (454, 458), (458, 462), (463, 466), (467, 471), (472, 477), (478, 482), (483, 487), (488, 491), (492, 501), (502, 504), (505, 508), (509, 514), (515, 518), (518, 519), (520, 522), (523, 526), (527, 531), (532, 536), (536, 537), (538, 542), (543, 544), (544, 546), (546, 549), (550, 556), (557, 565), (566, 568), (569, 581), (582, 584), (585, 590), (591, 596), (596, 599), (599, 601), (602, 605), (606, 609), (610, 612), (613, 621), (622, 624), (625, 631), (632, 637), (638, 646), (647, 658), (658, 659), (661, 664), (665, 671), (672, 673), (674, 677), (678, 687), (687, 690), (690, 691), (693, 694), (694, 695), (696, 700), (700, 701), (701, 703), (703, 704), (704, 707), (708, 713), (713, 714), (714, 716), (716, 717), (717, 720), (720, 721), (721, 724), (725, 731), (731, 732), (735, 736), (737, 741), (742, 753), (754, 756), (757, 759), (759, 760), (761, 764), (765, 767), (767, 768), (769, 774), (774, 775), (778, 779), (780, 783), (784, 791), (792, 794), (795, 800), (801, 804), (805, 814), (815, 818), (819, 832), (832, 833), (836, 837), (838, 841), (842, 846), (846, 847), (847, 853), (854, 862), (862, 866), (867, 871), (872, 876), (877, 882), (883, 890), (891, 893), (894, 899), (899, 900), (903, 904), (905, 906), (907, 914), (915, 918), (919, 922), (923, 928), (929, 932), (933, 936), (937, 940), (940, 943), (943, 947), (948, 954), (954, 955), (958, 959), (960, 965), (966, 969), (969, 973), (974, 979), (979, 980), (981, 988), (989, 995), (996, 1000), (1000, 1005), (1005, 1006), (1008, 1009), (1009, 1010), (1011, 1022), (1023, 1026), (1027, 1032), (1033, 1040), (1041, 1047), (1048, 1055), (1056, 1058), (1059, 1065), (1066, 1068), (1068, 1071), (1071, 1076), (1076, 1077), (1080, 1081), (1082, 1083), (1083, 1086), (1086, 1088), (1088, 1092), (1092, 1095), (1096, 1099), (1100, 1106), (1107, 1110), (1110, 1114), (1115, 1119), (1120, 1123), (1124, 1134), (1135, 1140), (1140, 1141), (1144, 1145), (1146, 1148), (1148, 1151), (1151, 1152), (1153, 1155), (1156, 1168), (1168, 1169), (1170, 1174), (1175, 1179), (1180, 1182), (1182, 1183), (1183, 1186), (1187, 1190), (1190, 1193), (1193, 1197), (1198, 1204), (1204, 1205), (1206, 1210), (1211, 1213), (1214, 1218), (1219, 1223), (1224, 1227), (1228, 1237), (1237, 1238), (1240, 1241), (1241, 1242), (1243, 1246), (1246, 1249), (1250, 1253), (1254, 1259), (1260, 1269), (1270, 1273), (1274, 1279), (1280, 1284), (1285, 1287), (1288, 1289), (1290, 1295), (1295, 1296), (1296, 1297), (1298, 1306), (1306, 1307), (1310, 1311), (1312, 1317), (1318, 1323), (1323, 1324), (1324, 1326), (1326, 1327), (1327, 1330), (1330, 1331), (1331, 1334), (1335, 1341), (1342, 1344), (1345, 1346), (1347, 1352), (1353, 1355), (1356, 1360), (1360, 1361), (1364, 1365), (1366, 1370), (1371, 1372), (1373, 1379), (1380, 1385), (1386, 1393), (1394, 1395), (1395, 1396), (1396, 1397), (1397, 1398), (1398, 1399), (1399, 1400), (1400, 1401), (1401, 1402), (1402, 1403), (1403, 1405), (1405, 1407), (1408, 1417), (1417, 1418), (1419, 1421), (1422, 1426), (1426, 1427), (1430, 1431), (1432, 1435), (1435, 1436), (1436, 1442), (1443, 1447), (1448, 1451), (1452, 1457), (1458, 1467), (1468, 1471), (1472, 1480), (1480, 1481), (1482, 1492), (1493, 1496), (1497, 1507), (1507, 1508), (1510, 1511), (1511, 1512), (1513, 1514), (1514, 1519), (1520, 1526), (1526, 1529), (1530, 1531), (1532, 1537), (1538, 1540), (1541, 1543), (1544, 1551), (1552, 1554), (1555, 1559), (1560, 1565), (1565, 1566), (1569, 1570), (1571, 1573), (1574, 1581), (1582, 1590), (1591, 1594), (1595, 1599), (1600, 1612), (1613, 1615), (1616, 1625), (1626, 1629), (1630, 1637), (1638, 1647), (1648, 1650), (1651, 1654), (1655, 1663), (1664, 1671), (1671, 1672), (1675, 1676), (1677, 1682), (1683, 1691), (0, 0)], [(0, 0), (0, 5), (6, 10), (11, 19), (20, 29), (30, 34), (35, 36), (37, 49), (49, 50), (0, 0), (1146, 1148), (1148, 1151), (1151, 1152), (1153, 1155), (1156, 1168), (1168, 1169), (1170, 1174), (1175, 1179), (1180, 1182), (1182, 1183), (1183, 1186), (1187, 1190), (1190, 1193), (1193, 1197), (1198, 1204), (1204, 1205), (1206, 1210), (1211, 1213), (1214, 1218), (1219, 1223), (1224, 1227), (1228, 1237), (1237, 1238), (1240, 1241), (1241, 1242), (1243, 1246), (1246, 1249), (1250, 1253), (1254, 1259), (1260, 1269), (1270, 1273), (1274, 1279), (1280, 1284), (1285, 1287), (1288, 1289), (1290, 1295), (1295, 1296), (1296, 1297), (1298, 1306), (1306, 1307), (1310, 1311), (1312, 1317), (1318, 1323), (1323, 1324), (1324, 1326), (1326, 1327), (1327, 1330), (1330, 1331), (1331, 1334), (1335, 1341), (1342, 1344), (1345, 1346), (1347, 1352), (1353, 1355), (1356, 1360), (1360, 1361), (1364, 1365), (1366, 1370), (1371, 1372), (1373, 1379), (1380, 1385), (1386, 1393), (1394, 1395), (1395, 1396), (1396, 1397), (1397, 1398), (1398, 1399), (1399, 1400), (1400, 1401), (1401, 1402), (1402, 1403), (1403, 1405), (1405, 1407), (1408, 1417), (1417, 1418), (1419, 1421), (1422, 1426), (1426, 1427), (1430, 1431), (1432, 1435), (1435, 1436), (1436, 1442), (1443, 1447), (1448, 1451), (1452, 1457), (1458, 1467), (1468, 1471), (1472, 1480), (1480, 1481), (1482, 1492), (1493, 1496), (1497, 1507), (1507, 1508), (1510, 1511), (1511, 1512), (1513, 1514), (1514, 1519), (1520, 1526), (1526, 1529), (1530, 1531), (1532, 1537), (1538, 1540), (1541, 1543), (1544, 1551), (1552, 1554), (1555, 1559), (1560, 1565), (1565, 1566), (1569, 1570), (1571, 1573), (1574, 1581), (1582, 1590), (1591, 1594), (1595, 1599), (1600, 1612), (1613, 1615), (1616, 1625), (1626, 1629), (1630, 1637), (1638, 1647), (1648, 1650), (1651, 1654), (1655, 1663), (1664, 1671), (1671, 1672), (1675, 1676), (1677, 1682), (1683, 1691), (1691, 1692), (1693, 1696), (1697, 1704), (1705, 1707), (1708, 1720), (1721, 1723), (1724, 1732), (1732, 1733), (1736, 1737), (1738, 1743), (1744, 1749), (1750, 1753), (1754, 1756), (1757, 1761), (1762, 1775), (1776, 1778), (1779, 1782), (1783, 1790), (1791, 1794), (1795, 1800), (1801, 1812), (1812, 1813), (1815, 1816), (1817, 1829), (1830, 1832), (1833, 1839), (1840, 1842), (1843, 1846), (1847, 1852), (1853, 1857), (1858, 1865), (1866, 1870), (1871, 1879), (1880, 1889), (1890, 1891), (1892, 1895), (1895, 1896), (1897, 1898), (1898, 1899), (1899, 1900), (1900, 1902), (1902, 1904), (1905, 1908), (1909, 1912), (1912, 1915), (1915, 1916), (1916, 1919), (1920, 1921), (1922, 1926), (1927, 1928), (1929, 1932), (1932, 1933), (1933, 1937), (1938, 1949), (1950, 1957), (1958, 1962), (1962, 1963), (1964, 1966), (1966, 1967), (1967, 1968), (1969, 1984), (1985, 1987), (1988, 1993), (1994, 1998), (1999, 2005), (2006, 2010), (2011, 2014), (2015, 2021), (2022, 2029), (2030, 2034), (2035, 2038), (2039, 2041), (2041, 2048), (2049, 2053), (2054, 2057), (2058, 2063), (2063, 2064), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0, 0]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6db2c02a-40bd-4be4-ae3b-dad403a2905e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "inputs = inputs.convert_to_tensors(\"pt\")\n",
    "print(inputs[\"input_ids\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b897f33a-ab80-4d4b-924d-45b44d632291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384]) torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5dd5cb41-b27d-4a09-8f80-d19908c9340f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5979,  1996,  3776,  9818,  1171,   100, 25267,   136,   102,\n",
       "           100, 25267,   131,  1426,  1104,  1103,  2051, 21239,  2101,   100,\n",
       "         25267,  2790,  4674,  1104,  3073,  4487,  9044,  3584,  1106,  3870,\n",
       "          8249,  1113,  6685,  1216,  1112,  5393,   117,  1869, 16026,   117,\n",
       "          2304, 10937,   117,  7584,  7317,  2734,   117,  5179,   117,  3087,\n",
       "          3964,  1105,  1167,  1107,  1166,  1620,  3483,   119,  2098,  6457,\n",
       "          1110,  1106,  1294,  5910,   118,  2652, 21239,  2101,  5477,  1106,\n",
       "          1329,  1111,  2490,   119,   100, 25267,  2790, 20480,  1116,  1106,\n",
       "          1976,  9133,  1105,  1329,  1343,  3073,  4487,  9044,  3584,  1113,\n",
       "           170,  1549,  3087,   117,  2503,   118,  9253,  1172,  1113,  1240,\n",
       "          1319,  2233, 27948,  1105,  1173,  2934,  1172,  1114,  1103,  1661,\n",
       "          1113,  1412,  2235, 10960,   119,  1335,  1103,  1269,  1159,   117,\n",
       "          1296,   185, 25669,  8613, 13196, 13682,  1126,  4220,  1110,  3106,\n",
       "          2484, 20717,  1673,  1105,  1169,  1129,  5847,  1106,  9396,  3613,\n",
       "          1844,  7857,   119,  2009,  1431,   146,  1329, 11303,  1468,   136,\n",
       "           122,   119, 12167,   118,  1106,   118,  1329,  1352,   118,  1104,\n",
       "           118,  1103,   118,  1893,  3584,   131,   118,  1693,  2099,  1113,\n",
       "         21239,  2591,  1105, 21239,  2349,  8249,   119,   118,  8274,  9391,\n",
       "          1106,  3990,  1111, 24937,  1105, 16681,   119,   118, 17751,  4795,\n",
       "           118,  4749, 11108,  5266,  1114,  1198,  1210,  3553,  1106,  3858,\n",
       "           119,   118,   138, 13943, 20480,  1111,  1606,  1155,  1412,  3073,\n",
       "          4487,  9044,  3584,   119,   118,  5738,  3254, 22662,  4692,   117,\n",
       "          2964,  6302,  2555, 10988,   131,   123,   119, 26982,  1169,  2934,\n",
       "          3972,  3584,  1939,  1104,  1579,  1231,  4487, 16534,   119,   118,\n",
       "           153, 19366,  3121,  2116,  1468,  1169,  4851,  3254, 22662,  1159,\n",
       "          1105,  1707,  4692,   119,   118,  2091, 10947,  1116,  1104,  4220,\n",
       "          1116,  1114,  1166,  1275,   117,  1288,  3073,  4487,  9044,  3584,\n",
       "           117,  1199,  1107,  1167,  1190,  1620,  3483,   119,   124,   119,\n",
       "         22964,  6787,  1103,  1268,  8297,  1111,  1451,  1226,  1104,   170,\n",
       "          2235,   112,   188,  7218,   131,   118,  9791,  1352,   118,  1104,\n",
       "           118,  1103,   118,  1893,  3584,  1107,   124,  2442,  1104,  3463,\n",
       "           119,   118, 15729,   170,  1423,  2235,  1206,   157,  2271,  1477,\n",
       "           119,   121,   120,   153,  1183,  1942,  1766,  1732,  8297,  1116,\n",
       "          1120,  1209,   119,   118,  3017,  1306,  8709,  3368,  1103,  1268,\n",
       "          8297,  1111,  2013,   117, 10540,  1105,  1707,   119,   125,   119,\n",
       "           142, 20158,  8156,  3708,   170,  2235,  1137,  1126,  1859,  1106,\n",
       "          1240,  2993,   131,   118,  1284,  2194,  5136,  1111,  1296,  4220,\n",
       "          1106, 23577,  1103,  2686,  1502,  1118,  1157,  1560,  5752,   119,\n",
       "           118,  6747,  4422,   102],\n",
       "        [  101,  5979,  1996,  3776,  9818,  1171,   100, 25267,   136,   102,\n",
       "          2091, 10947,  1116,  1104,  4220,  1116,  1114,  1166,  1275,   117,\n",
       "          1288,  3073,  4487,  9044,  3584,   117,  1199,  1107,  1167,  1190,\n",
       "          1620,  3483,   119,   124,   119, 22964,  6787,  1103,  1268,  8297,\n",
       "          1111,  1451,  1226,  1104,   170,  2235,   112,   188,  7218,   131,\n",
       "           118,  9791,  1352,   118,  1104,   118,  1103,   118,  1893,  3584,\n",
       "          1107,   124,  2442,  1104,  3463,   119,   118, 15729,   170,  1423,\n",
       "          2235,  1206,   157,  2271,  1477,   119,   121,   120,   153,  1183,\n",
       "          1942,  1766,  1732,  8297,  1116,  1120,  1209,   119,   118,  3017,\n",
       "          1306,  8709,  3368,  1103,  1268,  8297,  1111,  2013,   117, 10540,\n",
       "          1105,  1707,   119,   125,   119,   142, 20158,  8156,  3708,   170,\n",
       "          2235,  1137,  1126,  1859,  1106,  1240,  2993,   131,   118,  1284,\n",
       "          2194,  5136,  1111,  1296,  4220,  1106, 23577,  1103,  2686,  1502,\n",
       "          1118,  1157,  1560,  5752,   119,   118,  6747,  4422,  1116,  1132,\n",
       "          5490,  1112, 10887,  1112,  1936,   119,   118,  6747,  7004,  1169,\n",
       "          1129,  1215,  8942,  1104,  1103,  3340,  1111,  3613,  7857,   119,\n",
       "           100, 25267,  1110,  5534,  1118,  1103,  1210,  1211,  1927,  1996,\n",
       "          3776,  9818,   783, 13612,   117,   153,  1183,  1942,  1766,  1732,\n",
       "          1105,  5157, 21484,  2271,  6737,   783,  1114,   170,  2343,  1306,\n",
       "          2008,  9111,  1206,  1172,   119,  1135,   112,   188, 21546,  1106,\n",
       "          2669,  1240,  3584,  1114,  1141,  1196, 10745,  1172,  1111,  1107,\n",
       "         16792,  1114,  1103,  1168,   119,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a41b6744-c38b-48b1-9dd1-6c5eccc6fc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n",
      "384\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"input_ids\"].shape)\n",
    "print(len(inputs.sequence_ids()))\n",
    "print(inputs.sequence_ids()) # the sequence_ids is applied to every chuck, the first and the second chunks in this case  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "22c5f3dd-c10b-468f-a727-3faac93ace04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.sequence_ids(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "554ef561-c349-42f3-9348-369653cbd0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.sequence_ids(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "791f750e-b0d6-45dc-b7ce-f022dcc11995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384]) torch.Size([2, 384])\n",
      "torch.Size([2, 384])\n",
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False,  True],\n",
      "        [False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "# Mask all the [PAD] tokens\n",
    "print(torch.tensor(mask)[None].shape, inputs[\"attention_mask\"].shape)\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "print(mask.shape)\n",
    "print(mask)\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "98f90683-8da5-45c6-b1ef-c631707ef1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cfa3123e-d2e0-427c-bd83-359250d922da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n",
      "tensor([[6.0396e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1519e-02, 1.8898e-02,\n",
      "         2.3307e-03, 2.6660e-01, 3.3987e-04, 5.7259e-03, 8.8251e-03, 2.4441e-02,\n",
      "         1.1653e-03, 1.6499e-02, 4.2213e-03, 2.5967e-04, 5.1531e-04, 4.8240e-05,\n",
      "         1.0526e-03, 3.3808e-05, 4.8656e-05, 3.3979e-04, 7.6070e-05, 8.2786e-05,\n",
      "         2.0089e-04, 3.7564e-05, 3.8198e-04, 2.2247e-05, 1.9128e-05, 3.1091e-04,\n",
      "         2.6787e-05, 2.9143e-04, 5.4116e-05, 2.5045e-05, 2.0686e-04, 5.5922e-05,\n",
      "         3.0922e-05, 1.8686e-04, 3.0462e-05, 3.8663e-05, 3.2295e-05, 2.6850e-04,\n",
      "         3.2148e-05, 5.0713e-04, 7.8036e-05, 2.6229e-05, 2.1632e-04, 1.0072e-04,\n",
      "         2.8341e-04, 2.6911e-04, 1.7327e-04, 6.4009e-05, 4.9601e-04, 1.2098e-04,\n",
      "         4.9918e-05, 2.3866e-04, 2.2675e-04, 7.6235e-04, 5.3037e-05, 6.1205e-05,\n",
      "         2.1717e-03, 6.1977e-05, 8.0132e-05, 3.5199e-05, 3.8517e-05, 1.9568e-05,\n",
      "         3.2789e-05, 8.4875e-05, 1.1264e-03, 1.1658e-03, 9.1064e-05, 4.2216e-04,\n",
      "         2.4844e-05, 4.7227e-05, 1.0703e-04, 2.7814e-04, 9.0607e-06, 5.7355e-05,\n",
      "         3.1021e-05, 1.4257e-04, 1.6705e-05, 1.8516e-05, 3.1405e-05, 1.3833e-05,\n",
      "         4.3620e-05, 3.9557e-05, 3.6288e-05, 1.0668e-05, 2.3371e-04, 1.8859e-05,\n",
      "         4.0527e-05, 3.2564e-05, 1.7001e-05, 5.2079e-05, 5.3438e-05, 6.8894e-05,\n",
      "         9.4097e-06, 5.4008e-06, 5.1005e-05, 1.7664e-04, 3.1185e-05, 2.7753e-05,\n",
      "         7.6582e-05, 1.0711e-04, 2.4014e-05, 1.3228e-04, 2.2080e-04, 5.1977e-05,\n",
      "         3.1118e-05, 8.0990e-05, 2.6433e-05, 3.6156e-05, 2.9454e-05, 3.4986e-05,\n",
      "         1.6331e-04, 1.9274e-04, 2.1196e-05, 2.4516e-05, 4.6693e-05, 4.4601e-05,\n",
      "         3.1723e-05, 5.3226e-05, 2.1222e-05, 7.3337e-05, 9.1684e-05, 1.3768e-05,\n",
      "         1.9348e-05, 1.1236e-05, 4.2474e-05, 1.7870e-05, 4.7483e-05, 3.5773e-05,\n",
      "         4.6925e-05, 8.1235e-05, 7.9116e-05, 5.4204e-05, 2.5807e-05, 9.0443e-05,\n",
      "         2.8478e-05, 6.4565e-05, 3.6055e-05, 1.4395e-04, 1.5277e-05, 6.1496e-05,\n",
      "         1.4717e-04, 2.4373e-05, 3.9865e-04, 3.3044e-05, 4.8797e-05, 2.5091e-05,\n",
      "         5.5642e-05, 2.8450e-04, 2.8919e-05, 3.4556e-05, 2.2984e-05, 3.6495e-05,\n",
      "         1.6050e-05, 4.6959e-05, 5.8836e-05, 2.1062e-05, 6.8295e-05, 1.7455e-04,\n",
      "         4.2067e-05, 2.5771e-05, 9.0461e-04, 3.4682e-05, 2.2831e-05, 7.0017e-04,\n",
      "         2.5974e-05, 3.3501e-05, 2.3248e-05, 6.1960e-05, 2.0418e-04, 7.1498e-05,\n",
      "         2.8672e-05, 6.8584e-05, 1.5240e-05, 2.9584e-03, 1.9563e-05, 8.2177e-05,\n",
      "         3.0757e-05, 8.0593e-05, 9.2864e-05, 1.1457e-04, 1.3182e-05, 2.9157e-05,\n",
      "         4.4604e-05, 5.8819e-06, 1.6354e-05, 2.6683e-05, 4.4594e-05, 1.7754e-05,\n",
      "         1.2513e-05, 1.7692e-05, 1.2350e-05, 3.8307e-05, 5.4325e-05, 8.5396e-05,\n",
      "         2.5757e-05, 1.1572e-05, 3.0818e-05, 2.6117e-05, 2.2340e-05, 8.0805e-05,\n",
      "         1.6667e-05, 1.3175e-05, 1.8923e-05, 1.1987e-05, 4.5682e-05, 8.2841e-05,\n",
      "         5.7296e-05, 2.6128e-05, 4.1435e-05, 1.3660e-05, 7.4009e-05, 6.9716e-05,\n",
      "         3.5102e-05, 2.9658e-05, 2.5441e-05, 4.9155e-05, 1.0304e-05, 2.7758e-04,\n",
      "         1.5747e-05, 3.6439e-05, 5.2869e-05, 2.6449e-05, 2.0917e-05, 2.1051e-05,\n",
      "         2.8080e-05, 6.2091e-05, 1.5358e-05, 1.2988e-05, 1.7279e-05, 7.9639e-05,\n",
      "         1.7420e-04, 2.1415e-05, 2.3101e-05, 2.7047e-05, 1.5998e-05, 2.5636e-05,\n",
      "         3.8288e-05, 7.8743e-05, 2.3411e-05, 3.8740e-05, 1.8684e-05, 5.8870e-05,\n",
      "         2.8242e-05, 3.7386e-05, 9.4584e-05, 3.3563e-04, 4.2749e-05, 3.2027e-05,\n",
      "         1.8438e-05, 1.5639e-04, 9.2072e-06, 2.6871e-05, 5.0628e-05, 6.6634e-05,\n",
      "         1.7043e-05, 1.8963e-05, 1.0072e-04, 1.7822e-05, 1.5416e-05, 3.5239e-05,\n",
      "         1.5112e-05, 8.0784e-05, 2.8089e-05, 8.6342e-05, 2.1300e-05, 5.9262e-05,\n",
      "         7.2382e-05, 2.3301e-05, 1.4347e-04, 9.3701e-06, 1.4267e-04, 3.0287e-05,\n",
      "         5.5216e-05, 8.5921e-05, 5.5442e-05, 3.4037e-05, 4.3847e-05, 2.7366e-05,\n",
      "         2.6062e-05, 2.8268e-05, 3.3615e-05, 1.9389e-05, 1.7744e-05, 6.4289e-05,\n",
      "         3.7703e-05, 8.7453e-05, 1.2262e-04, 7.8571e-05, 1.8368e-05, 2.9913e-05,\n",
      "         1.3881e-05, 2.9113e-05, 1.3188e-05, 2.8436e-05, 3.6310e-05, 2.5213e-05,\n",
      "         5.4479e-05, 2.3161e-05, 2.2188e-05, 5.3402e-05, 3.3268e-05, 9.5148e-05,\n",
      "         1.5824e-04, 4.0980e-05, 6.4439e-05, 4.2738e-05, 4.3797e-05, 3.6252e-04,\n",
      "         2.9179e-05, 2.9772e-05, 2.4612e-05, 5.0071e-05, 1.9387e-05, 2.4742e-04,\n",
      "         5.0116e-05, 3.6276e-05, 1.6146e-05, 1.4490e-05, 5.9163e-05, 9.6002e-06,\n",
      "         3.0809e-05, 3.4815e-05, 2.1627e-05, 7.6142e-05, 1.4263e-04, 2.4654e-05,\n",
      "         3.9240e-05, 1.2094e-04, 5.3953e-05, 8.1789e-05, 3.9092e-05, 2.6804e-05,\n",
      "         2.2500e-04, 1.7146e-05, 1.4356e-04, 2.0048e-05, 4.8189e-05, 2.8893e-05,\n",
      "         1.5475e-04, 1.2219e-05, 1.6598e-04, 4.1446e-05, 1.9321e-04, 3.1978e-05,\n",
      "         3.7745e-05, 6.2245e-05, 1.3202e-05, 4.2301e-05, 4.2694e-05, 4.4277e-05,\n",
      "         4.8065e-05, 4.4595e-05, 2.4264e-05, 7.9127e-05, 1.4261e-04, 5.8166e-05,\n",
      "         8.0192e-05, 2.8839e-05, 5.9347e-05, 5.9969e-05, 5.1162e-05, 6.3475e-05,\n",
      "         4.5755e-05, 6.8251e-05, 4.7683e-05, 2.7384e-05, 4.3412e-05, 6.5305e-05,\n",
      "         4.7019e-05, 2.1791e-05, 1.2243e-04, 8.8607e-05, 9.0091e-05, 0.0000e+00],\n",
      "        [1.1368e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2314e-07, 5.9025e-08,\n",
      "         6.2459e-08, 2.3798e-08, 2.2705e-07, 3.0308e-08, 1.8969e-08, 2.1334e-07,\n",
      "         4.0238e-07, 2.4029e-08, 7.1910e-08, 6.4756e-08, 8.9087e-09, 1.3505e-08,\n",
      "         5.6688e-08, 1.3920e-08, 6.3992e-08, 1.5222e-08, 1.2033e-07, 1.2319e-08,\n",
      "         1.9396e-07, 4.8797e-08, 2.0268e-08, 4.0331e-07, 1.4179e-08, 1.1957e-07,\n",
      "         1.3498e-08, 3.1293e-08, 9.4422e-08, 4.4795e-08, 1.3279e-08, 7.7697e-08,\n",
      "         3.0483e-08, 1.0293e-08, 3.6512e-08, 5.6327e-08, 9.5182e-09, 1.2713e-08,\n",
      "         5.6483e-08, 1.9430e-08, 6.3572e-08, 1.0990e-07, 4.3279e-08, 9.5519e-09,\n",
      "         7.5802e-09, 8.8012e-09, 1.2954e-08, 7.8799e-09, 1.8946e-08, 3.2146e-08,\n",
      "         1.1211e-08, 2.2150e-07, 1.6858e-08, 6.0710e-09, 2.6907e-08, 2.2479e-08,\n",
      "         3.9411e-08, 4.5914e-08, 1.6407e-08, 3.0637e-08, 1.7782e-08, 2.0490e-08,\n",
      "         4.6370e-07, 3.9837e-08, 5.4038e-08, 1.6316e-08, 5.9424e-08, 6.4794e-08,\n",
      "         3.3809e-06, 2.5210e-07, 1.5108e-07, 3.5514e-08, 2.0449e-07, 9.1037e-08,\n",
      "         1.2905e-08, 7.7222e-09, 2.8369e-08, 1.2272e-08, 5.5504e-08, 7.0431e-08,\n",
      "         9.3695e-09, 1.6006e-08, 2.2205e-08, 1.6542e-08, 6.6328e-08, 3.9618e-08,\n",
      "         5.6273e-09, 1.1277e-07, 7.6991e-09, 6.0051e-08, 6.9510e-09, 3.0206e-08,\n",
      "         1.3110e-08, 2.5250e-07, 1.1995e-08, 9.7927e-08, 1.3093e-08, 1.7087e-07,\n",
      "         1.0297e-08, 2.4735e-08, 6.2069e-08, 4.7821e-09, 1.6018e-08, 2.2945e-08,\n",
      "         5.0551e-09, 1.8481e-08, 3.2550e-08, 1.3735e-08, 6.3496e-08, 5.8157e-08,\n",
      "         2.3714e-08, 2.7957e-08, 8.0293e-09, 5.9257e-08, 4.8374e-08, 1.2007e-08,\n",
      "         3.5277e-08, 1.6356e-08, 2.8144e-08, 2.0316e-08, 9.4891e-09, 3.8508e-08,\n",
      "         5.0658e-08, 4.3834e-08, 2.9351e-08, 1.4243e-07, 5.1797e-07, 5.7161e-08,\n",
      "         1.5481e-08, 7.2372e-09, 3.2087e-08, 1.5419e-08, 1.9990e-08, 9.9486e-09,\n",
      "         2.8079e-08, 6.0376e-08, 3.8895e-07, 1.2782e-06, 1.0835e-07, 2.9641e-08,\n",
      "         9.2210e-09, 2.9912e-08, 1.7717e-07, 1.4182e-08, 7.7738e-08, 1.0693e-07,\n",
      "         2.5558e-08, 1.4011e-07, 8.3328e-08, 1.4974e-06, 1.8472e-04, 3.9754e-05,\n",
      "         3.9864e-06, 2.1657e-05, 4.6537e-06, 4.3566e-04, 7.3081e-03, 3.6535e-05,\n",
      "         1.6967e-05, 1.5730e-05, 4.3978e-07, 5.8429e-06, 4.4758e-04, 9.9120e-01,\n",
      "         7.0922e-06, 1.4898e-04, 8.5743e-06, 8.5240e-06, 1.8337e-06, 3.5312e-06,\n",
      "         4.2669e-06, 4.5402e-05, 5.4259e-06, 3.6447e-06, 9.6407e-06, 2.7798e-06,\n",
      "         7.6299e-07, 5.5816e-07, 1.4856e-06, 4.0436e-08, 7.5785e-08, 8.0974e-07,\n",
      "         6.6366e-08, 3.1352e-07, 1.6462e-07, 2.3133e-06, 5.3338e-08, 3.1590e-08,\n",
      "         4.9209e-07, 4.1981e-08, 3.9056e-07, 1.5715e-07, 2.8285e-07, 1.9840e-08,\n",
      "         1.5615e-07, 3.9218e-08, 2.6883e-07, 4.2738e-08, 1.8187e-08, 5.4750e-08,\n",
      "         4.1528e-08, 1.0004e-08, 2.4393e-08, 6.5669e-08, 7.0237e-08, 7.2691e-08,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)\n",
    "print(start_probabilities.shape)\n",
    "print(start_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2cc43c09-6e8f-497f-831a-93687b51f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6040], grad_fn=<TopkBackward0>) tensor([0])\n",
      "tensor([0.5608], grad_fn=<TopkBackward0>) tensor([18])\n",
      "tensor([0.3387], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "v0, i = torch.topk(start_probabilities[0], 1)\n",
    "print(v0, i)\n",
    "\n",
    "import torch\n",
    "v, j = torch.topk(end_probabilities[0], 1)\n",
    "print(v, j)\n",
    "print(v0*v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cf957278-3ab9-4c4c-8ff4-d27b63bdf628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤗 Transformers: State of the Art NLP\n"
     ]
    }
   ],
   "source": [
    "start_index, _ = offsets[0][i.item()]\n",
    "_, end_index = offsets[0][j.item()]\n",
    "print(long_context[start_index:end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3f5b761e-905f-4ef8-b0c2-72a2ac736841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9912], grad_fn=<TopkBackward0>) tensor([173])\n",
      "tensor([0.9801], grad_fn=<TopkBackward0>) tensor([184])\n",
      "tensor([0.9715], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "v0, i = torch.topk(start_probabilities[1], 1)\n",
    "print(v0, i)\n",
    "\n",
    "import torch\n",
    "v, j = torch.topk(end_probabilities[1], 1)\n",
    "print(v, j)\n",
    "print(v0*v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7c94d9cf-c52a-47e5-ba60-9488b4e42327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax, PyTorch and TensorFlow\n"
     ]
    }
   ],
   "source": [
    "start_index, _ = offsets[1][i.item()]\n",
    "_, end_index = offsets[1][j.item()]\n",
    "print(long_context[start_index:end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5214ea68-a81c-4a6c-96b0-14346a8fa20b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 18, 0.33867067098617554), (173, 184, 0.9714868664741516)]\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx // scores.shape[0]\n",
    "    end_idx = idx % scores.shape[0]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f6c8742f-ec67-4303-866e-7ffaf92ade1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '\\n🤗 Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867067098617554}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714868664741516}\n"
     ]
    }
   ],
   "source": [
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\":start_char, \"end\":end_char, \"score\":score}\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7941ab-cdbf-4658-b3f7-d0c4a161e452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
