{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4daf9166-b2b7-4416-a08a-9ace06802b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'token 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'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GITHUB_TOKEN = \"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\"   ## 본인의 personal access token을 지정하세요.\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b26c70b-a647-4102-b5b0-b32ef0b9961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-897594128',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
       "  'id': 897594128,\n",
       "  'node_id': 'IC_kwDODunzps41gDMQ',\n",
       "  'user': {'login': 'bhavitvyamalik',\n",
       "   'id': 19718818,\n",
       "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
       "   'html_url': 'https://github.com/bhavitvyamalik',\n",
       "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
       "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
       "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2021-08-12T12:21:52Z',\n",
       "  'updated_at': '2021-08-12T12:31:17Z',\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'body': \"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-898644889',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
       "  'id': 898644889,\n",
       "  'node_id': 'IC_kwDODunzps41kDuZ',\n",
       "  'user': {'login': 'bhavitvyamalik',\n",
       "   'id': 19718818,\n",
       "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
       "   'html_url': 'https://github.com/bhavitvyamalik',\n",
       "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
       "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
       "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2021-08-13T18:28:27Z',\n",
       "  'updated_at': '2021-08-13T18:28:27Z',\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'body': 'Thanks for the help, @albertvillanova! All tests are passing now.',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "issue_number = 2792\n",
    "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "response = requests.get(url, headers=headers)\n",
    "#response = requests.get(url)\n",
    "response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a54b6250-188b-41d7-be57-55872ff743fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
       " 'Thanks for the help, @albertvillanova! All tests are passing now.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_comments(issue_number):\n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return[r[\"body\"] for r in response.json()]\n",
    "\n",
    "get_comments(2792)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb92517b-a7c5-418a-818c-39a252adc441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason'],\n",
      "    num_rows: 7076\n",
      "})\n",
      ">> URL: https://github.com/huggingface/datasets/pull/145\n",
      ">> Pull request: {'diff_url': 'https://github.com/huggingface/datasets/pull/145.diff', 'html_url': 'https://github.com/huggingface/datasets/pull/145', 'merged_at': '2020-05-16T13:54:22Z', 'patch_url': 'https://github.com/huggingface/datasets/pull/145.patch', 'url': 'https://api.github.com/repos/huggingface/datasets/pulls/145'}\n",
      "\n",
      ">> Comments: 0\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/7072\n",
      ">> Pull request: None\n",
      "\n",
      ">> Comments: 0\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/1843\n",
      ">> Pull request: None\n",
      "\n",
      ">> Comments: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json(\"datasets-issues.jsonl\", lines=True)\n",
    "#print(df.head())\n",
    "\n",
    "from datasets import Dataset\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "print(issues_dataset)\n",
    "\n",
    "sample = issues_dataset.shuffle(seed=66).select(range(3))\n",
    "\n",
    "# Print out the URL and pull request entries\n",
    "for url, pr, comments in zip(sample[\"html_url\"], sample[\"pull_request\"], sample[\"comments\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Pull request: {pr}\\n\")\n",
    "    print(f\">> Comments: {comments}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1bbce1e-835a-4e4d-946c-6504ce796cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason'],\n",
       "    num_rows: 7076\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2413ee2-0cc3-45e4-a212-69f5e3078179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hi @patrickvonplaten  I would like to work on this dataset. \\r\\n\\r\\nThanks! ',\n",
       " \"That's awesome! Actually, I just noticed that this dataset might become a bit too big!\\r\\n\\r\\nMuST-C is the main dataset used for IWSLT19 and should probably be added as a standalone dataset. Would you be interested also in adding `datasets/MuST-C` instead?\\r\\n\\r\\nDescription: \\r\\n_MuST-C is a multilingual speech translation corpus whose size and quality facilitates the training of end-to-end systems for speech translation from English into several languages. For each target language, MuST-C comprises several hundred hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations._\\r\\n\\r\\nPaper: https://www.aclweb.org/anthology/N19-1202.pdf\\r\\n\\r\\nDataset: https://ict.fbk.eu/must-c/ (One needs to fill out a short from to download the data, but it's very easy).\\r\\n\\r\\nIt would be awesome if you're interested in adding this datates. I'm very happy to guide you through the PR! I think the easiest way to start would probably be to read [this README on how to add a dataset](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md) and open a PR. Think you can copy & paste some code from:\\r\\n\\r\\n- Librispeech_asr: https://github.com/huggingface/datasets/blob/master/datasets/librispeech_asr/librispeech_asr.py\\r\\n- Flores Translation: https://github.com/huggingface/datasets/blob/master/datasets/flores/flores.py\\r\\n\\r\\nThink all the rest can be handled on the PR :-) \",\n",
       " 'Hi @patrickvonplaten \\r\\nI have tried downloading this dataset, but the connection seems to reset all the time. I have tried it via the browser, wget, and using gdown . But it gives me an error message. _\"The server is busy or down, pls try again\"_ (rephrasing the message here)\\r\\n\\r\\nI have completed adding 4 datasets in the previous data sprint (including the IWSLT dataset #1676 ) ...so just checking if you are able to download it at your end. Otherwise will write to the dataset authors to update the links. \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n',\n",
       " 'Let me check tomorrow! Thanks for leaving this message!',\n",
       " 'cc @patil-suraj for notification ',\n",
       " \"@skyprince999, I think I'm getting the same error you're getting :-/\\r\\n\\r\\n```\\r\\nSorry, you can't view or download this file at this time.\\r\\n\\r\\nToo many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can't access a file after 24 hours, contact your domain administrator.\\r\\n```\\r\\n\\r\\nIt would be great if you could write the authors to see whether they can fix it.\\r\\nAlso cc @lhoestq - do you think we could mirror the dataset? \",\n",
       " \"Also there are huge those datasets. Think downloading MuST-C v1.2 amounts to ~ 1000GB... because there are 14 possible configs each around 60-70GB. I think users mostly will only use one of the 14 configs so that they would only need, in theory, will have to download ~60GB which is ok. But I think this functionality doesn't exist yet in `datasets` no? cc @lhoestq \",\n",
       " \"> Also cc @lhoestq - do you think we could mirror the dataset?\\r\\n\\r\\nYes we can mirror it if the authors are fine with it. You can create a dataset repo on huggingface.co (possibly under the relevant org) and add the mirrored data files.\\r\\n\\r\\n> I think users mostly will only use one of the 14 configs so that they would only need, in theory, will have to download ~60GB which is ok. But I think this functionality doesn't exist yet in datasets no? cc @lhoestq\\r\\n\\r\\nIf there are different download links for each configuration we can make the dataset builder download only the files related to the requested configuration.\",\n",
       " \"I have written to the dataset authors, highlighting this issue. Waiting for their response. \\r\\n\\r\\nUpdate on 25th Feb: \\r\\nThe authors have replied back, they are updating the download link and will revert back shortly! \\r\\n\\r\\n```\\r\\nfirst of all thanks a lot for being interested in MuST-C and for building the data-loader.\\r\\n\\r\\nBefore answering your request, I'd like to clarify that the creation, maintenance, and expansion of MuST-c are not supported by any funded project, so this means that we need to find economic support for all these activities. This also includes permanently moving all the data to AWS or GCP.  We are working at this with the goal of facilitating the use of MuST-C, but this is not something that can happen today. We hope to have some news ASAP and you will be among the first to be informed.\\r\\n\\r\\nI hope you understand our situation.\\r\\n```\\r\\n\\r\\n\",\n",
       " \"Awesome, actually @lhoestq let's just ask the authors if we should host the dataset no? They could just use our links then as well for their website - what do you think? Is it fine to use our AWS dataset storage also as external links? \",\n",
       " 'Yes definitely. Shall we suggest them to create a dataset repository under their org on huggingface.co ? @julien-c \\r\\nThe dataset is around 1TB',\n",
       " 'Sounds good! \\r\\n\\r\\nOrder of magnitude is storage costs ~$20 per TB per month (not including bandwidth). \\r\\n\\r\\nHappy to provide this to the community as I feel this is an important dataset. Let us know what the authors want to do!\\r\\n\\r\\n',\n",
       " 'Great! @skyprince999, do you think you could ping the authors here or link to this thread? I think it could be a cool idea to host the dataset on our side then',\n",
       " 'Done. They replied back, and they want to have a call over a meet/ skype. Is that possible ? \\r\\nBtw @patrickvonplaten you are looped in that email (_pls check you gmail account_)  ',\n",
       " 'Hello! Any news on this?',\n",
       " \"@gegallego  there were some concerns regarding dataset usage & attribution by a for-profit company, so couldn't take it forward. Also the download links were unstable. \\r\\nBut I guess if you want to test the fairseq benchmarks, you can connect with them directly for downloading the dataset.  \",\n",
       " 'Yes, that dataset is not easy to download... I had to copy it to my Google Drive and use `rsync` to be able to download it.\\r\\nHowever, we could add the dataset with a manual download, right?',\n",
       " \"yes that is possible. I couldn't unfortunately complete this PR, If you would like to add it, please feel free to do it. \"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_comments(1843)\n",
    "print(len(get_comments(1843)))\n",
    "get_comments(1843)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62276819-18c0-4975-b803-cd0ae893b726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
