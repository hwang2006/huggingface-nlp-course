{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c044816-c20e-425d-a6cc-789c4b5b56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GPU_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def get_dataloaders(accelerator: Accelerator, batch_size: int = 16):\n",
    "    \"\"\"\n",
    "    Creates a set of `DataLoader`s for the `glue` dataset,\n",
    "    using \"bert-base-cased\" as the tokenizer.\n",
    "\n",
    "    Args:\n",
    "        accelerator (`Accelerator`):\n",
    "            An `Accelerator` object\n",
    "        batch_size (`int`, *optional*):\n",
    "            The batch size for the train and validation DataLoaders.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # max_length=None => use the model max length (it's actually the default)\n",
    "        outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], \n",
    "                            truncation=True, max_length=None)\n",
    "        return outputs\n",
    "\n",
    "    # Apply the method we just defined to all the examples in all the splits of the dataset\n",
    "    # starting with the main process first:\n",
    "    with accelerator.main_process_first():\n",
    "        tokenized_datasets = datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n",
    "        )\n",
    "\n",
    "    # We also rename the 'label' column to 'labels' which is the expected name for labels \n",
    "    # by the models of the transformers library\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        # On TPU it's best to pad everything to the same length or training will be very slow.\n",
    "        max_length = 128 if accelerator.distributed_type == DistributedType.TPU else None\n",
    "        # When using mixed precision we want round multiples of 8/16\n",
    "        if accelerator.mixed_precision == \"fp8\":\n",
    "            pad_to_multiple_of = 16\n",
    "        elif accelerator.mixed_precision != \"no\":\n",
    "            pad_to_multiple_of = 8\n",
    "        else:\n",
    "            pad_to_multiple_of = None\n",
    "            \n",
    "        #print(\"examples:\\n\", examples)    \n",
    "\n",
    "        # tokenizer.pad() is a method used to pad sequences of tokens to a specified length.\n",
    "        tokens = tokenizer.pad(\n",
    "            examples,\n",
    "            padding=\"longest\",\n",
    "            max_length=max_length,\n",
    "            #pad_to_multiple_of = 16,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_tensors=\"pt\")\n",
    "        \n",
    "        ## print each token's shape \n",
    "        #for i, token in enumerate(tokens[\"input_ids\"]):\n",
    "        #    print(i, token.shape)\n",
    "        \n",
    "        return tokens\n",
    "        \n",
    "        #return tokenizer.pad(\n",
    "        #    examples,\n",
    "        #    padding=\"longest\",\n",
    "        #    max_length=max_length,\n",
    "        #    pad_to_multiple_of=pad_to_multiple_of,\n",
    "        #    return_tensors=\"pt\",\n",
    "        #)\n",
    "\n",
    "    # Instantiate dataloaders.\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"], # each tokenized dataset element may have different length.\n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,  # each element in a batch w/ batch_size to be paded with the specified length \n",
    "        batch_size=batch_size, \n",
    "        drop_last=True\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=EVAL_BATCH_SIZE,\n",
    "        drop_last=(accelerator.mixed_precision == \"fp8\"),\n",
    "    )\n",
    "\n",
    "    return train_dataloader, eval_dataloader\n",
    "\n",
    "\n",
    "#def training_function(config, args):\n",
    "def training_function():\n",
    "    # Initialize accelerator\n",
    "    #accelerator = Accelerator(cpu=args.cpu, mixed_precision=args.mixed_precision)\n",
    "    accelerator = Accelerator()\n",
    "    # Sample hyper-parameters for learning rate, batch size, seed and a few other HPs\n",
    "    lr = config[\"lr\"]\n",
    "    num_epochs = int(config[\"num_epochs\"])\n",
    "    seed = int(config[\"seed\"])\n",
    "    batch_size = int(config[\"batch_size\"])\n",
    "\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "    # If the batch size is too big we use gradient accumulation\n",
    "    gradient_accumulation_steps = 1\n",
    "    if batch_size > MAX_GPU_BATCH_SIZE and accelerator.distributed_type != DistributedType.TPU:\n",
    "        gradient_accumulation_steps = batch_size // MAX_GPU_BATCH_SIZE\n",
    "        batch_size = MAX_GPU_BATCH_SIZE\n",
    "\n",
    "    set_seed(seed)\n",
    "    train_dataloader, eval_dataloader = get_dataloaders(accelerator, batch_size)\n",
    "    # Instantiate the model (we build the model here so that the seed also control new weights initialization)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", return_dict=True)\n",
    "\n",
    "    # We could avoid this line since the accelerator is set with `device_placement=True` (default value).\n",
    "    # Note that if you are placing tensors on devices manually, this line absolutely needs to be before the optimizer\n",
    "    # creation otherwise training will not work on TPU (`accelerate` will kindly throw an error to make us aware of that).\n",
    "    model = model.to(accelerator.device)\n",
    "    # Instantiate optimizer\n",
    "    optimizer = AdamW(params=model.parameters(), lr=lr)\n",
    "\n",
    "    # Instantiate scheduler\n",
    "    num_training_steps= (len(train_dataloader) * num_epochs) // gradient_accumulation_steps\n",
    "    #print(\"num_training_steps: \", num_training_steps)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        #num_training_steps=(len(train_dataloader) * num_epochs) // gradient_accumulation_steps,\n",
    "        num_training_steps = num_training_steps,\n",
    "    )\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
    "    # prepare method.\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )\n",
    "     \n",
    "    num_training_steps = num_epochs * len(train_dataloader)  // gradient_accumulation_steps\n",
    "    #print(\"after accelerator prepared, num_training_steps: \", num_training_steps)\n",
    "    #print(\"length of train_dataloader: \", len(train_dataloader)) \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    \n",
    "    # Now we train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
    "            #batch.to(accelerator.device)\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            #if step % gradient_accumulation_steps == 0:\n",
    "            if (step+1) % gradient_accumulation_steps == 0:\n",
    "                #print(step+1)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
    "            #batch.to(accelerator.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            references = batch[\"labels\"]\n",
    "            #predictions, references = accelerator.gather_for_metrics((predictions, references))\n",
    "            metric.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "            )\n",
    "\n",
    "        eval_metric = metric.compute()\n",
    "        # Use accelerator.print to print only on the main process.\n",
    "        #print(f\"epoch {epoch}:\", eval_metric)\n",
    "        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n",
    "\n",
    "#config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 16}\n",
    "config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 8}\n",
    "#training_function(config, args)\n",
    "\n",
    "#notebook_launcher(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=1)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "dp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
