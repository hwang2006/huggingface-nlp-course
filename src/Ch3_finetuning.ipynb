{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc273cc-d8fe-406b-9153-7e862c0a6486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 1])}\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 12, None]\n",
      "[None, 0, 1, 2, 3, 4, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "## 감성분석 예제, 2개의 문장, 2개의 레이블\n",
    "\n",
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 2장의 예제와 동일합니다.\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# 감성 분석을 위한 문장 2개 입력\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# 새롭게 추가된 코드입니다. positive label 2개 추가\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "#print(tokenizer)\n",
    "print(batch)\n",
    "print(batch.word_ids(0))\n",
    "print(batch.word_ids(1))\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d776cda7-50dd-473e-87ff-42fde8b07179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 0]), 'input_ids': tensor([[ 101, 2002, 3005,  102],\n",
      "        [ 101, 1045, 2046,  102]]), 'attention_mask': tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]])}\n",
      "DefaultDataCollator(return_tensors=[{'input_ids': [101, 2002, 3005, 102], 'label': 1, 'attention_mask': [0, 0, 0, 0]}, {'input_ids': [101, 1045, 2046, 102], 'label': 0, 'attention_mask': [0, 0, 0, 0]}])\n",
      "DataCollatorWithPadding(tokenizer=[{'input_ids': [101, 2002, 3005], 'label': 1, 'attention_mask': [0, 0, 0]}, {'input_ids': [101, 1045, 2046, 102], 'label': 0, 'attention_mask': [0, 0, 0, 0]}], padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')\n"
     ]
    }
   ],
   "source": [
    "## transformers.default_data_collator method 예제\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "# Load a pre-trained text classification model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Prepare some sample training examples\n",
    "train_features = [\n",
    "    {\"input_ids\": [101, 2002, 3005, 102], \"label\": 1, \"attention_mask\": [0, 0, 0, 0]},\n",
    "    {\"input_ids\": [101, 1045, 2046, 102], \"label\": 0, \"attention_mask\": [0, 0, 0, 0]},\n",
    "    # ... more examples\n",
    "]\n",
    "\n",
    "# Use the default data collator with padding\n",
    "data_collator = transformers.default_data_collator\n",
    "\n",
    "# Prepare a batch of data for training\n",
    "batch = data_collator(train_features)\n",
    "\n",
    "# Check the contents of the batched tensors\n",
    "print(batch)  # Output: {'input_ids': tensor(...), 'attention_mask': tensor(...), 'labels': tensor(...)}\n",
    "#{'labels': tensor([1, 0]), 'input_ids': tensor([[ 101, 2002, 3005,  102],\n",
    "#        [ 101, 1045, 2046,  102]]), 'attention_mask': tensor([[0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0]])}\n",
    "\n",
    "\n",
    "# Now you can use this batch to train your model efficiently!\n",
    "\n",
    "# Use the default data collator with padding\n",
    "data_collator = transformers.DefaultDataCollator\n",
    "\n",
    "# Prepare a batch of data for training\n",
    "batch = data_collator(train_features)\n",
    "\n",
    "# Check the contents of the batched tensors\n",
    "print(batch)  # Output: {'input_ids': tensor(...), 'attention_mask': tensor(...), 'labels': tensor(...)}\n",
    "#DefaultDataCollator(return_tensors=[{'input_ids': [101, 2002, 3005, 102], 'label': 1, 'attention_mask': [0, 0, 0, 0]}, \n",
    "#                                    {'input_ids': [101, 1045, 2046, 102], 'label': 0, 'attention_mask': [0, 0, 0, 0]}])\n",
    "\n",
    "train_features = [\n",
    "    {\"input_ids\": [101, 2002, 3005], \"label\": 1, \"attention_mask\": [0, 0, 0]},\n",
    "    {\"input_ids\": [101, 1045, 2046, 102], \"label\": 0, \"attention_mask\": [0, 0, 0, 0]},\n",
    "    # ... more examples\n",
    "]\n",
    "\n",
    "# Use the default data collator with padding\n",
    "data_collator = transformers.DataCollatorWithPadding\n",
    "\n",
    "# Prepare a batch of data for training\n",
    "batch = data_collator(train_features)\n",
    "\n",
    "# Check the contents of the batched tensors\n",
    "print(batch)  # Output: {'input_ids': tensor(...), 'attention_mask': tensor(...), 'labels': tensor(...)}\n",
    "#DataCollatorWithPadding(tokenizer=[{'input_ids': [101, 2002, 3005], 'label': 1, 'attention_mask': [0, 0, 0]}, \n",
    "#                                   {'input_ids': [101, 1045, 2046, 102], 'label': 0, 'attention_mask': [0, 0, 0, 0]}], \n",
    "#                        padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff90a516-e97a-451f-91f5-be70acfcdd26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "{'input_ids': tensor([[  101,  1000,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "         12172,  2607,  2026,  2878,  2166,  1012,  1000,  1010,  1000,  2023,\n",
      "          2607,  2003,  6429,   999,  1000,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'labels': tensor([1])}\n",
      "['[CLS]', '\"', 'i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.', '\"', ',', '\"', 'this', 'course', 'is', 'amazing', '!', '\"', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 2장의 예제와 동일. 1개의 문장으로 해석, 1개의 레이블, 감성 분석??\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = f\"\"\"\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\" \"\"\" \n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 새롭게 추가된 코드입니다.\n",
    "batch[\"labels\"] = torch.tensor([1])\n",
    "\n",
    "print(tokenizer)\n",
    "print(batch)\n",
    "print(batch.tokens())\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b60721-77f2-4e2e-9066-c74947bfc36f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 2023, 2607, 2003, 6429, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}\n",
      "['[CLS]', 'i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.', '[SEP]', 'this', 'course', 'is', 'amazing', '!', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 12, None, 0, 1, 2, 3, 4, None]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 2장의 예제와 동일합니다. \n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# sequences = f\"\"\"\n",
    "#    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "#    \"This course is amazing!\" \"\"\" \n",
    "\n",
    "#batch = tokenizer(\"I've been waiting for a HuggingFace course my whole life.\", \n",
    "#                  \"This course is amazing!\",\n",
    "#                  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "batch = tokenizer(\"I've been waiting for a HuggingFace course my whole life.\", \n",
    "                  \"This course is amazing!\",\n",
    "                  padding=True, truncation=True)\n",
    "\n",
    "# 새롭게 추가된 코드입니다.\n",
    "#batch[\"labels\"] = torch.tensor([1])\n",
    "batch[\"labels\"] = 1\n",
    "\n",
    "print(tokenizer)\n",
    "print(batch)\n",
    "print(batch.tokens())\n",
    "print(batch.word_ids(0))\n",
    "\n",
    "#optimizer = AdamW(model.parameters())\n",
    "#loss = model(**batch).loss\n",
    "#loss.backward()\n",
    "#optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4c95be-59c5-4abc-9b3d-1391155abea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f8869c-3f4c-4f9d-9c36-b1a8f6d7634e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102,  2023,  2607,  2003,  6429,\n",
      "           999,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([batch[\"input_ids\"]])\n",
    "token_type_ids = torch.tensor([batch[\"token_type_ids\"]])\n",
    "attention_mask = torch.tensor([batch[\"attention_mask\"]])\n",
    "labels = torch.tensor([batch[\"labels\"]])\n",
    "print(input_ids)\n",
    "print(token_type_ids)\n",
    "print(attention_mask)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9858af-b42b-4d38-8704-66df0b67bef1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1183, -0.0089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102,  2023,  2607,  2003,  6429,\n",
      "           999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "#loss = model(inputs_ids = inputs_ids, \n",
    "#             token_type_ids = token_type_ids,\n",
    "#             attention_mask = attention_mask,\n",
    "#             labels = labels)\n",
    "output = model(input_ids)\n",
    "print(output)\n",
    "\n",
    "batch = {}\n",
    "batch[\"input_ids\"]=input_ids\n",
    "batch[\"token_type_ids\"]=token_type_ids\n",
    "batch[\"attention_mask\"]=attention_mask\n",
    "batch[\"labels\"]=labels\n",
    "\n",
    "print(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e299e54d-e420-4137-8371-1f6d33b2af88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8890e5-fc53-4abc-a6a1-bd555e698750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa6924-d2e4-4647-8568-d94abff243e1",
   "metadata": {},
   "source": [
    "**MRPC The Microsoft Research Paraphrase Corpus** (Dolan & Brockett, 2005) is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for **whether\r\n",
    "the sentences in the pair are semantically equivalen**t. Because the classes are imbalanced (68%\r\n",
    "positive), we follow common practice and report both accuracy and F1 score.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50672cb9-72a7-4fed-b667-e160d6aadcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f9eb657-b3c1-4bdb-b3d4-67f93c29038d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2cb88d-b843-432c-961e-371193db17f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "316db474-e489-4344-8948-e4afba1d872e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab6a54f7-51db-4704-acdd-3f4b03e73024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[ 101, 2023, 2003, 1996, 2034, 6251, 1012,  102, 2023, 2003, 1996, 2117,\n",
      "         2028, 1012,  102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102],\n",
      "               [101, 2023, 2003, 1996, 2117, 2028, 1012, 102],\n",
      "               [101, 2023, 2003, 1996, 2034, 6251, 1012, 102],\n",
      "               [101, 2023, 2003, 1996, 2117, 2028, 1012, 102]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102],\n",
      "               [101, 2023, 2003, 1996, 2117, 2028, 1012, 102]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[ 101, 2023, 2003, 1996, 2034, 6251, 1012,  102],\n",
      "        [ 101, 2023, 2003, 1996, 2117, 2028, 1012,  102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "input = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "print(input)\n",
    "\n",
    "from pprint import pprint\n",
    "input = tokenizer(\"This is the first sentence.\", \"This is the second one.\", return_tensors=\"pt\")\n",
    "pprint(input)\n",
    "\n",
    "input = tokenizer([\"This is the first sentence.\", \"This is the second one.\", \"This is the first sentence.\", \"This is the second one.\"] )\n",
    "pprint(input)\n",
    "\n",
    "input = tokenizer([\"This is the first sentence.\", \"This is the second one.\"])\n",
    "pprint(input)\n",
    "\n",
    "input = tokenizer([\"This is the first sentence.\", \"This is the second one.\"], return_tensors=\"pt\")\n",
    "pprint(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de904f16-a240-44ab-bcf9-332c5adbb2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3c5f716-d169-4685-9475-cb84903348d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# fill paddings for the 'input_ids', 'token_type_ids', 'attention_mask'\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d3e1f44-7429-43c8-82ab-08ee9e011a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19615668-b57e-495e-882b-b98b0b6e85f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28c84397-8564-4975-90d2-1be62c82aeaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': ['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\"], 'sentence2': ['Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\"], 'label': [1, 0], 'idx': [0, 1], 'input_ids': [[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], [101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b00d4134-5d59-4842-8b77-cadcd825f2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21d4458d-f9db-4d87-bd44-75e1e60aa4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "280c977e-15cd-417b-be8b-e21c669ef911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.39.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "472d72d0-8276-4bae-8719-f905c071ac34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.523400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.296000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3398694084793567, metrics={'train_runtime': 51.9027, 'train_samples_per_second': 212.012, 'train_steps_per_second': 26.53, 'total_flos': 405114969714960.0, 'train_loss': 0.3398694084793567, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32e08d9b-46c0-4be1-8bed-40f1412fa783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26a333ba-fd68-47fc-81d7-ddd938321e78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.816679   3.3098261]\n",
      " [ 3.1239483 -2.9603717]\n",
      " [ 0.4028756 -0.6478887]\n",
      " [-3.7489088  3.2260523]\n",
      " [ 2.8513675 -2.8319807]]\n",
      "[1 0 0 1 0]\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0][:5])\n",
    "print(predictions.label_ids[:5])\n",
    "print(type(predictions.label_ids[:5][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00807672-a2e6-4168-8feb-b9db4f4e8e90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"validation\"][\"label\"][:5]\n",
    "type(tokenized_datasets[\"validation\"][\"label\"][:5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b07657e-de12-426f-aed4-1af456009293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2)\n",
      "<class 'numpy.ndarray'>\n",
      "(408,)\n",
      "<class 'numpy.ndarray'>\n",
      "(408,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "print(predictions.predictions.shape)\n",
    "print(type(preds))\n",
    "print(preds.shape)\n",
    "print(type(predictions.label_ids))\n",
    "print(predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e77471ca-a54a-4bb2-9c0d-c6377f953840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67c4d5c8-4fb6-48b8-94e4-4f14114fbbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27072/1543290331.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"mrpc\")\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8480392156862745, 'f1': 0.8959731543624161}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c81dfdd0-ead6-46c2-b4a6-099b26f46893",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(408,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8480392156862745, 'f1': 0.8959731543624161}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "\n",
    "labels = tokenized_datasets[\"validation\"][\"label\"]\n",
    "labels = np.array(labels)\n",
    "print(type(labels))\n",
    "print(labels.shape)\n",
    "metric.compute(predictions=preds, references=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3eb0d-56e2-4f25-9880-3bb7c097df67",
   "metadata": {},
   "source": [
    "모델 헤드를 무작위로 초기화하면 계산된 메트릭이 변경될 수 있으므로 정확한 결과는 다를 수 있습니다. 여기서는 모델이 검증 집합에서 86.76%의 정확도(accuracy)와 90.69의 F1 점수를 가지고 있음을 알 수 있습니다(여러분의 결과값과는 다를 수도 있습니다.). 이는 GLUE 벤치마크의 MRPC 데이터셋에 대한 예측 결과를 평가하는데 사용되는 두 가지 메트릭(metrics)입니다. BERT 논문의 테이블은 기본 모델에 대해 F1 점수 88.9를 보고했습니다. 해당 논문에서 사용된 모델은 소문자 모델(uncased model)이었고 여기서는 대소문자 구분 모델(cased model)을 활용했으므로 성능 차이가 발생하고 있습니다.\n",
    "\n",
    "지금까지 설명한 모든 것을 함께 종합하면 compute_metrics() 함수를 얻을 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ea89c4b-bc77-4e7a-b217-278b62acb44c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "428b2cfe-1044-45a0-9db2-25eb5bdd9391",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8480392156862745}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04187ac3-afe7-4b23-a265-817ae27627d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8480392156862745,\n",
       " 'f1': 0.8959731543624161,\n",
       " 'precision': 0.8422712933753943,\n",
       " 'recall': 0.956989247311828}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "clf_metrics.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3780a41-ddcb-4ee1-b0b1-a1c7497cc8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "def compute_metrics(eval_preds):\n",
    "    #metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    #print(logits.shape, labels.shape)  #(408,2), (408,)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "871b9cf4-2b46-4fd9-9f0a-0a76758a3089",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.363859</td>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.885609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.422261</td>\n",
       "      <td>0.855392</td>\n",
       "      <td>0.896309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.623663</td>\n",
       "      <td>0.850490</td>\n",
       "      <td>0.897822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.33511093516408663, metrics={'train_runtime': 53.5323, 'train_samples_per_second': 205.558, 'train_steps_per_second': 25.723, 'total_flos': 405114969714960.0, 'train_loss': 0.33511093516408663, 'epoch': 3.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f2f65db-aa0f-471f-8e9b-2dee4dbd1457",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.403378</td>\n",
       "      <td>0.840686</td>\n",
       "      <td>0.888508</td>\n",
       "      <td>0.851974</td>\n",
       "      <td>0.928315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>0.454073</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.880546</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.622920</td>\n",
       "      <td>0.855392</td>\n",
       "      <td>0.898451</td>\n",
       "      <td>0.864238</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3681085956382336, metrics={'train_runtime': 53.3967, 'train_samples_per_second': 206.08, 'train_steps_per_second': 25.788, 'total_flos': 405114969714960.0, 'train_loss': 0.3681085956382336, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "#metric = evaluate.load(\"accuracy\")\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    #metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85407989-f9f0-49fc-aee8-cab8b1879fc9",
   "metadata": {},
   "source": [
    "### 전체 학습 (Full Training)\n",
    "이제 Trainer 클래스를 사용하지 않고 이전 섹션에서 했던 것과 동일한 결과를 얻는 방법을 살펴보겠습니다. 다시 말하지만, 섹션 2에서 이미 데이터 처리를 완료했다고 가정합니다. 이번 섹션을 공부할 때 필요한 모든 작업을 수행하는 코드는 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4025e61-77d8-4c29-b680-221e5cf116a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True) #return_type = \"pt\"\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d52e4-21e7-4be8-b0fa-b9caf9d8da13",
   "metadata": {},
   "source": [
    "### 학습을 위한 준비\n",
    "실제로 학습 루프(training loop)를 작성하기 전에 몇 가지 객체를 정의해야 합니다. 첫 번째는 배치(batch)를 반복하는 데 사용할 dataloaders입니다. 그러나 이 dataloaders를 정의하기 전에 Trainer가 자동으로 수행한 몇 가지 작업을 직접 처리하기 위해 tokenized_datasets에 약간의 후처리를 적용해야 합니다. 구체적으로 다음을 수행해야 합니다:\n",
    "\n",
    "- 모델이 필요로 하지 않는 값이 저장된 열(columns)을 제거합니다. (sentence1, sentence2 등)\n",
    "\n",
    "- 열 레이블(column label)의 이름을 labels로 바꿉니다. 이는 모델이 labels라는 이름으로 매개변수를 받기 때문입니다.\n",
    "\n",
    "- 파이썬 리스트 대신 PyTorch 텐서(tensors)를 반환하도록 datasets의 형식을 설정합니다.\n",
    "\n",
    "tokenized_datasets에는 이러한 작업을 위한 별도의 메서드들이 존재합니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5ee5804-a39f-49d3-be20-dd4f92de53c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b79a3b4f-722f-472b-b655-26f867cfcc5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6baa076b-fe56-4299-add3-d48e7bbd0db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57b0e8d9-2f68-4a98-968c-bf1413b36bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6550846-fb3f-452d-80e9-4a9c34e97085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_train_datasets = tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1913d8d1-18a2-45e1-a844-a5738dc221a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_datasets.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc77f83c-e99f-4590-b5c8-ce2ddc758563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5d18407-3206-4b7e-b172-4ae4f419e619",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_datasets = tokenized_datasets[\"train\"]\n",
    "tokenized_train_datasets.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65d12622-9187-4865-a9ea-89e64e71f902",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor([1, 0]), 'input_ids': [tensor([  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
      "         2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
      "         3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
      "         1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
      "         2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102]), tensor([  101,  9805,  3540, 11514,  2050,  3079, 11282,  2243,  1005,  1055,\n",
      "         2077,  4855,  1996,  4677,  2000,  3647,  4576,  1999,  2687,  2005,\n",
      "         1002,  1016,  1012,  1019,  4551,  1012,   102,  9805,  3540, 11514,\n",
      "         2050,  4149, 11282,  2243,  1005,  1055,  1999,  2786,  2005,  1002,\n",
      "         6353,  2509,  2454,  1998,  2853,  2009,  2000,  3647,  4576,  2005,\n",
      "         1002,  1015,  1012,  1022,  4551,  1999,  2687,  1012,   102])], 'token_type_ids': [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])], 'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_datasets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff210df2-a414-49e5-8cf1-9e85c69664b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e5f7d88-5bd4-4ff3-9204-565dcaecb9a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad951df-bec9-454b-8279-1f2dc53717d1",
   "metadata": {},
   "source": [
    "위에서 보듯이 결과적으로 tokenized_datasets에는 모델이 허용하는 columns만 존재함을 알 수 있습니다.\n",
    "\n",
    "이제 이 작업이 완료되었으므로 dataloader를 쉽게 정의할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af07d9ec-72d2-4da5-9f83-f94d9478b2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], # tensor\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator, # automatically padding \n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159fd2a5-06a2-45b7-9e10-60455b8d27a4",
   "metadata": {},
   "source": [
    "데이터 처리에 오류가 없는지 빠르게 확인하기 위해 다음과 같이 배치(batch)를 검사할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3525ac1a-07ca-49f6-b1b1-218d540946d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 81]),\n",
       " 'token_type_ids': torch.Size([8, 81]),\n",
       " 'attention_mask': torch.Size([8, 81]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f22dee3f-99f9-4f35-bd11-ec862ec59f70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  6736, 11582,  2008,  1996,  1000, 25710,  2064,  9033,\n",
       "          6199,  2083, 15288,  1999,  1996,  5894,  1010,  2105,  2422, 17407,\n",
       "          2030,  2105,  5894,  4599,  1012,  1000,   102,  1996, 25710,  2064,\n",
       "          9033,  6199,  2083, 15288,  1999,  1996,  5894,  1010,  2105,  2422,\n",
       "         17407,  2030,  2105,  5894,  4599,  1010,  2061, 15288,  2323,  2022,\n",
       "         10203,  1010,  2027, 16755,  1012,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1996,  3291,  3497,  2097,  2812,  6149,  3512,  3431,  2077,\n",
       "          1996, 10382,  4170,  4627,  3909,  2153,  1012,   102,  2002,  2056,\n",
       "          1996,  3291,  3791,  2000,  2022, 13371,  2077,  1996,  2686, 10382,\n",
       "          4170,  2003,  5985,  2000,  4875,  2153,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  7641,  3373,  2008, 18766,  2075,  1037,  4126,  2002,  2106,\n",
       "          2025, 10797,  2001,  1996,  2069,  2126,  2041,  1010,  9482,  2056,\n",
       "          1012,   102,  2000,  1996, 10363,  7641,  1010,  9482,  2056,  1010,\n",
       "         18766,  2075,  1037,  4126,  2002,  2106,  2025, 10797,  2246,  2066,\n",
       "          1996,  2069,  2041,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1996,  6957,  8617,  2274,  3514, 25416, 26455,  3111,  1998,\n",
       "          2062,  2084,  1016,  1010,  2531,  8110,  3703,  1999,  3607,  1998,\n",
       "          5924,  1012,   102, 28286,  2243,  1011, 17531,  2038,  2416,  3514,\n",
       "          5155,  3197,  1010,  2274, 25416, 26455,  3111,  1998,  1016,  1010,\n",
       "          2531,  8110,  3703,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1996,  2820,  2005,  4425,  2968,  2056,  2049,  5950,  1997,\n",
       "          2512,  1011,  5814,  4023,  3123,  2000,  2753,  1012,  1021,  2013,\n",
       "          4700,  1012,  1023,  1999,  2233,  1012,   102,  1996,  2820,  2005,\n",
       "          4425,  2968,  2056,  2049,  5950,  1997,  2512,  1011,  5814,  4023,\n",
       "          3123,  2000,  2753,  1012,  1021,  1010, 16361,  2067,  2013,  1037,\n",
       "          2028,  1011,  3204, 21963,  1999,  2233,  2012,  4700,  1012,  1023,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1037,  2918,  9563,  2007,  1037,  1016,  1012,  1014,  5603,\n",
       "          2480, 13420,  8292,  3917,  2239, 13151,  1010, 11899,  2213, 27507,\n",
       "          1997,  3638,  1010,  1037,  2871,  2290,  1011, 24880,  2524,  3298,\n",
       "          1010,  1998,  1037,  3729,  1011, 17083,  3298,  5366,  2149,  1002,\n",
       "          5824,  2683,  1012,   102,  1037,  2918,  9563,  2007,  1037,  1016,\n",
       "          1012,  1018,  5603,  2480,  7279, 16398,  1018,  1010, 11899, 14905,\n",
       "          1997,  8223,  1010,  1037,  2871, 18259,  2524,  3298,  1010,  1998,\n",
       "          1037,  3729,  1011, 17083,  3298,  5366,  1002,  6353,  2683,  1012,\n",
       "           102],\n",
       "        [  101,  6746,  2015, 24529,  2497,  4484,  2006,  9857,  2009,  2001,\n",
       "          1000,  6195,  2049,  7047,  1000,  2058,  1996,  5096,  1997,  2049,\n",
       "          2120,  2924,  1997,  2047,  3414,  7506,  1012,   102,  6746,  2015,\n",
       "         24529,  2497,  7483,  4484,  2008,  2009,  2018,  2363, 20723,  2005,\n",
       "          2049,  2120,  2924,  1997,  2047,  3414,  3169,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  4001,  2343,  4013, 13657,  2890,  2728,  1040,  1012, 11721,\n",
       "         11715,  1010,  1054,  1011,  8912,  1010,  2056,  2002,  3764,  2007,\n",
       "         22762,  2319,  2025,  2146,  2077,  1996, 11292,  1997,  2436,  5103,\n",
       "          1012,   102,  4001,  2343,  4013, 13657,  2890,  2728,  1040,  1012,\n",
       "         11721, 11715,  1010,  1054,  1011,  8912,  1010,  2056,  2002,  1005,\n",
       "          2222,  3335,  1996,  3099,  1005,  1055,  2942,  3012,  2087,  1012,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([0, 1, 1, 0, 1, 1, 1, 0])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6caf6023-0800-4691-8f7c-d5f32b9a0917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 81]),\n",
       " 'token_type_ids': torch.Size([8, 81]),\n",
       " 'attention_mask': torch.Size([8, 81]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e630cba8-6243-4afe-b3ad-13baa6b2c8eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = {k: v.to(device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f969a30-6f08-48ee-8223-1cafd5073264",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  6736, 11582,  2008,  1996,  1000, 25710,  2064,  9033,\n",
       "           6199,  2083, 15288,  1999,  1996,  5894,  1010,  2105,  2422, 17407,\n",
       "           2030,  2105,  5894,  4599,  1012,  1000,   102,  1996, 25710,  2064,\n",
       "           9033,  6199,  2083, 15288,  1999,  1996,  5894,  1010,  2105,  2422,\n",
       "          17407,  2030,  2105,  5894,  4599,  1010,  2061, 15288,  2323,  2022,\n",
       "          10203,  1010,  2027, 16755,  1012,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  1996,  3291,  3497,  2097,  2812,  6149,  3512,  3431,  2077,\n",
       "           1996, 10382,  4170,  4627,  3909,  2153,  1012,   102,  2002,  2056,\n",
       "           1996,  3291,  3791,  2000,  2022, 13371,  2077,  1996,  2686, 10382,\n",
       "           4170,  2003,  5985,  2000,  4875,  2153,  1012,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  7641,  3373,  2008, 18766,  2075,  1037,  4126,  2002,  2106,\n",
       "           2025, 10797,  2001,  1996,  2069,  2126,  2041,  1010,  9482,  2056,\n",
       "           1012,   102,  2000,  1996, 10363,  7641,  1010,  9482,  2056,  1010,\n",
       "          18766,  2075,  1037,  4126,  2002,  2106,  2025, 10797,  2246,  2066,\n",
       "           1996,  2069,  2041,  1012,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  1996,  6957,  8617,  2274,  3514, 25416, 26455,  3111,  1998,\n",
       "           2062,  2084,  1016,  1010,  2531,  8110,  3703,  1999,  3607,  1998,\n",
       "           5924,  1012,   102, 28286,  2243,  1011, 17531,  2038,  2416,  3514,\n",
       "           5155,  3197,  1010,  2274, 25416, 26455,  3111,  1998,  1016,  1010,\n",
       "           2531,  8110,  3703,  1012,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  1996,  2820,  2005,  4425,  2968,  2056,  2049,  5950,  1997,\n",
       "           2512,  1011,  5814,  4023,  3123,  2000,  2753,  1012,  1021,  2013,\n",
       "           4700,  1012,  1023,  1999,  2233,  1012,   102,  1996,  2820,  2005,\n",
       "           4425,  2968,  2056,  2049,  5950,  1997,  2512,  1011,  5814,  4023,\n",
       "           3123,  2000,  2753,  1012,  1021,  1010, 16361,  2067,  2013,  1037,\n",
       "           2028,  1011,  3204, 21963,  1999,  2233,  2012,  4700,  1012,  1023,\n",
       "           1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  1037,  2918,  9563,  2007,  1037,  1016,  1012,  1014,  5603,\n",
       "           2480, 13420,  8292,  3917,  2239, 13151,  1010, 11899,  2213, 27507,\n",
       "           1997,  3638,  1010,  1037,  2871,  2290,  1011, 24880,  2524,  3298,\n",
       "           1010,  1998,  1037,  3729,  1011, 17083,  3298,  5366,  2149,  1002,\n",
       "           5824,  2683,  1012,   102,  1037,  2918,  9563,  2007,  1037,  1016,\n",
       "           1012,  1018,  5603,  2480,  7279, 16398,  1018,  1010, 11899, 14905,\n",
       "           1997,  8223,  1010,  1037,  2871, 18259,  2524,  3298,  1010,  1998,\n",
       "           1037,  3729,  1011, 17083,  3298,  5366,  1002,  6353,  2683,  1012,\n",
       "            102],\n",
       "         [  101,  6746,  2015, 24529,  2497,  4484,  2006,  9857,  2009,  2001,\n",
       "           1000,  6195,  2049,  7047,  1000,  2058,  1996,  5096,  1997,  2049,\n",
       "           2120,  2924,  1997,  2047,  3414,  7506,  1012,   102,  6746,  2015,\n",
       "          24529,  2497,  7483,  4484,  2008,  2009,  2018,  2363, 20723,  2005,\n",
       "           2049,  2120,  2924,  1997,  2047,  3414,  3169,  1012,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  4001,  2343,  4013, 13657,  2890,  2728,  1040,  1012, 11721,\n",
       "          11715,  1010,  1054,  1011,  8912,  1010,  2056,  2002,  3764,  2007,\n",
       "          22762,  2319,  2025,  2146,  2077,  1996, 11292,  1997,  2436,  5103,\n",
       "           1012,   102,  4001,  2343,  4013, 13657,  2890,  2728,  1040,  1012,\n",
       "          11721, 11715,  1010,  1054,  1011,  8912,  1010,  2056,  2002,  1005,\n",
       "           2222,  3335,  1996,  3099,  1005,  1055,  2942,  3012,  2087,  1012,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'labels': tensor([0, 1, 1, 0, 1, 1, 1, 0], device='cuda:0')}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bfba77-df03-4c8f-b226-af32181d0d64",
   "metadata": {
    "tags": []
   },
   "source": [
    "실제 데이터 형태(shapes)가 살짝 다를 수 있는데 이는 학습 dataloader에 대해 shuffle=True를 설정하고 배치(batch) 내에서의 최대 길이로 패딩(padding)하기 때문입니다.\n",
    "\n",
    "이제 데이터 전처리가 완전히 끝났습니다. 기계학습 실무자(ML practitioners)들에게는 만족스럽기도 하겠지만 일부 명확하지 않은 부분도 있을 수 있겠습니다. 이제 모델로 돌아가 봅시다. 이전 섹션에서 수행한 것과 동일한 방법으로 모델을 인스턴스화(instantiate)합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32800675-1ad6-4247-8ca3-b196c6f4e238",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75395fa1-5f37-45e6-a4fb-d2cdb5ba1f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(model.config)\n",
    "print(model.config.num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471f4fe-335c-4ce2-a4d5-9fdc8f46c0d0",
   "metadata": {},
   "source": [
    "학습 과정에서 모든 것들이 원활하게 진행될 수 있는지 확인하기 위해 배치(batch)를 이 모델에 한번 전달해 봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28555a06-9c8b-43a0-ac06-212fb5b04746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9363, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6608, -0.3372],\n",
      "        [ 0.6689, -0.3575],\n",
      "        [ 0.6599, -0.3340],\n",
      "        [ 0.6668, -0.3561],\n",
      "        [ 0.6559, -0.3264],\n",
      "        [ 0.6782, -0.3260],\n",
      "        [ 0.6623, -0.3201],\n",
      "        [ 0.6589, -0.3497]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "loss: 0.9362912178039551, logits.shape: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs)\n",
    "print(f\"loss: {outputs.loss}, logits.shape: {outputs.logits.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237574bd-3f32-4eb4-9adc-b20ca76e040b",
   "metadata": {},
   "source": [
    "모든 🤗Transformers 모델은 매개변수에 labels이 포함되어 있다면 손실(loss)과 함께 logit값(batch내 각 입력에 대해 logit값이 2개이므로 크기가 8 x 2인 텐서)도 반환합니다.\n",
    "\n",
    "학습 루프(training loop)를 작성할 준비가 거의 되었습니다! 그런데 아직 최적화 함수(optimizer) 및 학습률 스케줄러(learning rate scheduler) 지정 작업이 남아 있습니다. 여기서는 앞에서 배운 Trainer의 기본 설정을 그대로 사용하겠습니다. Trainer가 사용하는 최적화 함수는 AdamW이며 이는 Adam과 거의 동일하지만 weight decay regularization을 적용했다는 사실에 차이가 납니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8442fce0-3645-41ac-9712-e74c0b151431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a16605-fc77-4fb7-80b4-8d5bf5a8ae3d",
   "metadata": {},
   "source": [
    "마지막으로, Trainer에서 디폴트로 사용되는 학습률 스케줄러(learning rate scheduler)는 최대값(5e-5)에서 0까지 선형 감쇠(linear decay)합니다. 이를 적절하게 정의하려면 우리가 수행할 학습 단계의 횟수를 알아야 합니다. 이는 실행하려는 에포크(epochs) 수에 학습 배치(batch)의 개수를 곱한 것입니다. 학습 배치의 개수는 학습 dataloader의 길이와 같습니다. Trainer는 디폴트로 3개의 에포크(epochs)를 사용하므로 다음을 따릅니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a96353c5-bd3b-4c49-a8be-cbe2a4149b48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459\n",
      "1377\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(len(train_dataloader))\n",
    "print(num_training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ac85c70-054c-4bab-83de-1f47009dfb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf2b96-f6a4-4833-93e6-f31bd12e65a1",
   "metadata": {},
   "source": [
    "#### 학습 루프 (Training Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "207e0d42-03de-463e-9aea-9a2f42bc24a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "486953d0-d02f-4eb3-984a-c6cfdd619777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ab7b711a05452086e1c800295348ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ba659-ed87-4ae7-9b08-8aea73e1a089",
   "metadata": {},
   "source": [
    "#### 평가 루프 (Evaluation Loop)\n",
    "이전에 수행했던 것처럼, 🤗Datasets 라이브러리에서 제공하는 평가 메트릭(metrics)을 사용합니다. 우리는 이미 metric.compute() 메서드를 살펴보았지만 metric.add_batch() 메서드로 평가 루프(evaluation loop)를 실행하면서 배치(batch)별 평가 메트릭(metrics) 계산 결과를 누적할 수 있습니다. 모든 배치(batch)를 누적하고 나면 metric.compute()로 최종 결과를 얻을 수 있습니다. 평가 루프에서 이 모든 것을 구현하는 방법은 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c022a15-25b3-4f5b-9d4a-ca18d3ffe96f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5882352941176471, 'f1': 0.6363636363636364}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d892bef-6966-4046-9193-771993631376",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5882352941176471, 'f1': 0.6363636363636364}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "249acfb5-dd88-48d2-b4c8-f6de7734233c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5882352941176471,\n",
       " 'f1': 0.6363636363636364,\n",
       " 'precision': 0.8032786885245902,\n",
       " 'recall': 0.5268817204301075}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "#metric = evaluate.load(\"accuracy\")\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    clf_metrics.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "clf_metrics.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3683c-796e-4427-a555-d9cd0894cec3",
   "metadata": {},
   "source": [
    "### 🤗Accelerate 라이브러리를 사용한 학습 루프 가속화\n",
    "앞에서 정의한 학습 루프(training loop)는 단일 CPU 또는 단일 GPU에서 제대로 작동합니다. 그러나 🤗Accelerate 라이브러리를 사용하여 몇 가지 설정만 하면 여러 GPU 또는 TPU에서 분산 학습(distributed training)을 수행할 수 있습니다. 학습 및 검증 dataloader를 생성 한 후, 학습 루프(training loop)의 형태는 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "848c3c64-71f6-4be7-8e7e-c0dc2c813f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44991d444d274830b26136864791bbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07661e74-c615-4cd4-9c40-f06b99738629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543716cc7fca4b0ba009a988799045a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f39d0-a1ae-489d-929f-8e4b4b297278",
   "metadata": {},
   "source": [
    "### Full Training 코드 w/ Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e641b28-ef55-4259-849a-bfc189cc9f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.442705</td>\n",
       "      <td>0.796569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.508943</td>\n",
       "      <td>0.843137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.298500</td>\n",
       "      <td>0.663428</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3.0,\n",
      " 'eval_accuracy': 0.8371014492753623,\n",
      " 'eval_loss': 0.7649620175361633,\n",
      " 'eval_runtime': 2.3493,\n",
      " 'eval_samples_per_second': 734.25,\n",
      " 'eval_steps_per_second': 91.941}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 3668\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 408\n",
    "#    })\n",
    "#    test: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 1725\n",
    "#    })\n",
    "#})\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#        num_rows: 3668\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#        num_rows: 408\n",
    "#    })\n",
    "#    test: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#        num_rows: 1725\n",
    "#    })\n",
    "#})\n",
    "\n",
    "# 데이터셋 \"label\" 또는 \"labels\" 둘 다 작동함.  \n",
    "#tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    #metric = load_metric(\"glue\", \"mrpc\")\n",
    "    #print(eval_preds)\n",
    "    logits, labels = eval_preds\n",
    "    # print(logits.shape, labels.shape) (408,2) (408,) note that the len(Dataset[\"validation\"])==408\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# the default per_device_train_batch_size is equal to 8\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "#print(training_args) \n",
    "# check out training_args.per_device_train_batch_size\n",
    "# print(training_args.per_device_train_batch_size)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, # based on evaluation_strategy, it evaluate \n",
    ")\n",
    "\n",
    "#print(trainer)\n",
    "trainer.train() # train datasets 3668 * 3 (default epochs) / 8 (default batch size) = 1377 steps\n",
    "\n",
    "#save\n",
    "#trainer.save_model \n",
    "\n",
    "#eval\n",
    "from pprint import pprint\n",
    "metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"]) # test datasets 1725 / 8 = 216 steps \n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6bb8e7-8fca-40fc-99e0-6a19a6b0e2db",
   "metadata": {},
   "source": [
    "### Full Training 코드 w/o Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f9a853c8-73f5-4009-980b-b67db1914803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fb34d6f0c142dfb7cb33d60c212d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8553921568627451, 'f1': 0.8973913043478261}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# 데이터 셋 적재\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 3668\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 408\n",
    "#    })\n",
    "#    test: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 1725\n",
    "#    })\n",
    "#})\n",
    "\n",
    "# 사전학습 언어모델 checkpoint 이름 지정\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "# 지정된 사전학습 언어모델에서 토크나이저 인스턴스화\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "# 토크나이저 함수 사용자 정의화 (sentence1, sentence2 컬럼에 대해서만 토크나이징 수행)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "# 토크나이징 수행\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# 배치(batch)별 패딩(padding)을 위한 data collator 정의\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 불필요한 입력 컬럼을 제거하고 사전학습 언어모델에 필요한 입력만 남김.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "# 데이터셋의 label 컬럼명을 labels로 변경\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "# 데이터셋의 유형을 PyTorch tensor로 변경\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# 변경된 컬럼 출력\n",
    "print(tokenized_datasets[\"train\"].column_names)\n",
    "\n",
    "\n",
    "# 각 종류별 데이터 로더 생성\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], \n",
    "                              shuffle=True, \n",
    "                              batch_size=8, \n",
    "                              collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"],\n",
    "                             shuffle=True,\n",
    "                             batch_size=8,\n",
    "                             collate_fn=data_collator)\n",
    "\n",
    "# 사전학습 언어모델 인스턴스화\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# 최적화 함수 정의\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 에포크 개수 설정\n",
    "num_epochs = 3\n",
    "# 학습 스텝 수 계산\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# 학습 스케쥴러 설정\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# GPU로 모델을 이동\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 진행 상황바 정의\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# 모델을 학습 모드로 전환\n",
    "model.train()\n",
    "# 학습 루프 시작\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # 현재 배치 중에서 입력값을 모두 GPU로 이동.\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # 모델 실행\n",
    "        outputs = model(**batch)\n",
    "        # 손실값 가져오기\n",
    "        loss = outputs.loss\n",
    "        # 역전파 수행\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# 평가 메트릭 가져오기\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# 평가 결과 계산 및 출력 \n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad3b1c-c923-40c5-ac6d-7b331b869181",
   "metadata": {},
   "source": [
    "### Full Training w/ evaluate \n",
    "\n",
    "```\n",
    "from datasets import load_metrics \n",
    "metric = load_metric(\"glue\", \"mrpc\") \n",
    "\n",
    "===>\n",
    "\n",
    "import evaluate\n",
    "metric = evaluat.load(\"accuracy\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d98b92df-ef4c-40a6-80e6-2dd78f3b05f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dc85076a0d41dd987bf968dc2c109c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8651960784313726}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# 데이터 셋 적재\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "# 사전학습 언어모델 checkpoint 이름 지정\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "# 지정된 사전학습 언어모델에서 토크나이저 인스턴스화\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "# 토크나이저 함수 사용자 정의화 (sentence1, sentence2 컬럼에 대해서만 토크나이징 수행)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "# 토크나이징 수행\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# 배치(batch)별 패딩(padding)을 위한 data collator 정의\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 불필요한 입력 컬럼을 제거하고 사전학습 언어모델에 필요한 입력만 남김.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "# 데이터셋의 label 컬럼명을 labels로 변경\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "# 데이터셋의 유형을 PyTorch tensor로 변경\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# 변경된 컬럼 출력\n",
    "print(tokenized_datasets[\"train\"].column_names)\n",
    "\n",
    "\n",
    "# 각 종류별 데이터 로더 생성\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], \n",
    "                              shuffle=True, \n",
    "                              batch_size=8, \n",
    "                              collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"],\n",
    "                             shuffle=True,\n",
    "                             batch_size=8,\n",
    "                             collate_fn=data_collator)\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "#accelerator = Accelerator()\n",
    "\n",
    "# 사전학습 언어모델 인스턴스화\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# 최적화 함수 정의\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "#train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "#    train_dataloader, eval_dataloader, model, optimizer\n",
    "#)\n",
    "\n",
    "# 에포크 개수 설정\n",
    "num_epochs = 3\n",
    "# 학습 스텝 수 계산\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# 학습 스케쥴러 설정\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# GPU로 모델을 이동\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 진행 상황바 정의\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# 모델을 학습 모드로 전환\n",
    "model.train()\n",
    "# 학습 루프 시작\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # 현재 배치 중에서 입력값을 모두 GPU로 이동.\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # 모델 실행\n",
    "        outputs = model(**batch)\n",
    "        # 손실값 가져오기\n",
    "        loss = outputs.loss\n",
    "        # 역전파 수행\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# 평가 메트릭 가져오기\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# 평가 결과 계산 및 출력 \n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c875cbe-337f-4490-8435-e3e21c6436fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Accelerate() 라이브러리 사용\n",
    "\n",
    "There is no need to specify model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f271ef7d-28fd-4f72-bb74-a379f565433b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf7380cfe5643e99ca29007c36cb82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8700980392156863}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "\n",
    "import evaluate\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 데이터 셋 적재\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "# 사전학습 언어모델 checkpoint 이름 지정\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "# 지정된 사전학습 언어모델에서 토크나이저 인스턴스화\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "# 토크나이저 함수 사용자 정의화 (sentence1, sentence2 컬럼에 대해서만 토크나이징 수행)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "# 토크나이징 수행\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# 배치(batch)별 패딩(padding)을 위한 data collator 정의\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 불필요한 입력 컬럼을 제거하고 사전학습 언어모델에 필요한 입력만 남김.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "# 데이터셋의 label 컬럼명을 labels로 변경\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "# 데이터셋의 유형을 PyTorch tensor로 변경\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# 변경된 컬럼 출력\n",
    "print(tokenized_datasets[\"train\"].column_names)\n",
    "\n",
    "\n",
    "# 각 종류별 데이터 로더 생성\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], \n",
    "                              shuffle=True, \n",
    "                              batch_size=8, \n",
    "                              collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"],\n",
    "                             shuffle=True,\n",
    "                             batch_size=8,\n",
    "                             collate_fn=data_collator)\n",
    "\n",
    "# 사전학습 언어모델 인스턴스화\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# 최적화 함수 정의\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "# 에포크 개수 설정\n",
    "num_epochs = 3\n",
    "# 학습 스텝 수 계산\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# 학습 스케쥴러 설정\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# GPU로 모델을 이동\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "\n",
    "# 진행 상황바 정의\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# 모델을 학습 모드로 전환\n",
    "model.train()\n",
    "# 학습 루프 시작\n",
    "#for epoch in range(num_epochs):\n",
    "#    for batch in train_dataloader:\n",
    "#        # 현재 배치 중에서 입력값을 모두 GPU로 이동.\n",
    "#        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#        # 모델 실행\n",
    "#        outputs = model(**batch)\n",
    "#        # 손실값 가져오기\n",
    "#        loss = outputs.loss\n",
    "#        # 역전파 수행\n",
    "#        loss.backward()\n",
    "#\n",
    "#        optimizer.step()\n",
    "#        lr_scheduler.step()\n",
    "#        optimizer.zero_grad()\n",
    "#        progress_bar.update(1)\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "\n",
    "# 평가 메트릭 가져오기\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "#from accelerate import notebook_launcher\n",
    "#\n",
    "#notebook_launcher(training_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5caab7d-d01e-448c-a62e-352cd07d740a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6c8ddbd709463f965da5550a1ad986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e5d5032f49460a8bed16229b493807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b370cc39dfc442c4a3221b509a1a7314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714dd02af6e34868a45e04bce14d6dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'accuracy': 0.8137254901960784, 'f1': 0.8538461538461538}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'accuracy': 0.8700980392156863, 'f1': 0.9087779690189329}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'accuracy': 0.8651960784313726, 'f1': 0.9043478260869565}\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huggingface/accelerate/tree/main/examples\n",
    "\n",
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import argparse\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "from accelerate import Accelerator, DistributedType\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "########################################################################\n",
    "# This is a fully working simple example to use Accelerate\n",
    "#\n",
    "# This example trains a Bert base model on GLUE MRPC\n",
    "# in any of the following settings (with the same script):\n",
    "#   - single CPU or single GPU\n",
    "#   - multi GPUS (using PyTorch distributed mode)\n",
    "#   - (multi) TPUs\n",
    "#   - fp16 (mixed-precision) or fp32 (normal precision)\n",
    "#\n",
    "# To run it in each of these various modes, follow the instructions\n",
    "# in the readme for examples:\n",
    "# https://github.com/huggingface/accelerate/tree/main/examples\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "MAX_GPU_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def get_dataloaders(accelerator: Accelerator, batch_size: int = 16):\n",
    "    \"\"\"\n",
    "    Creates a set of `DataLoader`s for the `glue` dataset,\n",
    "    using \"bert-base-cased\" as the tokenizer.\n",
    "\n",
    "    Args:\n",
    "        accelerator (`Accelerator`):\n",
    "            An `Accelerator` object\n",
    "        batch_size (`int`, *optional*):\n",
    "            The batch size for the train and validation DataLoaders.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # max_length=None => use the model max length (it's actually the default)\n",
    "        outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], \n",
    "                            truncation=True, max_length=None)\n",
    "        return outputs\n",
    "\n",
    "    # Apply the method we just defined to all the examples in all the splits of the dataset\n",
    "    # starting with the main process first:\n",
    "    with accelerator.main_process_first():\n",
    "        tokenized_datasets = datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n",
    "        )\n",
    "\n",
    "    # We also rename the 'label' column to 'labels' which is the expected name for labels \n",
    "    # by the models of the transformers library\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        # On TPU it's best to pad everything to the same length or training will be very slow.\n",
    "        max_length = 128 if accelerator.distributed_type == DistributedType.TPU else None\n",
    "        # When using mixed precision we want round multiples of 8/16\n",
    "        if accelerator.mixed_precision == \"fp8\":\n",
    "            pad_to_multiple_of = 16\n",
    "        elif accelerator.mixed_precision != \"no\":\n",
    "            pad_to_multiple_of = 8\n",
    "        else:\n",
    "            pad_to_multiple_of = None\n",
    "            \n",
    "        #print(\"examples:\\n\", examples)    \n",
    "\n",
    "        # tokenizer.pad() is a method used to pad sequences of tokens to a specified length.\n",
    "        tokens = tokenizer.pad(\n",
    "            examples,\n",
    "            padding=\"longest\",\n",
    "            max_length=max_length,\n",
    "            #pad_to_multiple_of = 16,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_tensors=\"pt\")\n",
    "        \n",
    "        ## print each token's shape \n",
    "        #for i, token in enumerate(tokens[\"input_ids\"]):\n",
    "        #    print(i, token.shape)\n",
    "        \n",
    "        return tokens\n",
    "        \n",
    "        #return tokenizer.pad(\n",
    "        #    examples,\n",
    "        #    padding=\"longest\",\n",
    "        #    max_length=max_length,\n",
    "        #    pad_to_multiple_of=pad_to_multiple_of,\n",
    "        #    return_tensors=\"pt\",\n",
    "        #)\n",
    "\n",
    "    # Instantiate dataloaders.\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"], # each tokenized dataset element may have different length.\n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,  # each element in a batch w/ batch_size to be paded with the specified length \n",
    "        batch_size=batch_size, \n",
    "        drop_last=True\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=EVAL_BATCH_SIZE,\n",
    "        drop_last=(accelerator.mixed_precision == \"fp8\"),\n",
    "    )\n",
    "\n",
    "    return train_dataloader, eval_dataloader\n",
    "\n",
    "\n",
    "#def training_function(config, args):\n",
    "def training_function():\n",
    "    # Initialize accelerator\n",
    "    #accelerator = Accelerator(cpu=args.cpu, mixed_precision=args.mixed_precision)\n",
    "    accelerator = Accelerator()\n",
    "    # Sample hyper-parameters for learning rate, batch size, seed and a few other HPs\n",
    "    lr = config[\"lr\"]\n",
    "    num_epochs = int(config[\"num_epochs\"])\n",
    "    seed = int(config[\"seed\"])\n",
    "    batch_size = int(config[\"batch_size\"])\n",
    "\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "    # If the batch size is too big we use gradient accumulation\n",
    "    gradient_accumulation_steps = 1\n",
    "    if batch_size > MAX_GPU_BATCH_SIZE and accelerator.distributed_type != DistributedType.TPU:\n",
    "        gradient_accumulation_steps = batch_size // MAX_GPU_BATCH_SIZE\n",
    "        batch_size = MAX_GPU_BATCH_SIZE\n",
    "\n",
    "    set_seed(seed)\n",
    "    train_dataloader, eval_dataloader = get_dataloaders(accelerator, batch_size)\n",
    "    # Instantiate the model (we build the model here so that the seed also control new weights initialization)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", return_dict=True)\n",
    "\n",
    "    # We could avoid this line since the accelerator is set with `device_placement=True` (default value).\n",
    "    # Note that if you are placing tensors on devices manually, this line absolutely needs to be before the optimizer\n",
    "    # creation otherwise training will not work on TPU (`accelerate` will kindly throw an error to make us aware of that).\n",
    "    model = model.to(accelerator.device)\n",
    "    # Instantiate optimizer\n",
    "    optimizer = AdamW(params=model.parameters(), lr=lr)\n",
    "\n",
    "    # Instantiate scheduler\n",
    "    num_training_steps= (len(train_dataloader) * num_epochs) // gradient_accumulation_steps\n",
    "    #print(\"num_training_steps: \", num_training_steps)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        #num_training_steps=(len(train_dataloader) * num_epochs) // gradient_accumulation_steps,\n",
    "        num_training_steps = num_training_steps,\n",
    "    )\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
    "    # prepare method.\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )\n",
    "     \n",
    "    num_training_steps = num_epochs * len(train_dataloader)  // gradient_accumulation_steps\n",
    "    #print(\"after accelerator prepared, num_training_steps: \", num_training_steps)\n",
    "    #print(\"length of train_dataloader: \", len(train_dataloader)) \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    \n",
    "    # Now we train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
    "            #batch.to(accelerator.device)\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            #if step % gradient_accumulation_steps == 0:\n",
    "            if (step+1) % gradient_accumulation_steps == 0:\n",
    "                #print(step+1)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
    "            #batch.to(accelerator.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            references = batch[\"labels\"]\n",
    "            #predictions, references = accelerator.gather_for_metrics((predictions, references))\n",
    "            metric.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "            )\n",
    "\n",
    "        eval_metric = metric.compute()\n",
    "        # Use accelerator.print to print only on the main process.\n",
    "        #print(f\"epoch {epoch}:\", eval_metric)\n",
    "        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n",
    "\n",
    "#config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 16}\n",
    "config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 8}\n",
    "#training_function(config, args)\n",
    "\n",
    "#notebook_launcher(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=1)\n",
    "        \n",
    "#def main():\n",
    "#    parser = argparse.ArgumentParser(description=\"Simple example of training script.\")\n",
    "#    parser.add_argument(\n",
    "#        \"--mixed_precision\",\n",
    "#        type=str,\n",
    "#        default=None,\n",
    "#        choices=[\"no\", \"fp16\", \"bf16\", \"fp8\"],\n",
    "#        help=\"Whether to use mixed precision. Choose\"\n",
    "#        \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n",
    "#        \"and an Nvidia Ampere GPU.\",\n",
    "#    )\n",
    "#    parser.add_argument(\"--cpu\", action=\"store_true\", help=\"If passed, will train on the CPU.\")\n",
    "#    args = parser.parse_args()\n",
    "#    config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 16}\n",
    "#    training_function(config, args)\n",
    "#\n",
    "#\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e0527518-f33b-4f14-a6be-f6eb36b9363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/dp/bin/python: No module named bitsandbytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#!python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e806bcbb-0e97-41d8-a982-ca9a990113d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "Namespace(mixed_precision=None, cpu=False)\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Map: 100%|███████████████████████████| 408/408 [00:00<00:00, 6843.57 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "num_training_steps: 1374\n",
      "After accelerator prepared, num_training_steps:  1374\n",
      "  0%|                                                  | 0/1374 [00:00<?, ?it/s]/scratch/qualis/miniconda3/envs/dp/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|█████████████▏                          | 455/1374 [00:15<00:32, 28.56it/s]epoch 0: {'accuracy': 0.8137254901960784, 'f1': 0.8538461538461538}\n",
      "/scratch/qualis/miniconda3/envs/dp/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n",
      " 67%|██████████████████████████▋             | 916/1374 [00:31<00:15, 28.63it/s]epoch 1: {'accuracy': 0.8700980392156863, 'f1': 0.9087779690189329}\n",
      "/scratch/qualis/miniconda3/envs/dp/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████▉| 1371/1374 [00:48<00:00, 29.19it/s]epoch 2: {'accuracy': 0.8651960784313726, 'f1': 0.9043478260869565}\n",
      "100%|███████████████████████████████████████| 1374/1374 [00:48<00:00, 28.26it/s]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch nlp_example.py # it detects the # of proceeses available and set it to num_processes.\n",
    "#!accelerate launch --num_processes=2 nlp_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15b8a5-a812-464e-90d2-1e40b14b9dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5f666-320f-410c-933e-e58a48279013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
