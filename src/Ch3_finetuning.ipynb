{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc273cc-d8fe-406b-9153-7e862c0a6486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 1])}\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 12, None]\n",
      "[None, 0, 1, 2, 3, 4, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "## Í∞êÏÑ±Î∂ÑÏÑù ÏòàÏ†ú, 2Í∞úÏùò Î¨∏Ïû•, 2Í∞úÏùò Î†àÏù¥Î∏î\n",
    "\n",
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 2Ïû•Ïùò ÏòàÏ†úÏôÄ ÎèôÏùºÌï©ÎãàÎã§.\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# Í∞êÏÑ± Î∂ÑÏÑùÏùÑ ÏúÑÌïú Î¨∏Ïû• 2Í∞ú ÏûÖÎ†•\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# ÏÉàÎ°≠Í≤å Ï∂îÍ∞ÄÎêú ÏΩîÎìúÏûÖÎãàÎã§. positive label 2Í∞ú Ï∂îÍ∞Ä\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "#print(tokenizer)\n",
    "print(batch)\n",
    "print(batch.word_ids(0))\n",
    "print(batch.word_ids(1))\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d776cda7-50dd-473e-87ff-42fde8b07179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 0]), 'input_ids': tensor([[ 101, 2002, 3005,  102],\n",
      "        [ 101, 1045, 2046,  102]]), 'attention_mask': tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]])}\n",
      "DefaultDataCollator(return_tensors=[{'input_ids': [101, 2002, 3005, 102], 'label': 1, 'attention_mask': [0, 0, 0, 0]}, {'input_ids': [101, 1045, 2046, 102], 'label': 0, 'attention_mask': [0, 0, 0, 0]}])\n",
      "DataCollatorWithPadding(tokenizer=[{'input_ids': [101, 2002, 3005], 'label': 1, 'attention_mask': [0, 0, 0]}, {'input_ids': [101, 1045, 2046, 102], 'label': 0, 'attention_mask': [0, 0, 0, 0]}], padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')\n"
     ]
    }
   ],
   "source": [
    "## transformers.default_data_collator method ÏòàÏ†ú\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "# Load a pre-trained text classification model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Prepare some sample training examples\n",
    "train_features = [\n",
    "    {\"input_ids\": [101, 2002, 3005, 102], \"label\": 1, \"attention_mask\": [0, 0, 0, 0]},\n",
    "    {\"input_ids\": [101, 1045, 2046, 102], \"label\": 0, \"attention_mask\": [0, 0, 0, 0]},\n",
    "    # ... more examples\n",
    "]\n",
    "\n",
    "# Use the default data collator with padding\n",
    "data_collator = transformers.default_data_collator\n",
    "\n",
    "# Prepare a batch of data for training\n",
    "batch = data_collator(train_features)\n",
    "\n",
    "# Check the contents of the batched tensors\n",
    "print(batch)  # Output: {'input_ids': tensor(...), 'attention_mask': tensor(...), 'labels': tensor(...)}\n",
    "#{'labels': tensor([1, 0]), 'input_ids': tensor([[ 101, 2002, 3005,  102],\n",
    "#        [ 101, 1045, 2046,  102]]), 'attention_mask': tensor([[0, 0, 0, 0],\n",
    "#        [0, 0, 0, 0]])}\n",
    "\n",
    "\n",
    "# Now you can use this batch to train your model efficiently!\n",
    "\n",
    "# Use the default data collator with padding\n",
    "data_collator = transformers.DefaultDataCollator\n",
    "\n",
    "# Prepare a batch of data for training\n",
    "batch = data_collator(train_features)\n",
    "\n",
    "# Check the contents of the batched tensors\n",
    "print(batch)  # Output: {'input_ids': tensor(...), 'attention_mask': tensor(...), 'labels': tensor(...)}\n",
    "#DefaultDataCollator(return_tensors=[{'input_ids': [101, 2002, 3005, 102], 'label': 1, 'attention_mask': [0, 0, 0, 0]}, \n",
    "#                                    {'input_ids': [101, 1045, 2046, 102], 'label': 0, 'attention_mask': [0, 0, 0, 0]}])\n",
    "\n",
    "train_features = [\n",
    "    {\"input_ids\": [101, 2002, 3005], \"label\": 1, \"attention_mask\": [0, 0, 0]},\n",
    "    {\"input_ids\": [101, 1045, 2046, 102], \"label\": 0, \"attention_mask\": [0, 0, 0, 0]},\n",
    "    # ... more examples\n",
    "]\n",
    "\n",
    "# Use the default data collator with padding\n",
    "data_collator = transformers.DataCollatorWithPadding\n",
    "\n",
    "# Prepare a batch of data for training\n",
    "batch = data_collator(train_features)\n",
    "\n",
    "# Check the contents of the batched tensors\n",
    "print(batch)  # Output: {'input_ids': tensor(...), 'attention_mask': tensor(...), 'labels': tensor(...)}\n",
    "#DataCollatorWithPadding(tokenizer=[{'input_ids': [101, 2002, 3005], 'label': 1, 'attention_mask': [0, 0, 0]}, \n",
    "#                                   {'input_ids': [101, 1045, 2046, 102], 'label': 0, 'attention_mask': [0, 0, 0, 0]}], \n",
    "#                        padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff90a516-e97a-451f-91f5-be70acfcdd26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "{'input_ids': tensor([[  101,  1000,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "         12172,  2607,  2026,  2878,  2166,  1012,  1000,  1010,  1000,  2023,\n",
      "          2607,  2003,  6429,   999,  1000,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'labels': tensor([1])}\n",
      "['[CLS]', '\"', 'i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.', '\"', ',', '\"', 'this', 'course', 'is', 'amazing', '!', '\"', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 2Ïû•Ïùò ÏòàÏ†úÏôÄ ÎèôÏùº. 1Í∞úÏùò Î¨∏Ïû•ÏúºÎ°ú Ìï¥ÏÑù, 1Í∞úÏùò Î†àÏù¥Î∏î, Í∞êÏÑ± Î∂ÑÏÑù??\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = f\"\"\"\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\" \"\"\" \n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# ÏÉàÎ°≠Í≤å Ï∂îÍ∞ÄÎêú ÏΩîÎìúÏûÖÎãàÎã§.\n",
    "batch[\"labels\"] = torch.tensor([1])\n",
    "\n",
    "print(tokenizer)\n",
    "print(batch)\n",
    "print(batch.tokens())\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b60721-77f2-4e2e-9066-c74947bfc36f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 2023, 2607, 2003, 6429, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}\n",
      "['[CLS]', 'i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.', '[SEP]', 'this', 'course', 'is', 'amazing', '!', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 12, None, 0, 1, 2, 3, 4, None]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 2Ïû•Ïùò ÏòàÏ†úÏôÄ ÎèôÏùºÌï©ÎãàÎã§. \n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# sequences = f\"\"\"\n",
    "#    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "#    \"This course is amazing!\" \"\"\" \n",
    "\n",
    "#batch = tokenizer(\"I've been waiting for a HuggingFace course my whole life.\", \n",
    "#                  \"This course is amazing!\",\n",
    "#                  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "batch = tokenizer(\"I've been waiting for a HuggingFace course my whole life.\", \n",
    "                  \"This course is amazing!\",\n",
    "                  padding=True, truncation=True)\n",
    "\n",
    "# ÏÉàÎ°≠Í≤å Ï∂îÍ∞ÄÎêú ÏΩîÎìúÏûÖÎãàÎã§.\n",
    "#batch[\"labels\"] = torch.tensor([1])\n",
    "batch[\"labels\"] = 1\n",
    "\n",
    "print(tokenizer)\n",
    "print(batch)\n",
    "print(batch.tokens())\n",
    "print(batch.word_ids(0))\n",
    "\n",
    "#optimizer = AdamW(model.parameters())\n",
    "#loss = model(**batch).loss\n",
    "#loss.backward()\n",
    "#optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4c95be-59c5-4abc-9b3d-1391155abea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f8869c-3f4c-4f9d-9c36-b1a8f6d7634e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102,  2023,  2607,  2003,  6429,\n",
      "           999,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([batch[\"input_ids\"]])\n",
    "token_type_ids = torch.tensor([batch[\"token_type_ids\"]])\n",
    "attention_mask = torch.tensor([batch[\"attention_mask\"]])\n",
    "labels = torch.tensor([batch[\"labels\"]])\n",
    "print(input_ids)\n",
    "print(token_type_ids)\n",
    "print(attention_mask)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9858af-b42b-4d38-8704-66df0b67bef1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1183, -0.0089]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102,  2023,  2607,  2003,  6429,\n",
      "           999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "#loss = model(inputs_ids = inputs_ids, \n",
    "#             token_type_ids = token_type_ids,\n",
    "#             attention_mask = attention_mask,\n",
    "#             labels = labels)\n",
    "output = model(input_ids)\n",
    "print(output)\n",
    "\n",
    "batch = {}\n",
    "batch[\"input_ids\"]=input_ids\n",
    "batch[\"token_type_ids\"]=token_type_ids\n",
    "batch[\"attention_mask\"]=attention_mask\n",
    "batch[\"labels\"]=labels\n",
    "\n",
    "print(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e299e54d-e420-4137-8371-1f6d33b2af88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8890e5-fc53-4abc-a6a1-bd555e698750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa6924-d2e4-4647-8568-d94abff243e1",
   "metadata": {},
   "source": [
    "**MRPC The Microsoft Research Paraphrase Corpus** (Dolan & Brockett, 2005) is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for **whether\r\n",
    "the sentences in the pair are semantically equivalen**t. Because the classes are imbalanced (68%\r\n",
    "positive), we follow common practice and report both accuracy and F1 score.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50672cb9-72a7-4fed-b667-e160d6aadcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f9eb657-b3c1-4bdb-b3d4-67f93c29038d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2cb88d-b843-432c-961e-371193db17f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "316db474-e489-4344-8948-e4afba1d872e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab6a54f7-51db-4704-acdd-3f4b03e73024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[ 101, 2023, 2003, 1996, 2034, 6251, 1012,  102, 2023, 2003, 1996, 2117,\n",
      "         2028, 1012,  102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102],\n",
      "               [101, 2023, 2003, 1996, 2117, 2028, 1012, 102],\n",
      "               [101, 2023, 2003, 1996, 2034, 6251, 1012, 102],\n",
      "               [101, 2023, 2003, 1996, 2117, 2028, 1012, 102]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102],\n",
      "               [101, 2023, 2003, 1996, 2117, 2028, 1012, 102]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[ 101, 2023, 2003, 1996, 2034, 6251, 1012,  102],\n",
      "        [ 101, 2023, 2003, 1996, 2117, 2028, 1012,  102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "input = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "print(input)\n",
    "\n",
    "from pprint import pprint\n",
    "input = tokenizer(\"This is the first sentence.\", \"This is the second one.\", return_tensors=\"pt\")\n",
    "pprint(input)\n",
    "\n",
    "input = tokenizer([\"This is the first sentence.\", \"This is the second one.\", \"This is the first sentence.\", \"This is the second one.\"] )\n",
    "pprint(input)\n",
    "\n",
    "input = tokenizer([\"This is the first sentence.\", \"This is the second one.\"])\n",
    "pprint(input)\n",
    "\n",
    "input = tokenizer([\"This is the first sentence.\", \"This is the second one.\"], return_tensors=\"pt\")\n",
    "pprint(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de904f16-a240-44ab-bcf9-332c5adbb2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3c5f716-d169-4685-9475-cb84903348d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# fill paddings for the 'input_ids', 'token_type_ids', 'attention_mask'\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d3e1f44-7429-43c8-82ab-08ee9e011a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19615668-b57e-495e-882b-b98b0b6e85f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28c84397-8564-4975-90d2-1be62c82aeaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': ['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\"], 'sentence2': ['Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\"], 'label': [1, 0], 'idx': [0, 1], 'input_ids': [[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], [101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b00d4134-5d59-4842-8b77-cadcd825f2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21d4458d-f9db-4d87-bd44-75e1e60aa4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "280c977e-15cd-417b-be8b-e21c669ef911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.39.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "472d72d0-8276-4bae-8719-f905c071ac34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.523400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.296000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3398694084793567, metrics={'train_runtime': 51.9027, 'train_samples_per_second': 212.012, 'train_steps_per_second': 26.53, 'total_flos': 405114969714960.0, 'train_loss': 0.3398694084793567, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32e08d9b-46c0-4be1-8bed-40f1412fa783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26a333ba-fd68-47fc-81d7-ddd938321e78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.816679   3.3098261]\n",
      " [ 3.1239483 -2.9603717]\n",
      " [ 0.4028756 -0.6478887]\n",
      " [-3.7489088  3.2260523]\n",
      " [ 2.8513675 -2.8319807]]\n",
      "[1 0 0 1 0]\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0][:5])\n",
    "print(predictions.label_ids[:5])\n",
    "print(type(predictions.label_ids[:5][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00807672-a2e6-4168-8feb-b9db4f4e8e90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"validation\"][\"label\"][:5]\n",
    "type(tokenized_datasets[\"validation\"][\"label\"][:5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b07657e-de12-426f-aed4-1af456009293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2)\n",
      "<class 'numpy.ndarray'>\n",
      "(408,)\n",
      "<class 'numpy.ndarray'>\n",
      "(408,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "print(predictions.predictions.shape)\n",
    "print(type(preds))\n",
    "print(preds.shape)\n",
    "print(type(predictions.label_ids))\n",
    "print(predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e77471ca-a54a-4bb2-9c0d-c6377f953840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67c4d5c8-4fb6-48b8-94e4-4f14114fbbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27072/1543290331.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"mrpc\")\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8480392156862745, 'f1': 0.8959731543624161}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c81dfdd0-ead6-46c2-b4a6-099b26f46893",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(408,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8480392156862745, 'f1': 0.8959731543624161}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "\n",
    "labels = tokenized_datasets[\"validation\"][\"label\"]\n",
    "labels = np.array(labels)\n",
    "print(type(labels))\n",
    "print(labels.shape)\n",
    "metric.compute(predictions=preds, references=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3eb0d-56e2-4f25-9880-3bb7c097df67",
   "metadata": {},
   "source": [
    "Î™®Îç∏ Ìó§ÎìúÎ•º Î¨¥ÏûëÏúÑÎ°ú Ï¥àÍ∏∞ÌôîÌïòÎ©¥ Í≥ÑÏÇ∞Îêú Î©îÌä∏Î¶≠Ïù¥ Î≥ÄÍ≤ΩÎê† Ïàò ÏûàÏúºÎØÄÎ°ú Ï†ïÌôïÌïú Í≤∞Í≥ºÎäî Îã§Î•º Ïàò ÏûàÏäµÎãàÎã§. Ïó¨Í∏∞ÏÑúÎäî Î™®Îç∏Ïù¥ Í≤ÄÏ¶ù ÏßëÌï©ÏóêÏÑú 86.76%Ïùò Ï†ïÌôïÎèÑ(accuracy)ÏôÄ 90.69Ïùò F1 Ï†êÏàòÎ•º Í∞ÄÏßÄÍ≥† ÏûàÏùåÏùÑ Ïïå Ïàò ÏûàÏäµÎãàÎã§(Ïó¨Îü¨Î∂ÑÏùò Í≤∞Í≥ºÍ∞íÍ≥ºÎäî Îã§Î•º ÏàòÎèÑ ÏûàÏäµÎãàÎã§.). Ïù¥Îäî GLUE Î≤§ÏπòÎßàÌÅ¨Ïùò MRPC Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÎåÄÌïú ÏòàÏ∏° Í≤∞Í≥ºÎ•º ÌèâÍ∞ÄÌïòÎäîÎç∞ ÏÇ¨Ïö©ÎêòÎäî Îëê Í∞ÄÏßÄ Î©îÌä∏Î¶≠(metrics)ÏûÖÎãàÎã§. BERT ÎÖºÎ¨∏Ïùò ÌÖåÏù¥Î∏îÏùÄ Í∏∞Î≥∏ Î™®Îç∏Ïóê ÎåÄÌï¥ F1 Ï†êÏàò 88.9Î•º Î≥¥Í≥†ÌñàÏäµÎãàÎã§. Ìï¥Îãπ ÎÖºÎ¨∏ÏóêÏÑú ÏÇ¨Ïö©Îêú Î™®Îç∏ÏùÄ ÏÜåÎ¨∏Ïûê Î™®Îç∏(uncased model)Ïù¥ÏóàÍ≥† Ïó¨Í∏∞ÏÑúÎäî ÎåÄÏÜåÎ¨∏Ïûê Íµ¨Î∂Ñ Î™®Îç∏(cased model)ÏùÑ ÌôúÏö©ÌñàÏúºÎØÄÎ°ú ÏÑ±Îä• Ï∞®Ïù¥Í∞Ä Î∞úÏÉùÌïòÍ≥† ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "ÏßÄÍ∏àÍπåÏßÄ ÏÑ§Î™ÖÌïú Î™®Îì† Í≤ÉÏùÑ Ìï®Íªò Ï¢ÖÌï©ÌïòÎ©¥ compute_metrics() Ìï®ÏàòÎ•º ÏñªÏùÑ Ïàò ÏûàÏäµÎãàÎã§:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ea89c4b-bc77-4e7a-b217-278b62acb44c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "428b2cfe-1044-45a0-9db2-25eb5bdd9391",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8480392156862745}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04187ac3-afe7-4b23-a265-817ae27627d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8480392156862745,\n",
       " 'f1': 0.8959731543624161,\n",
       " 'precision': 0.8422712933753943,\n",
       " 'recall': 0.956989247311828}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "clf_metrics.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3780a41-ddcb-4ee1-b0b1-a1c7497cc8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "def compute_metrics(eval_preds):\n",
    "    #metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    #print(logits.shape, labels.shape)  #(408,2), (408,)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "871b9cf4-2b46-4fd9-9f0a-0a76758a3089",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.363859</td>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.885609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.422261</td>\n",
       "      <td>0.855392</td>\n",
       "      <td>0.896309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.623663</td>\n",
       "      <td>0.850490</td>\n",
       "      <td>0.897822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.33511093516408663, metrics={'train_runtime': 53.5323, 'train_samples_per_second': 205.558, 'train_steps_per_second': 25.723, 'total_flos': 405114969714960.0, 'train_loss': 0.33511093516408663, 'epoch': 3.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f2f65db-aa0f-471f-8e9b-2dee4dbd1457",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.403378</td>\n",
       "      <td>0.840686</td>\n",
       "      <td>0.888508</td>\n",
       "      <td>0.851974</td>\n",
       "      <td>0.928315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>0.454073</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.880546</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.622920</td>\n",
       "      <td>0.855392</td>\n",
       "      <td>0.898451</td>\n",
       "      <td>0.864238</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3681085956382336, metrics={'train_runtime': 53.3967, 'train_samples_per_second': 206.08, 'train_steps_per_second': 25.788, 'total_flos': 405114969714960.0, 'train_loss': 0.3681085956382336, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "#metric = evaluate.load(\"accuracy\")\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    #metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85407989-f9f0-49fc-aee8-cab8b1879fc9",
   "metadata": {},
   "source": [
    "### Ï†ÑÏ≤¥ ÌïôÏäµ (Full Training)\n",
    "Ïù¥Ï†ú Trainer ÌÅ¥ÎûòÏä§Î•º ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÍ≥† Ïù¥Ï†Ñ ÏÑπÏÖòÏóêÏÑú ÌñàÎçò Í≤ÉÍ≥º ÎèôÏùºÌïú Í≤∞Í≥ºÎ•º ÏñªÎäî Î∞©Î≤ïÏùÑ ÏÇ¥Ìé¥Î≥¥Í≤†ÏäµÎãàÎã§. Îã§Ïãú ÎßêÌïòÏßÄÎßå, ÏÑπÏÖò 2ÏóêÏÑú Ïù¥ÎØ∏ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨Î•º ÏôÑÎ£åÌñàÎã§Í≥† Í∞ÄÏ†ïÌï©ÎãàÎã§. Ïù¥Î≤à ÏÑπÏÖòÏùÑ Í≥µÎ∂ÄÌï† Îïå ÌïÑÏöîÌïú Î™®Îì† ÏûëÏóÖÏùÑ ÏàòÌñâÌïòÎäî ÏΩîÎìúÎäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4025e61-77d8-4c29-b680-221e5cf116a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True) #return_type = \"pt\"\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d52e4-21e7-4be8-b0fa-b9caf9d8da13",
   "metadata": {},
   "source": [
    "### ÌïôÏäµÏùÑ ÏúÑÌïú Ï§ÄÎπÑ\n",
    "Ïã§Ï†úÎ°ú ÌïôÏäµ Î£®ÌîÑ(training loop)Î•º ÏûëÏÑ±ÌïòÍ∏∞ Ï†ÑÏóê Î™á Í∞ÄÏßÄ Í∞ùÏ≤¥Î•º Ï†ïÏùòÌï¥Ïïº Ìï©ÎãàÎã§. Ï≤´ Î≤àÏß∏Îäî Î∞∞Ïπò(batch)Î•º Î∞òÎ≥µÌïòÎäî Îç∞ ÏÇ¨Ïö©Ìï† dataloadersÏûÖÎãàÎã§. Í∑∏Îü¨ÎÇò Ïù¥ dataloadersÎ•º Ï†ïÏùòÌïòÍ∏∞ Ï†ÑÏóê TrainerÍ∞Ä ÏûêÎèôÏúºÎ°ú ÏàòÌñâÌïú Î™á Í∞ÄÏßÄ ÏûëÏóÖÏùÑ ÏßÅÏ†ë Ï≤òÎ¶¨ÌïòÍ∏∞ ÏúÑÌï¥ tokenized_datasetsÏóê ÏïΩÍ∞ÑÏùò ÌõÑÏ≤òÎ¶¨Î•º Ï†ÅÏö©Ìï¥Ïïº Ìï©ÎãàÎã§. Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Îã§ÏùåÏùÑ ÏàòÌñâÌï¥Ïïº Ìï©ÎãàÎã§:\n",
    "\n",
    "- Î™®Îç∏Ïù¥ ÌïÑÏöîÎ°ú ÌïòÏßÄ ÏïäÎäî Í∞íÏù¥ Ï†ÄÏû•Îêú Ïó¥(columns)ÏùÑ Ï†úÍ±∞Ìï©ÎãàÎã§. (sentence1, sentence2 Îì±)\n",
    "\n",
    "- Ïó¥ Î†àÏù¥Î∏î(column label)Ïùò Ïù¥Î¶ÑÏùÑ labelsÎ°ú Î∞îÍøâÎãàÎã§. Ïù¥Îäî Î™®Îç∏Ïù¥ labelsÎùºÎäî Ïù¥Î¶ÑÏúºÎ°ú Îß§Í∞úÎ≥ÄÏàòÎ•º Î∞õÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§.\n",
    "\n",
    "- ÌååÏù¥Ïç¨ Î¶¨Ïä§Ìä∏ ÎåÄÏã† PyTorch ÌÖêÏÑú(tensors)Î•º Î∞òÌôòÌïòÎèÑÎ°ù datasetsÏùò ÌòïÏãùÏùÑ ÏÑ§Ï†ïÌï©ÎãàÎã§.\n",
    "\n",
    "tokenized_datasetsÏóêÎäî Ïù¥Îü¨Ìïú ÏûëÏóÖÏùÑ ÏúÑÌïú Î≥ÑÎèÑÏùò Î©îÏÑúÎìúÎì§Ïù¥ Ï°¥Ïû¨Ìï©ÎãàÎã§:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5ee5804-a39f-49d3-be20-dd4f92de53c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b79a3b4f-722f-472b-b655-26f867cfcc5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6baa076b-fe56-4299-add3-d48e7bbd0db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57b0e8d9-2f68-4a98-968c-bf1413b36bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6550846-fb3f-452d-80e9-4a9c34e97085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_train_datasets = tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1913d8d1-18a2-45e1-a844-a5738dc221a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_datasets.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc77f83c-e99f-4590-b5c8-ce2ddc758563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5d18407-3206-4b7e-b172-4ae4f419e619",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_datasets = tokenized_datasets[\"train\"]\n",
    "tokenized_train_datasets.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65d12622-9187-4865-a9ea-89e64e71f902",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor([1, 0]), 'input_ids': [tensor([  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
      "         2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
      "         3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
      "         1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
      "         2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102]), tensor([  101,  9805,  3540, 11514,  2050,  3079, 11282,  2243,  1005,  1055,\n",
      "         2077,  4855,  1996,  4677,  2000,  3647,  4576,  1999,  2687,  2005,\n",
      "         1002,  1016,  1012,  1019,  4551,  1012,   102,  9805,  3540, 11514,\n",
      "         2050,  4149, 11282,  2243,  1005,  1055,  1999,  2786,  2005,  1002,\n",
      "         6353,  2509,  2454,  1998,  2853,  2009,  2000,  3647,  4576,  2005,\n",
      "         1002,  1015,  1012,  1022,  4551,  1999,  2687,  1012,   102])], 'token_type_ids': [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])], 'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_datasets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff210df2-a414-49e5-8cf1-9e85c69664b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e5f7d88-5bd4-4ff3-9204-565dcaecb9a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad951df-bec9-454b-8279-1f2dc53717d1",
   "metadata": {},
   "source": [
    "ÏúÑÏóêÏÑú Î≥¥ÎìØÏù¥ Í≤∞Í≥ºÏ†ÅÏúºÎ°ú tokenized_datasetsÏóêÎäî Î™®Îç∏Ïù¥ ÌóàÏö©ÌïòÎäî columnsÎßå Ï°¥Ïû¨Ìï®ÏùÑ Ïïå Ïàò ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "Ïù¥Ï†ú Ïù¥ ÏûëÏóÖÏù¥ ÏôÑÎ£åÎêòÏóàÏúºÎØÄÎ°ú dataloaderÎ•º ÏâΩÍ≤å Ï†ïÏùòÌï† Ïàò ÏûàÏäµÎãàÎã§:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af07d9ec-72d2-4da5-9f83-f94d9478b2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], # tensor\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator, # automatically padding \n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159fd2a5-06a2-45b7-9e10-60455b8d27a4",
   "metadata": {},
   "source": [
    "Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨Ïóê Ïò§Î•òÍ∞Ä ÏóÜÎäîÏßÄ Îπ†Î•¥Í≤å ÌôïÏù∏ÌïòÍ∏∞ ÏúÑÌï¥ Îã§ÏùåÍ≥º Í∞ôÏù¥ Î∞∞Ïπò(batch)Î•º Í≤ÄÏÇ¨Ìï† Ïàò ÏûàÏäµÎãàÎã§:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3525ac1a-07ca-49f6-b1b1-218d540946d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 81]),\n",
       " 'token_type_ids': torch.Size([8, 81]),\n",
       " 'attention_mask': torch.Size([8, 81]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f22dee3f-99f9-4f35-bd11-ec862ec59f70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  6736, 11582,  2008,  1996,  1000, 25710,  2064,  9033,\n",
       "          6199,  2083, 15288,  1999,  1996,  5894,  1010,  2105,  2422, 17407,\n",
       "          2030,  2105,  5894,  4599,  1012,  1000,   102,  1996, 25710,  2064,\n",
       "          9033,  6199,  2083, 15288,  1999,  1996,  5894,  1010,  2105,  2422,\n",
       "         17407,  2030,  2105,  5894,  4599,  1010,  2061, 15288,  2323,  2022,\n",
       "         10203,  1010,  2027, 16755,  1012,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1996,  3291,  3497,  2097,  2812,  6149,  3512,  3431,  2077,\n",
       "          1996, 10382,  4170,  4627,  3909,  2153,  1012,   102,  2002,  2056,\n",
       "          1996,  3291,  3791,  2000,  2022, 13371,  2077,  1996,  2686, 10382,\n",
       "          4170,  2003,  5985,  2000,  4875,  2153,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  7641,  3373,  2008, 18766,  2075,  1037,  4126,  2002,  2106,\n",
       "          2025, 10797,  2001,  1996,  2069,  2126,  2041,  1010,  9482,  2056,\n",
       "          1012,   102,  2000,  1996, 10363,  7641,  1010,  9482,  2056,  1010,\n",
       "         18766,  2075,  1037,  4126,  2002,  2106,  2025, 10797,  2246,  2066,\n",
       "          1996,  2069,  2041,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1996,  6957,  8617,  2274,  3514, 25416, 26455,  3111,  1998,\n",
       "          2062,  2084,  1016,  1010,  2531,  8110,  3703,  1999,  3607,  1998,\n",
       "          5924,  1012,   102, 28286,  2243,  1011, 17531,  2038,  2416,  3514,\n",
       "          5155,  3197,  1010,  2274, 25416, 26455,  3111,  1998,  1016,  1010,\n",
       "          2531,  8110,  3703,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1996,  2820,  2005,  4425,  2968,  2056,  2049,  5950,  1997,\n",
       "          2512,  1011,  5814,  4023,  3123,  2000,  2753,  1012,  1021,  2013,\n",
       "          4700,  1012,  1023,  1999,  2233,  1012,   102,  1996,  2820,  2005,\n",
       "          4425,  2968,  2056,  2049,  5950,  1997,  2512,  1011,  5814,  4023,\n",
       "          3123,  2000,  2753,  1012,  1021,  1010, 16361,  2067,  2013,  1037,\n",
       "          2028,  1011,  3204, 21963,  1999,  2233,  2012,  4700,  1012,  1023,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  1037,  2918,  9563,  2007,  1037,  1016,  1012,  1014,  5603,\n",
       "          2480, 13420,  8292,  3917,  2239, 13151,  1010, 11899,  2213, 27507,\n",
       "          1997,  3638,  1010,  1037,  2871,  2290,  1011, 24880,  2524,  3298,\n",
       "          1010,  1998,  1037,  3729,  1011, 17083,  3298,  5366,  2149,  1002,\n",
       "          5824,  2683,  1012,   102,  1037,  2918,  9563,  2007,  1037,  1016,\n",
       "          1012,  1018,  5603,  2480,  7279, 16398,  1018,  1010, 11899, 14905,\n",
       "          1997,  8223,  1010,  1037,  2871, 18259,  2524,  3298,  1010,  1998,\n",
       "          1037,  3729,  1011, 17083,  3298,  5366,  1002,  6353,  2683,  1012,\n",
       "           102],\n",
       "        [  101,  6746,  2015, 24529,  2497,  4484,  2006,  9857,  2009,  2001,\n",
       "          1000,  6195,  2049,  7047,  1000,  2058,  1996,  5096,  1997,  2049,\n",
       "          2120,  2924,  1997,  2047,  3414,  7506,  1012,   102,  6746,  2015,\n",
       "         24529,  2497,  7483,  4484,  2008,  2009,  2018,  2363, 20723,  2005,\n",
       "          2049,  2120,  2924,  1997,  2047,  3414,  3169,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101,  4001,  2343,  4013, 13657,  2890,  2728,  1040,  1012, 11721,\n",
       "         11715,  1010,  1054,  1011,  8912,  1010,  2056,  2002,  3764,  2007,\n",
       "         22762,  2319,  2025,  2146,  2077,  1996, 11292,  1997,  2436,  5103,\n",
       "          1012,   102,  4001,  2343,  4013, 13657,  2890,  2728,  1040,  1012,\n",
       "         11721, 11715,  1010,  1054,  1011,  8912,  1010,  2056,  2002,  1005,\n",
       "          2222,  3335,  1996,  3099,  1005,  1055,  2942,  3012,  2087,  1012,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([0, 1, 1, 0, 1, 1, 1, 0])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6caf6023-0800-4691-8f7c-d5f32b9a0917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 81]),\n",
       " 'token_type_ids': torch.Size([8, 81]),\n",
       " 'attention_mask': torch.Size([8, 81]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e630cba8-6243-4afe-b3ad-13baa6b2c8eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = {k: v.to(device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f969a30-6f08-48ee-8223-1cafd5073264",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  6736, 11582,  2008,  1996,  1000, 25710,  2064,  9033,\n",
       "           6199,  2083, 15288,  1999,  1996,  5894,  1010,  2105,  2422, 17407,\n",
       "           2030,  2105,  5894,  4599,  1012,  1000,   102,  1996, 25710,  2064,\n",
       "           9033,  6199,  2083, 15288,  1999,  1996,  5894,  1010,  2105,  2422,\n",
       "          17407,  2030,  2105,  5894,  4599,  1010,  2061, 15288,  2323,  2022,\n",
       "          10203,  1010,  2027, 16755,  1012,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  1996,  3291,  3497,  2097,  2812,  6149,  3512,  3431,  2077,\n",
       "           1996, 10382,  4170,  4627,  3909,  2153,  1012,   102,  2002,  2056,\n",
       "           1996,  3291,  3791,  2000,  2022, 13371,  2077,  1996,  2686, 10382,\n",
       "           4170,  2003,  5985,  2000,  4875,  2153,  1012,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  7641,  3373,  2008, 18766,  2075,  1037,  4126,  2002,  2106,\n",
       "           2025, 10797,  2001,  1996,  2069,  2126,  2041,  1010,  9482,  2056,\n",
       "           1012,   102,  2000,  1996, 10363,  7641,  1010,  9482,  2056,  1010,\n",
       "          18766,  2075,  1037,  4126,  2002,  2106,  2025, 10797,  2246,  2066,\n",
       "           1996,  2069,  2041,  1012,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  1996,  6957,  8617,  2274,  3514, 25416, 26455,  3111,  1998,\n",
       "           2062,  2084,  1016,  1010,  2531,  8110,  3703,  1999,  3607,  1998,\n",
       "           5924,  1012,   102, 28286,  2243,  1011, 17531,  2038,  2416,  3514,\n",
       "           5155,  3197,  1010,  2274, 25416, 26455,  3111,  1998,  1016,  1010,\n",
       "           2531,  8110,  3703,  1012,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  1996,  2820,  2005,  4425,  2968,  2056,  2049,  5950,  1997,\n",
       "           2512,  1011,  5814,  4023,  3123,  2000,  2753,  1012,  1021,  2013,\n",
       "           4700,  1012,  1023,  1999,  2233,  1012,   102,  1996,  2820,  2005,\n",
       "           4425,  2968,  2056,  2049,  5950,  1997,  2512,  1011,  5814,  4023,\n",
       "           3123,  2000,  2753,  1012,  1021,  1010, 16361,  2067,  2013,  1037,\n",
       "           2028,  1011,  3204, 21963,  1999,  2233,  2012,  4700,  1012,  1023,\n",
       "           1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  1037,  2918,  9563,  2007,  1037,  1016,  1012,  1014,  5603,\n",
       "           2480, 13420,  8292,  3917,  2239, 13151,  1010, 11899,  2213, 27507,\n",
       "           1997,  3638,  1010,  1037,  2871,  2290,  1011, 24880,  2524,  3298,\n",
       "           1010,  1998,  1037,  3729,  1011, 17083,  3298,  5366,  2149,  1002,\n",
       "           5824,  2683,  1012,   102,  1037,  2918,  9563,  2007,  1037,  1016,\n",
       "           1012,  1018,  5603,  2480,  7279, 16398,  1018,  1010, 11899, 14905,\n",
       "           1997,  8223,  1010,  1037,  2871, 18259,  2524,  3298,  1010,  1998,\n",
       "           1037,  3729,  1011, 17083,  3298,  5366,  1002,  6353,  2683,  1012,\n",
       "            102],\n",
       "         [  101,  6746,  2015, 24529,  2497,  4484,  2006,  9857,  2009,  2001,\n",
       "           1000,  6195,  2049,  7047,  1000,  2058,  1996,  5096,  1997,  2049,\n",
       "           2120,  2924,  1997,  2047,  3414,  7506,  1012,   102,  6746,  2015,\n",
       "          24529,  2497,  7483,  4484,  2008,  2009,  2018,  2363, 20723,  2005,\n",
       "           2049,  2120,  2924,  1997,  2047,  3414,  3169,  1012,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [  101,  4001,  2343,  4013, 13657,  2890,  2728,  1040,  1012, 11721,\n",
       "          11715,  1010,  1054,  1011,  8912,  1010,  2056,  2002,  3764,  2007,\n",
       "          22762,  2319,  2025,  2146,  2077,  1996, 11292,  1997,  2436,  5103,\n",
       "           1012,   102,  4001,  2343,  4013, 13657,  2890,  2728,  1040,  1012,\n",
       "          11721, 11715,  1010,  1054,  1011,  8912,  1010,  2056,  2002,  1005,\n",
       "           2222,  3335,  1996,  3099,  1005,  1055,  2942,  3012,  2087,  1012,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'labels': tensor([0, 1, 1, 0, 1, 1, 1, 0], device='cuda:0')}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bfba77-df03-4c8f-b226-af32181d0d64",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÌòïÌÉú(shapes)Í∞Ä ÏÇ¥Ïßù Îã§Î•º Ïàò ÏûàÎäîÎç∞ Ïù¥Îäî ÌïôÏäµ dataloaderÏóê ÎåÄÌï¥ shuffle=TrueÎ•º ÏÑ§Ï†ïÌïòÍ≥† Î∞∞Ïπò(batch) ÎÇ¥ÏóêÏÑúÏùò ÏµúÎåÄ Í∏∏Ïù¥Î°ú Ìå®Îî©(padding)ÌïòÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§.\n",
    "\n",
    "Ïù¥Ï†ú Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨Í∞Ä ÏôÑÏ†ÑÌûà ÎÅùÎÇ¨ÏäµÎãàÎã§. Í∏∞Í≥ÑÌïôÏäµ Ïã§Î¨¥Ïûê(ML practitioners)Îì§ÏóêÍ≤åÎäî ÎßåÏ°±Ïä§ÎüΩÍ∏∞ÎèÑ ÌïòÍ≤†ÏßÄÎßå ÏùºÎ∂Ä Î™ÖÌôïÌïòÏßÄ ÏïäÏùÄ Î∂ÄÎ∂ÑÎèÑ ÏûàÏùÑ Ïàò ÏûàÍ≤†ÏäµÎãàÎã§. Ïù¥Ï†ú Î™®Îç∏Î°ú ÎèåÏïÑÍ∞Ä Î¥ÖÏãúÎã§. Ïù¥Ï†Ñ ÏÑπÏÖòÏóêÏÑú ÏàòÌñâÌïú Í≤ÉÍ≥º ÎèôÏùºÌïú Î∞©Î≤ïÏúºÎ°ú Î™®Îç∏ÏùÑ Ïù∏Ïä§ÌÑ¥Ïä§Ìôî(instantiate)Ìï©ÎãàÎã§:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32800675-1ad6-4247-8ca3-b196c6f4e238",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75395fa1-5f37-45e6-a4fb-d2cdb5ba1f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(model.config)\n",
    "print(model.config.num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471f4fe-335c-4ce2-a4d5-9fdc8f46c0d0",
   "metadata": {},
   "source": [
    "ÌïôÏäµ Í≥ºÏ†ïÏóêÏÑú Î™®Îì† Í≤ÉÎì§Ïù¥ ÏõêÌôúÌïòÍ≤å ÏßÑÌñâÎê† Ïàò ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÍ∏∞ ÏúÑÌï¥ Î∞∞Ïπò(batch)Î•º Ïù¥ Î™®Îç∏Ïóê ÌïúÎ≤à Ï†ÑÎã¨Ìï¥ Î¥ÖÏãúÎã§:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28555a06-9c8b-43a0-ac06-212fb5b04746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9363, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6608, -0.3372],\n",
      "        [ 0.6689, -0.3575],\n",
      "        [ 0.6599, -0.3340],\n",
      "        [ 0.6668, -0.3561],\n",
      "        [ 0.6559, -0.3264],\n",
      "        [ 0.6782, -0.3260],\n",
      "        [ 0.6623, -0.3201],\n",
      "        [ 0.6589, -0.3497]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "loss: 0.9362912178039551, logits.shape: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs)\n",
    "print(f\"loss: {outputs.loss}, logits.shape: {outputs.logits.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237574bd-3f32-4eb4-9adc-b20ca76e040b",
   "metadata": {},
   "source": [
    "Î™®Îì† ü§óTransformers Î™®Îç∏ÏùÄ Îß§Í∞úÎ≥ÄÏàòÏóê labelsÏù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎã§Î©¥ ÏÜêÏã§(loss)Í≥º Ìï®Íªò logitÍ∞í(batchÎÇ¥ Í∞Å ÏûÖÎ†•Ïóê ÎåÄÌï¥ logitÍ∞íÏù¥ 2Í∞úÏù¥ÎØÄÎ°ú ÌÅ¨Í∏∞Í∞Ä 8 x 2Ïù∏ ÌÖêÏÑú)ÎèÑ Î∞òÌôòÌï©ÎãàÎã§.\n",
    "\n",
    "ÌïôÏäµ Î£®ÌîÑ(training loop)Î•º ÏûëÏÑ±Ìï† Ï§ÄÎπÑÍ∞Ä Í±∞Ïùò ÎêòÏóàÏäµÎãàÎã§! Í∑∏Îü∞Îç∞ ÏïÑÏßÅ ÏµúÏ†ÅÌôî Ìï®Ïàò(optimizer) Î∞è ÌïôÏäµÎ•† Ïä§ÏºÄÏ§ÑÎü¨(learning rate scheduler) ÏßÄÏ†ï ÏûëÏóÖÏù¥ ÎÇ®ÏïÑ ÏûàÏäµÎãàÎã§. Ïó¨Í∏∞ÏÑúÎäî ÏïûÏóêÏÑú Î∞∞Ïö¥ TrainerÏùò Í∏∞Î≥∏ ÏÑ§Ï†ïÏùÑ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©ÌïòÍ≤†ÏäµÎãàÎã§. TrainerÍ∞Ä ÏÇ¨Ïö©ÌïòÎäî ÏµúÏ†ÅÌôî Ìï®ÏàòÎäî AdamWÏù¥Î©∞ Ïù¥Îäî AdamÍ≥º Í±∞Ïùò ÎèôÏùºÌïòÏßÄÎßå weight decay regularizationÏùÑ Ï†ÅÏö©ÌñàÎã§Îäî ÏÇ¨Ïã§Ïóê Ï∞®Ïù¥Í∞Ä ÎÇ©ÎãàÎã§:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8442fce0-3645-41ac-9712-e74c0b151431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a16605-fc77-4fb7-80b4-8d5bf5a8ae3d",
   "metadata": {},
   "source": [
    "ÎßàÏßÄÎßâÏúºÎ°ú, TrainerÏóêÏÑú ÎîîÌè¥Ìä∏Î°ú ÏÇ¨Ïö©ÎêòÎäî ÌïôÏäµÎ•† Ïä§ÏºÄÏ§ÑÎü¨(learning rate scheduler)Îäî ÏµúÎåÄÍ∞í(5e-5)ÏóêÏÑú 0ÍπåÏßÄ ÏÑ†Ìòï Í∞êÏá†(linear decay)Ìï©ÎãàÎã§. Ïù¥Î•º Ï†ÅÏ†àÌïòÍ≤å Ï†ïÏùòÌïòÎ†§Î©¥ Ïö∞Î¶¨Í∞Ä ÏàòÌñâÌï† ÌïôÏäµ Îã®Í≥ÑÏùò ÌöüÏàòÎ•º ÏïåÏïÑÏïº Ìï©ÎãàÎã§. Ïù¥Îäî Ïã§ÌñâÌïòÎ†§Îäî ÏóêÌè¨ÌÅ¨(epochs) ÏàòÏóê ÌïôÏäµ Î∞∞Ïπò(batch)Ïùò Í∞úÏàòÎ•º Í≥±Ìïú Í≤ÉÏûÖÎãàÎã§. ÌïôÏäµ Î∞∞ÏπòÏùò Í∞úÏàòÎäî ÌïôÏäµ dataloaderÏùò Í∏∏Ïù¥ÏôÄ Í∞ôÏäµÎãàÎã§. TrainerÎäî ÎîîÌè¥Ìä∏Î°ú 3Í∞úÏùò ÏóêÌè¨ÌÅ¨(epochs)Î•º ÏÇ¨Ïö©ÌïòÎØÄÎ°ú Îã§ÏùåÏùÑ Îî∞Î¶ÖÎãàÎã§:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a96353c5-bd3b-4c49-a8be-cbe2a4149b48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459\n",
      "1377\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(len(train_dataloader))\n",
    "print(num_training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ac85c70-054c-4bab-83de-1f47009dfb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf2b96-f6a4-4833-93e6-f31bd12e65a1",
   "metadata": {},
   "source": [
    "#### ÌïôÏäµ Î£®ÌîÑ (Training Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "207e0d42-03de-463e-9aea-9a2f42bc24a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "486953d0-d02f-4eb3-984a-c6cfdd619777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ab7b711a05452086e1c800295348ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ba659-ed87-4ae7-9b08-8aea73e1a089",
   "metadata": {},
   "source": [
    "#### ÌèâÍ∞Ä Î£®ÌîÑ (Evaluation Loop)\n",
    "Ïù¥Ï†ÑÏóê ÏàòÌñâÌñàÎçò Í≤ÉÏ≤òÎüº, ü§óDatasets ÎùºÏù¥Î∏åÎü¨Î¶¨ÏóêÏÑú Ï†úÍ≥µÌïòÎäî ÌèâÍ∞Ä Î©îÌä∏Î¶≠(metrics)ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§. Ïö∞Î¶¨Îäî Ïù¥ÎØ∏ metric.compute() Î©îÏÑúÎìúÎ•º ÏÇ¥Ìé¥Î≥¥ÏïòÏßÄÎßå metric.add_batch() Î©îÏÑúÎìúÎ°ú ÌèâÍ∞Ä Î£®ÌîÑ(evaluation loop)Î•º Ïã§ÌñâÌïòÎ©¥ÏÑú Î∞∞Ïπò(batch)Î≥Ñ ÌèâÍ∞Ä Î©îÌä∏Î¶≠(metrics) Í≥ÑÏÇ∞ Í≤∞Í≥ºÎ•º ÎàÑÏ†ÅÌï† Ïàò ÏûàÏäµÎãàÎã§. Î™®Îì† Î∞∞Ïπò(batch)Î•º ÎàÑÏ†ÅÌïòÍ≥† ÎÇòÎ©¥ metric.compute()Î°ú ÏµúÏ¢Ö Í≤∞Í≥ºÎ•º ÏñªÏùÑ Ïàò ÏûàÏäµÎãàÎã§. ÌèâÍ∞Ä Î£®ÌîÑÏóêÏÑú Ïù¥ Î™®Îì† Í≤ÉÏùÑ Íµ¨ÌòÑÌïòÎäî Î∞©Î≤ïÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c022a15-25b3-4f5b-9d4a-ca18d3ffe96f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5882352941176471, 'f1': 0.6363636363636364}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d892bef-6966-4046-9193-771993631376",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5882352941176471, 'f1': 0.6363636363636364}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "249acfb5-dd88-48d2-b4c8-f6de7734233c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5882352941176471,\n",
       " 'f1': 0.6363636363636364,\n",
       " 'precision': 0.8032786885245902,\n",
       " 'recall': 0.5268817204301075}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "#metric = evaluate.load(\"accuracy\")\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    clf_metrics.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "clf_metrics.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3683c-796e-4427-a555-d9cd0894cec3",
   "metadata": {},
   "source": [
    "### ü§óAccelerate ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÇ¨Ïö©Ìïú ÌïôÏäµ Î£®ÌîÑ Í∞ÄÏÜçÌôî\n",
    "ÏïûÏóêÏÑú Ï†ïÏùòÌïú ÌïôÏäµ Î£®ÌîÑ(training loop)Îäî Îã®Ïùº CPU ÎòêÎäî Îã®Ïùº GPUÏóêÏÑú Ï†úÎåÄÎ°ú ÏûëÎèôÌï©ÎãàÎã§. Í∑∏Îü¨ÎÇò ü§óAccelerate ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ Î™á Í∞ÄÏßÄ ÏÑ§Ï†ïÎßå ÌïòÎ©¥ Ïó¨Îü¨ GPU ÎòêÎäî TPUÏóêÏÑú Î∂ÑÏÇ∞ ÌïôÏäµ(distributed training)ÏùÑ ÏàòÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§. ÌïôÏäµ Î∞è Í≤ÄÏ¶ù dataloaderÎ•º ÏÉùÏÑ± Ìïú ÌõÑ, ÌïôÏäµ Î£®ÌîÑ(training loop)Ïùò ÌòïÌÉúÎäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "848c3c64-71f6-4be7-8e7e-c0dc2c813f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44991d444d274830b26136864791bbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07661e74-c615-4cd4-9c40-f06b99738629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543716cc7fca4b0ba009a988799045a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f39d0-a1ae-489d-929f-8e4b4b297278",
   "metadata": {},
   "source": [
    "### Full Training ÏΩîÎìú w/ Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e641b28-ef55-4259-849a-bfc189cc9f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.442705</td>\n",
       "      <td>0.796569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.508943</td>\n",
       "      <td>0.843137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.298500</td>\n",
       "      <td>0.663428</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3.0,\n",
      " 'eval_accuracy': 0.8371014492753623,\n",
      " 'eval_loss': 0.7649620175361633,\n",
      " 'eval_runtime': 2.3493,\n",
      " 'eval_samples_per_second': 734.25,\n",
      " 'eval_steps_per_second': 91.941}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 3668\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 408\n",
    "#    })\n",
    "#    test: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 1725\n",
    "#    })\n",
    "#})\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#        num_rows: 3668\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#        num_rows: 408\n",
    "#    })\n",
    "#    test: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
    "#        num_rows: 1725\n",
    "#    })\n",
    "#})\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã \"label\" ÎòêÎäî \"labels\" Îëò Îã§ ÏûëÎèôÌï®.  \n",
    "#tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    #metric = load_metric(\"glue\", \"mrpc\")\n",
    "    #print(eval_preds)\n",
    "    logits, labels = eval_preds\n",
    "    # print(logits.shape, labels.shape) (408,2) (408,) note that the len(Dataset[\"validation\"])==408\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# the default per_device_train_batch_size is equal to 8\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "#print(training_args) \n",
    "# check out training_args.per_device_train_batch_size\n",
    "# print(training_args.per_device_train_batch_size)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, # based on evaluation_strategy, it evaluate \n",
    ")\n",
    "\n",
    "#print(trainer)\n",
    "trainer.train() # train datasets 3668 * 3 (default epochs) / 8 (default batch size) = 1377 steps\n",
    "\n",
    "#save\n",
    "#trainer.save_model \n",
    "\n",
    "#eval\n",
    "from pprint import pprint\n",
    "metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"]) # test datasets 1725 / 8 = 216 steps \n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6bb8e7-8fca-40fc-99e0-6a19a6b0e2db",
   "metadata": {},
   "source": [
    "### Full Training ÏΩîÎìú w/o Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f9a853c8-73f5-4009-980b-b67db1914803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fb34d6f0c142dfb7cb33d60c212d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8553921568627451, 'f1': 0.8973913043478261}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ ÏÖã Ï†ÅÏû¨\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 3668\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 408\n",
    "#    })\n",
    "#    test: Dataset({\n",
    "#        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "#        num_rows: 1725\n",
    "#    })\n",
    "#})\n",
    "\n",
    "# ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏ checkpoint Ïù¥Î¶Ñ ÏßÄÏ†ï\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "# ÏßÄÏ†ïÎêú ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏ÏóêÏÑú ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ïù∏Ïä§ÌÑ¥Ïä§Ìôî\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ìï®Ïàò ÏÇ¨Ïö©Ïûê Ï†ïÏùòÌôî (sentence1, sentence2 Ïª¨ÎüºÏóê ÎåÄÌï¥ÏÑúÎßå ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï ÏàòÌñâ)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï ÏàòÌñâ\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# Î∞∞Ïπò(batch)Î≥Ñ Ìå®Îî©(padding)ÏùÑ ÏúÑÌïú data collator Ï†ïÏùò\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Î∂àÌïÑÏöîÌïú ÏûÖÎ†• Ïª¨ÎüºÏùÑ Ï†úÍ±∞ÌïòÍ≥† ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏Ïóê ÌïÑÏöîÌïú ÏûÖÎ†•Îßå ÎÇ®ÍπÄ.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖãÏùò label Ïª¨ÎüºÎ™ÖÏùÑ labelsÎ°ú Î≥ÄÍ≤Ω\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Ïú†ÌòïÏùÑ PyTorch tensorÎ°ú Î≥ÄÍ≤Ω\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Î≥ÄÍ≤ΩÎêú Ïª¨Îüº Ï∂úÎ†•\n",
    "print(tokenized_datasets[\"train\"].column_names)\n",
    "\n",
    "\n",
    "# Í∞Å Ï¢ÖÎ•òÎ≥Ñ Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÏÉùÏÑ±\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], \n",
    "                              shuffle=True, \n",
    "                              batch_size=8, \n",
    "                              collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"],\n",
    "                             shuffle=True,\n",
    "                             batch_size=8,\n",
    "                             collate_fn=data_collator)\n",
    "\n",
    "# ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§Ìôî\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# ÏµúÏ†ÅÌôî Ìï®Ïàò Ï†ïÏùò\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# ÏóêÌè¨ÌÅ¨ Í∞úÏàò ÏÑ§Ï†ï\n",
    "num_epochs = 3\n",
    "# ÌïôÏäµ Ïä§ÌÖù Ïàò Í≥ÑÏÇ∞\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# ÌïôÏäµ Ïä§ÏºÄÏ•¥Îü¨ ÏÑ§Ï†ï\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# GPUÎ°ú Î™®Îç∏ÏùÑ Ïù¥Îèô\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ÏßÑÌñâ ÏÉÅÌô©Î∞î Ï†ïÏùò\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# Î™®Îç∏ÏùÑ ÌïôÏäµ Î™®ÎìúÎ°ú Ï†ÑÌôò\n",
    "model.train()\n",
    "# ÌïôÏäµ Î£®ÌîÑ ÏãúÏûë\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # ÌòÑÏû¨ Î∞∞Ïπò Ï§ëÏóêÏÑú ÏûÖÎ†•Í∞íÏùÑ Î™®Îëê GPUÎ°ú Ïù¥Îèô.\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # Î™®Îç∏ Ïã§Ìñâ\n",
    "        outputs = model(**batch)\n",
    "        # ÏÜêÏã§Í∞í Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        loss = outputs.loss\n",
    "        # Ïó≠Ï†ÑÌåå ÏàòÌñâ\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# ÌèâÍ∞Ä Î©îÌä∏Î¶≠ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "# Î™®Îç∏ÏùÑ ÌèâÍ∞Ä Î™®ÎìúÎ°ú Ï†ÑÌôò\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# ÌèâÍ∞Ä Í≤∞Í≥º Í≥ÑÏÇ∞ Î∞è Ï∂úÎ†• \n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad3b1c-c923-40c5-ac6d-7b331b869181",
   "metadata": {},
   "source": [
    "### Full Training w/ evaluate \n",
    "\n",
    "```\n",
    "from datasets import load_metrics \n",
    "metric = load_metric(\"glue\", \"mrpc\") \n",
    "\n",
    "===>\n",
    "\n",
    "import evaluate\n",
    "metric = evaluat.load(\"accuracy\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d98b92df-ef4c-40a6-80e6-2dd78f3b05f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dc85076a0d41dd987bf968dc2c109c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8651960784313726}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ ÏÖã Ï†ÅÏû¨\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "# ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏ checkpoint Ïù¥Î¶Ñ ÏßÄÏ†ï\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "# ÏßÄÏ†ïÎêú ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏ÏóêÏÑú ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ïù∏Ïä§ÌÑ¥Ïä§Ìôî\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ìï®Ïàò ÏÇ¨Ïö©Ïûê Ï†ïÏùòÌôî (sentence1, sentence2 Ïª¨ÎüºÏóê ÎåÄÌï¥ÏÑúÎßå ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï ÏàòÌñâ)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï ÏàòÌñâ\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# Î∞∞Ïπò(batch)Î≥Ñ Ìå®Îî©(padding)ÏùÑ ÏúÑÌïú data collator Ï†ïÏùò\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Î∂àÌïÑÏöîÌïú ÏûÖÎ†• Ïª¨ÎüºÏùÑ Ï†úÍ±∞ÌïòÍ≥† ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏Ïóê ÌïÑÏöîÌïú ÏûÖÎ†•Îßå ÎÇ®ÍπÄ.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖãÏùò label Ïª¨ÎüºÎ™ÖÏùÑ labelsÎ°ú Î≥ÄÍ≤Ω\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Ïú†ÌòïÏùÑ PyTorch tensorÎ°ú Î≥ÄÍ≤Ω\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Î≥ÄÍ≤ΩÎêú Ïª¨Îüº Ï∂úÎ†•\n",
    "print(tokenized_datasets[\"train\"].column_names)\n",
    "\n",
    "\n",
    "# Í∞Å Ï¢ÖÎ•òÎ≥Ñ Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÏÉùÏÑ±\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], \n",
    "                              shuffle=True, \n",
    "                              batch_size=8, \n",
    "                              collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"],\n",
    "                             shuffle=True,\n",
    "                             batch_size=8,\n",
    "                             collate_fn=data_collator)\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "#accelerator = Accelerator()\n",
    "\n",
    "# ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§Ìôî\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# ÏµúÏ†ÅÌôî Ìï®Ïàò Ï†ïÏùò\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "#train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "#    train_dataloader, eval_dataloader, model, optimizer\n",
    "#)\n",
    "\n",
    "# ÏóêÌè¨ÌÅ¨ Í∞úÏàò ÏÑ§Ï†ï\n",
    "num_epochs = 3\n",
    "# ÌïôÏäµ Ïä§ÌÖù Ïàò Í≥ÑÏÇ∞\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# ÌïôÏäµ Ïä§ÏºÄÏ•¥Îü¨ ÏÑ§Ï†ï\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# GPUÎ°ú Î™®Îç∏ÏùÑ Ïù¥Îèô\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ÏßÑÌñâ ÏÉÅÌô©Î∞î Ï†ïÏùò\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# Î™®Îç∏ÏùÑ ÌïôÏäµ Î™®ÎìúÎ°ú Ï†ÑÌôò\n",
    "model.train()\n",
    "# ÌïôÏäµ Î£®ÌîÑ ÏãúÏûë\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # ÌòÑÏû¨ Î∞∞Ïπò Ï§ëÏóêÏÑú ÏûÖÎ†•Í∞íÏùÑ Î™®Îëê GPUÎ°ú Ïù¥Îèô.\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # Î™®Îç∏ Ïã§Ìñâ\n",
    "        outputs = model(**batch)\n",
    "        # ÏÜêÏã§Í∞í Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        loss = outputs.loss\n",
    "        # Ïó≠Ï†ÑÌåå ÏàòÌñâ\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# ÌèâÍ∞Ä Î©îÌä∏Î¶≠ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "# Î™®Îç∏ÏùÑ ÌèâÍ∞Ä Î™®ÎìúÎ°ú Ï†ÑÌôò\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# ÌèâÍ∞Ä Í≤∞Í≥º Í≥ÑÏÇ∞ Î∞è Ï∂úÎ†• \n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c875cbe-337f-4490-8435-e3e21c6436fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Accelerate() ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÇ¨Ïö©\n",
    "\n",
    "There is no need to specify model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f271ef7d-28fd-4f72-bb74-a379f565433b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf7380cfe5643e99ca29007c36cb82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8700980392156863}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "\n",
    "import evaluate\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ ÏÖã Ï†ÅÏû¨\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "# ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏ checkpoint Ïù¥Î¶Ñ ÏßÄÏ†ï\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "# ÏßÄÏ†ïÎêú ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏ÏóêÏÑú ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ïù∏Ïä§ÌÑ¥Ïä§Ìôî\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ìï®Ïàò ÏÇ¨Ïö©Ïûê Ï†ïÏùòÌôî (sentence1, sentence2 Ïª¨ÎüºÏóê ÎåÄÌï¥ÏÑúÎßå ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï ÏàòÌñâ)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï ÏàòÌñâ\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# Î∞∞Ïπò(batch)Î≥Ñ Ìå®Îî©(padding)ÏùÑ ÏúÑÌïú data collator Ï†ïÏùò\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Î∂àÌïÑÏöîÌïú ÏûÖÎ†• Ïª¨ÎüºÏùÑ Ï†úÍ±∞ÌïòÍ≥† ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏Ïóê ÌïÑÏöîÌïú ÏûÖÎ†•Îßå ÎÇ®ÍπÄ.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖãÏùò label Ïª¨ÎüºÎ™ÖÏùÑ labelsÎ°ú Î≥ÄÍ≤Ω\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Ïú†ÌòïÏùÑ PyTorch tensorÎ°ú Î≥ÄÍ≤Ω\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Î≥ÄÍ≤ΩÎêú Ïª¨Îüº Ï∂úÎ†•\n",
    "print(tokenized_datasets[\"train\"].column_names)\n",
    "\n",
    "\n",
    "# Í∞Å Ï¢ÖÎ•òÎ≥Ñ Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÏÉùÏÑ±\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], \n",
    "                              shuffle=True, \n",
    "                              batch_size=8, \n",
    "                              collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"],\n",
    "                             shuffle=True,\n",
    "                             batch_size=8,\n",
    "                             collate_fn=data_collator)\n",
    "\n",
    "# ÏÇ¨Ï†ÑÌïôÏäµ Ïñ∏Ïñ¥Î™®Îç∏ Ïù∏Ïä§ÌÑ¥Ïä§Ìôî\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# ÏµúÏ†ÅÌôî Ìï®Ïàò Ï†ïÏùò\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "# ÏóêÌè¨ÌÅ¨ Í∞úÏàò ÏÑ§Ï†ï\n",
    "num_epochs = 3\n",
    "# ÌïôÏäµ Ïä§ÌÖù Ïàò Í≥ÑÏÇ∞\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# ÌïôÏäµ Ïä§ÏºÄÏ•¥Îü¨ ÏÑ§Ï†ï\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# GPUÎ°ú Î™®Îç∏ÏùÑ Ïù¥Îèô\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "\n",
    "# ÏßÑÌñâ ÏÉÅÌô©Î∞î Ï†ïÏùò\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# Î™®Îç∏ÏùÑ ÌïôÏäµ Î™®ÎìúÎ°ú Ï†ÑÌôò\n",
    "model.train()\n",
    "# ÌïôÏäµ Î£®ÌîÑ ÏãúÏûë\n",
    "#for epoch in range(num_epochs):\n",
    "#    for batch in train_dataloader:\n",
    "#        # ÌòÑÏû¨ Î∞∞Ïπò Ï§ëÏóêÏÑú ÏûÖÎ†•Í∞íÏùÑ Î™®Îëê GPUÎ°ú Ïù¥Îèô.\n",
    "#        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#        # Î™®Îç∏ Ïã§Ìñâ\n",
    "#        outputs = model(**batch)\n",
    "#        # ÏÜêÏã§Í∞í Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "#        loss = outputs.loss\n",
    "#        # Ïó≠Ï†ÑÌåå ÏàòÌñâ\n",
    "#        loss.backward()\n",
    "#\n",
    "#        optimizer.step()\n",
    "#        lr_scheduler.step()\n",
    "#        optimizer.zero_grad()\n",
    "#        progress_bar.update(1)\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "\n",
    "# ÌèâÍ∞Ä Î©îÌä∏Î¶≠ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "#from accelerate import notebook_launcher\n",
    "#\n",
    "#notebook_launcher(training_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5caab7d-d01e-448c-a62e-352cd07d740a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6c8ddbd709463f965da5550a1ad986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e5d5032f49460a8bed16229b493807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b370cc39dfc442c4a3221b509a1a7314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714dd02af6e34868a45e04bce14d6dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'accuracy': 0.8137254901960784, 'f1': 0.8538461538461538}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'accuracy': 0.8700980392156863, 'f1': 0.9087779690189329}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'accuracy': 0.8651960784313726, 'f1': 0.9043478260869565}\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huggingface/accelerate/tree/main/examples\n",
    "\n",
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import argparse\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "from accelerate import Accelerator, DistributedType\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "########################################################################\n",
    "# This is a fully working simple example to use Accelerate\n",
    "#\n",
    "# This example trains a Bert base model on GLUE MRPC\n",
    "# in any of the following settings (with the same script):\n",
    "#   - single CPU or single GPU\n",
    "#   - multi GPUS (using PyTorch distributed mode)\n",
    "#   - (multi) TPUs\n",
    "#   - fp16 (mixed-precision) or fp32 (normal precision)\n",
    "#\n",
    "# To run it in each of these various modes, follow the instructions\n",
    "# in the readme for examples:\n",
    "# https://github.com/huggingface/accelerate/tree/main/examples\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "MAX_GPU_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def get_dataloaders(accelerator: Accelerator, batch_size: int = 16):\n",
    "    \"\"\"\n",
    "    Creates a set of `DataLoader`s for the `glue` dataset,\n",
    "    using \"bert-base-cased\" as the tokenizer.\n",
    "\n",
    "    Args:\n",
    "        accelerator (`Accelerator`):\n",
    "            An `Accelerator` object\n",
    "        batch_size (`int`, *optional*):\n",
    "            The batch size for the train and validation DataLoaders.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # max_length=None => use the model max length (it's actually the default)\n",
    "        outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], \n",
    "                            truncation=True, max_length=None)\n",
    "        return outputs\n",
    "\n",
    "    # Apply the method we just defined to all the examples in all the splits of the dataset\n",
    "    # starting with the main process first:\n",
    "    with accelerator.main_process_first():\n",
    "        tokenized_datasets = datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n",
    "        )\n",
    "\n",
    "    # We also rename the 'label' column to 'labels' which is the expected name for labels \n",
    "    # by the models of the transformers library\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        # On TPU it's best to pad everything to the same length or training will be very slow.\n",
    "        max_length = 128 if accelerator.distributed_type == DistributedType.TPU else None\n",
    "        # When using mixed precision we want round multiples of 8/16\n",
    "        if accelerator.mixed_precision == \"fp8\":\n",
    "            pad_to_multiple_of = 16\n",
    "        elif accelerator.mixed_precision != \"no\":\n",
    "            pad_to_multiple_of = 8\n",
    "        else:\n",
    "            pad_to_multiple_of = None\n",
    "            \n",
    "        #print(\"examples:\\n\", examples)    \n",
    "\n",
    "        # tokenizer.pad() is a method used to pad sequences of tokens to a specified length.\n",
    "        tokens = tokenizer.pad(\n",
    "            examples,\n",
    "            padding=\"longest\",\n",
    "            max_length=max_length,\n",
    "            #pad_to_multiple_of = 16,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_tensors=\"pt\")\n",
    "        \n",
    "        ## print each token's shape \n",
    "        #for i, token in enumerate(tokens[\"input_ids\"]):\n",
    "        #    print(i, token.shape)\n",
    "        \n",
    "        return tokens\n",
    "        \n",
    "        #return tokenizer.pad(\n",
    "        #    examples,\n",
    "        #    padding=\"longest\",\n",
    "        #    max_length=max_length,\n",
    "        #    pad_to_multiple_of=pad_to_multiple_of,\n",
    "        #    return_tensors=\"pt\",\n",
    "        #)\n",
    "\n",
    "    # Instantiate dataloaders.\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"], # each tokenized dataset element may have different length.\n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,  # each element in a batch w/ batch_size to be paded with the specified length \n",
    "        batch_size=batch_size, \n",
    "        drop_last=True\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=EVAL_BATCH_SIZE,\n",
    "        drop_last=(accelerator.mixed_precision == \"fp8\"),\n",
    "    )\n",
    "\n",
    "    return train_dataloader, eval_dataloader\n",
    "\n",
    "\n",
    "#def training_function(config, args):\n",
    "def training_function():\n",
    "    # Initialize accelerator\n",
    "    #accelerator = Accelerator(cpu=args.cpu, mixed_precision=args.mixed_precision)\n",
    "    accelerator = Accelerator()\n",
    "    # Sample hyper-parameters for learning rate, batch size, seed and a few other HPs\n",
    "    lr = config[\"lr\"]\n",
    "    num_epochs = int(config[\"num_epochs\"])\n",
    "    seed = int(config[\"seed\"])\n",
    "    batch_size = int(config[\"batch_size\"])\n",
    "\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "    # If the batch size is too big we use gradient accumulation\n",
    "    gradient_accumulation_steps = 1\n",
    "    if batch_size > MAX_GPU_BATCH_SIZE and accelerator.distributed_type != DistributedType.TPU:\n",
    "        gradient_accumulation_steps = batch_size // MAX_GPU_BATCH_SIZE\n",
    "        batch_size = MAX_GPU_BATCH_SIZE\n",
    "\n",
    "    set_seed(seed)\n",
    "    train_dataloader, eval_dataloader = get_dataloaders(accelerator, batch_size)\n",
    "    # Instantiate the model (we build the model here so that the seed also control new weights initialization)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", return_dict=True)\n",
    "\n",
    "    # We could avoid this line since the accelerator is set with `device_placement=True` (default value).\n",
    "    # Note that if you are placing tensors on devices manually, this line absolutely needs to be before the optimizer\n",
    "    # creation otherwise training will not work on TPU (`accelerate` will kindly throw an error to make us aware of that).\n",
    "    model = model.to(accelerator.device)\n",
    "    # Instantiate optimizer\n",
    "    optimizer = AdamW(params=model.parameters(), lr=lr)\n",
    "\n",
    "    # Instantiate scheduler\n",
    "    num_training_steps= (len(train_dataloader) * num_epochs) // gradient_accumulation_steps\n",
    "    #print(\"num_training_steps: \", num_training_steps)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        #num_training_steps=(len(train_dataloader) * num_epochs) // gradient_accumulation_steps,\n",
    "        num_training_steps = num_training_steps,\n",
    "    )\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
    "    # prepare method.\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )\n",
    "     \n",
    "    num_training_steps = num_epochs * len(train_dataloader)  // gradient_accumulation_steps\n",
    "    #print(\"after accelerator prepared, num_training_steps: \", num_training_steps)\n",
    "    #print(\"length of train_dataloader: \", len(train_dataloader)) \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    \n",
    "    # Now we train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
    "            #batch.to(accelerator.device)\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            #if step % gradient_accumulation_steps == 0:\n",
    "            if (step+1) % gradient_accumulation_steps == 0:\n",
    "                #print(step+1)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
    "            #batch.to(accelerator.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            references = batch[\"labels\"]\n",
    "            #predictions, references = accelerator.gather_for_metrics((predictions, references))\n",
    "            metric.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "            )\n",
    "\n",
    "        eval_metric = metric.compute()\n",
    "        # Use accelerator.print to print only on the main process.\n",
    "        #print(f\"epoch {epoch}:\", eval_metric)\n",
    "        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n",
    "\n",
    "#config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 16}\n",
    "config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 8}\n",
    "#training_function(config, args)\n",
    "\n",
    "#notebook_launcher(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=1)\n",
    "        \n",
    "#def main():\n",
    "#    parser = argparse.ArgumentParser(description=\"Simple example of training script.\")\n",
    "#    parser.add_argument(\n",
    "#        \"--mixed_precision\",\n",
    "#        type=str,\n",
    "#        default=None,\n",
    "#        choices=[\"no\", \"fp16\", \"bf16\", \"fp8\"],\n",
    "#        help=\"Whether to use mixed precision. Choose\"\n",
    "#        \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n",
    "#        \"and an Nvidia Ampere GPU.\",\n",
    "#    )\n",
    "#    parser.add_argument(\"--cpu\", action=\"store_true\", help=\"If passed, will train on the CPU.\")\n",
    "#    args = parser.parse_args()\n",
    "#    config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 16}\n",
    "#    training_function(config, args)\n",
    "#\n",
    "#\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e0527518-f33b-4f14-a6be-f6eb36b9363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/dp/bin/python: No module named bitsandbytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#!python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e806bcbb-0e97-41d8-a982-ca9a990113d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "Namespace(mixed_precision=None, cpu=False)\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 408/408 [00:00<00:00, 6843.57 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "num_training_steps: 1374\n",
      "After accelerator prepared, num_training_steps:  1374\n",
      "  0%|                                                  | 0/1374 [00:00<?, ?it/s]/scratch/qualis/miniconda3/envs/dp/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 455/1374 [00:15<00:32, 28.56it/s]epoch 0: {'accuracy': 0.8137254901960784, 'f1': 0.8538461538461538}\n",
      "/scratch/qualis/miniconda3/envs/dp/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 916/1374 [00:31<00:15, 28.63it/s]epoch 1: {'accuracy': 0.8700980392156863, 'f1': 0.9087779690189329}\n",
      "/scratch/qualis/miniconda3/envs/dp/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1371/1374 [00:48<00:00, 29.19it/s]epoch 2: {'accuracy': 0.8651960784313726, 'f1': 0.9043478260869565}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1374/1374 [00:48<00:00, 28.26it/s]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch nlp_example.py # it detects the # of proceeses available and set it to num_processes.\n",
    "#!accelerate launch --num_processes=2 nlp_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15b8a5-a812-464e-90d2-1e40b14b9dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5f666-320f-410c-933e-e58a48279013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
