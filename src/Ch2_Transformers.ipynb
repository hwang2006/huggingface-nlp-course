{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb607ca-1c5a-4cd5-9cad-586f62c29a3c",
   "metadata": {},
   "source": [
    "### Pipeline 내부 실행 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca1f18c-30c4-401b-8dff-f4890827d535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3be15f2-84a5-49a0-b883-34b583f5bbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e2b0ef-9f63-4b70-9c71-c37862f4f3e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48894b8cc8ad40f994b2ac33463ab6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360bc1bef30644cba764e5c57823edd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea026ac589904d209e75071c08968618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b624c852-9f96-4b05-ae3b-c0168e4897c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e74393d-38a1-4cad-96d1-fef50d1935f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50710a0c688e4cc981c7e3f63ae223e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603d5d13-c749-4129-9730-7dcb58686bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8e96f3-ae77-4630-9ba2-f32fcfdb380c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bfbada1-5d8b-43c4-829b-ebed1146f6e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
       "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
       "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
       "         ...,\n",
       "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
       "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
       "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
       "\n",
       "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
       "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
       "         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
       "         ...,\n",
       "         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n",
       "         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n",
       "         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bbee50d-621a-4902-81e1-c1b368fbf120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutput"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dfec826-0b21-4421-b109-a6e2a967fa69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e082c5c51ba45ca8e98889116d6d4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b9edaa382a460da89fd2f35687682b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bead14d-270b-4237-9c0d-cbb9e99617fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4727b4af-ede6-43d9-a748-0613f3fefbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3412, -0.1088,  0.1147,  ..., -0.2650,  0.1881, -0.1104],\n",
       "         [-0.0316, -0.9939,  0.3383,  ..., -0.2290,  0.5234,  0.2093],\n",
       "         [ 0.1651, -0.8795,  0.5937,  ..., -0.0165, -0.0986,  0.1480],\n",
       "         ...,\n",
       "         [-0.1512, -0.5010,  0.1329,  ..., -0.3520, -0.1183,  0.2425],\n",
       "         [ 0.1559, -0.4795,  0.1415,  ..., -0.4274, -0.2023,  0.2730],\n",
       "         [ 0.9356, -0.5068,  0.2157,  ..., -0.9903,  0.0920, -0.6285]],\n",
       "\n",
       "        [[-0.0725,  0.0540, -0.0037,  ...,  0.1450,  0.2381, -0.0164],\n",
       "         [-0.1619, -0.3062, -0.2282,  ...,  0.3782, -0.1170,  0.1295],\n",
       "         [-0.1173, -0.1003,  0.1703,  ...,  0.3227, -0.1996,  0.1646],\n",
       "         ...,\n",
       "         [-0.1968, -0.3095, -0.2132,  ...,  0.3795, -0.0715,  0.0696],\n",
       "         [-0.1486, -0.2172, -0.1510,  ...,  0.3222,  0.1034, -0.0320],\n",
       "         [-0.1927, -0.1105,  0.1996,  ...,  0.4188,  0.2443, -0.0229]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7012,  0.5009,  0.9999,  ...,  1.0000, -0.6453,  0.9923],\n",
       "        [-0.7491,  0.3085,  0.9996,  ...,  0.9998, -0.8474,  0.9730]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d740fbed-65cf-4700-8d58-0c967c89d26e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n",
      "tensor([[-0.7012,  0.5009,  0.9999,  ...,  1.0000, -0.6453,  0.9923],\n",
      "        [-0.7491,  0.3085,  0.9996,  ...,  0.9998, -0.8474,  0.9730]],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "torch.Size([2, 16, 768])\n",
      "tensor([[[ 0.3412, -0.1088,  0.1147,  ..., -0.2650,  0.1881, -0.1104],\n",
      "         [-0.0316, -0.9939,  0.3383,  ..., -0.2290,  0.5234,  0.2093],\n",
      "         [ 0.1651, -0.8795,  0.5937,  ..., -0.0165, -0.0986,  0.1480],\n",
      "         ...,\n",
      "         [-0.1512, -0.5010,  0.1329,  ..., -0.3520, -0.1183,  0.2425],\n",
      "         [ 0.1559, -0.4795,  0.1415,  ..., -0.4274, -0.2023,  0.2730],\n",
      "         [ 0.9356, -0.5068,  0.2157,  ..., -0.9903,  0.0920, -0.6285]],\n",
      "\n",
      "        [[-0.0725,  0.0540, -0.0037,  ...,  0.1450,  0.2381, -0.0164],\n",
      "         [-0.1619, -0.3062, -0.2282,  ...,  0.3782, -0.1170,  0.1295],\n",
      "         [-0.1173, -0.1003,  0.1703,  ...,  0.3227, -0.1996,  0.1646],\n",
      "         ...,\n",
      "         [-0.1968, -0.3095, -0.2132,  ...,  0.3795, -0.0715,  0.0696],\n",
      "         [-0.1486, -0.2172, -0.1510,  ...,  0.3222,  0.1034, -0.0320],\n",
      "         [-0.1927, -0.1105,  0.1996,  ...,  0.4188,  0.2443, -0.0229]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs[1].shape)\n",
    "print(outputs[1])\n",
    "print(outputs[0].shape)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "102c6a42-9359-4afe-a594-df626219a9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cbe7518-2567-401d-923d-1e1336aad7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "303eeccd-4403-4ad5-b081-cfa2549240ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f6be489-3dde-46ac-87d3-1750779ce26c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5b5116f-62c3-40c4-8a6e-1e87f853a9af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b4b2b3f-a65c-48a8-a41c-816c4d16818f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a1c14a6-a98c-4916-a918-6a6639274ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c19d1a5c-dd5c-4a39-92c9-a4558e29ffac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94e2475b-d700-42f8-95b5-2418014a03e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0032, 0.9930],\n",
       "        [0.9968, 0.0070]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "066d9799-63fa-4d6a-a3a2-8c5241ce5fac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Ignoring args : (\"I don't like it\",)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "outputs = classifier(\"I've been waiting for a HuggingFace course my whole life.\", \"I don't like it\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "662115f1-c78b-4ed2-9d81-ee0916e29395",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598049521446228}, {'label': 'NEGATIVE', 'score': 0.9978170394897461}]\n"
     ]
    }
   ],
   "source": [
    "outputs = classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I don't like it\"])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97c54b-67a8-4292-9400-ebb415a8e8e2",
   "metadata": {},
   "source": [
    "### 모델 (Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9da7de-9e56-457a-89db-605b6f352444",
   "metadata": {},
   "source": [
    "기본 설정(configuration)에서 모델을 생성하면 해당 모델을 임의의 값으로 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc972048-0f0a-4534-9598-5b6a3bb7f4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# config(설정)을 만듭니다.\n",
    "config = BertConfig()\n",
    "\n",
    "# 해당 config에서 모델을 생성합니다.\n",
    "model = BertModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9091716e-45fb-4dcb-8207-c83a0b72247f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009ae28-bfb0-4548-8c86-83f57c1e3ec2",
   "metadata": {},
   "source": [
    "이 상태에서 모델을 바로 사용할 수도 있겠지만 해당 출력은 그야말로 횡설수설입니다. 따라서 먼저 학습을 해야 합니다. 우리가 수행해야할 작업(task)을 위해서 모델을 처음부터(from scratch) 학습할 수도 있지만, 1장에서 보았듯이 이를 위해서는 오랜 실행 시간과 많은 데이터가 필요하며 게다가 환경에 미치는 영향은 무시할 수 없습니다. 불필요하고 중복되는 노력을 피하기 위해서는 이미 학습된 모델을 공유하고 재사용할 수 있어야 합니다.\n",
    "\n",
    "이미 사전 학습된 Transformer 모델을 로드하는 것은 간단합니다. from_pretrained() 메서드를 사용하여 이 작업을 수행할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34540a52-eb96-4a79-8465-dc10ed7e7d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d2ad2c0-b203-409a-bdae-c3f5fff39718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"saving_folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d789b43b-371a-461b-89f9-390e1a0ae21a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%ls saving_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5319eeb-e6d3-4f1b-938f-527cbe161a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat saving_folder/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe221904-df11-4060-a995-858206d2a693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80429240-431e-4e50-9c42-d61e5a01b496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a03ae34e-f045-4e02-8d5f-4ed7fae24473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28c5468f-cacd-40d0-9cb9-52d05e9fdb0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cf60f66-8cf8-4a3b-95e3-ce74175cae35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 7592,  999,  102],\n",
       "        [ 101, 4658, 1012,  102],\n",
       "        [ 101, 3835,  999,  102]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c8f6f1c-7ebe-47b8-ad5d-5c019d837678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = model(model_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f664cd44-53a7-4405-89dc-8cf2efbe4d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fb74315-150f-4b46-ac66-2b15c63334b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 4.4496e-01,  4.8276e-01,  2.7797e-01,  ..., -5.4032e-02,\n",
       "           3.9394e-01, -9.4770e-02],\n",
       "         [ 2.4943e-01, -4.4093e-01,  8.1772e-01,  ..., -3.1917e-01,\n",
       "           2.2992e-01, -4.1172e-02],\n",
       "         [ 1.3668e-01,  2.2518e-01,  1.4502e-01,  ..., -4.6915e-02,\n",
       "           2.8224e-01,  7.5566e-02],\n",
       "         [ 1.1789e+00,  1.6738e-01, -1.8187e-01,  ...,  2.4671e-01,\n",
       "           1.0441e+00, -6.1968e-03]],\n",
       "\n",
       "        [[ 3.6436e-01,  3.2465e-02,  2.0258e-01,  ...,  6.0110e-02,\n",
       "           3.2451e-01, -2.0995e-02],\n",
       "         [ 7.1866e-01, -4.8725e-01,  5.1740e-01,  ..., -4.4012e-01,\n",
       "           1.4553e-01, -3.7545e-02],\n",
       "         [ 3.3223e-01, -2.3271e-01,  9.4876e-02,  ..., -2.5268e-01,\n",
       "           3.2172e-01,  8.1109e-04],\n",
       "         [ 1.2523e+00,  3.5754e-01, -5.1320e-02,  ..., -3.7840e-01,\n",
       "           1.0526e+00, -5.6255e-01]],\n",
       "\n",
       "        [[ 2.4042e-01,  1.4718e-01,  1.2110e-01,  ...,  7.6062e-02,\n",
       "           3.3564e-01,  2.8262e-01],\n",
       "         [ 6.5701e-01, -3.2787e-01,  2.4968e-01,  ..., -2.5920e-01,\n",
       "           2.0175e-01,  3.3275e-01],\n",
       "         [ 2.0160e-01,  1.5783e-01,  9.8973e-03,  ..., -3.8850e-01,\n",
       "           4.1307e-01,  3.9732e-01],\n",
       "         [ 1.0175e+00,  6.4387e-01, -7.8147e-01,  ..., -4.2109e-01,\n",
       "           1.0925e+00, -4.8456e-02]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6856,  0.5262,  1.0000,  ...,  1.0000, -0.6112,  0.9971],\n",
       "        [-0.6055,  0.4997,  0.9998,  ...,  0.9999, -0.6753,  0.9769],\n",
       "        [-0.7702,  0.5447,  0.9999,  ...,  1.0000, -0.4655,  0.9894]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2381a65-c731-4b76-9d0a-b9f2ec4c2c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102, 103],\n",
    "    [101, 7592, 999, 102, 103],\n",
    "    [101, 4658, 1012, 102, 103],\n",
    "    [101, 3835, 999, 102, 103]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bcf4e807-5cb9-41a4-9f08-0f2c6bf3190d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 7592,  999,  102,  103],\n",
       "        [ 101, 7592,  999,  102,  103],\n",
       "        [ 101, 4658, 1012,  102,  103],\n",
       "        [ 101, 3835,  999,  102,  103]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = torch.tensor(encoded_sequences)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e9d79e3-b79b-4d6b-8915-63b4e2d155d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(model_inputs)\n",
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0bfd6dd7-76ab-4c7d-8fdf-69298237e970",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eea59651-2399-4b8f-ae6d-c688324b60dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0037, 0.9963]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0348a32-6f1b-43cd-b1e9-da7d5ad5c4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  146,   112,  1396,  1151,  2613,  1111,   170, 20164, 10932,  2271,\n",
      "          7954,  1736,  1139,  2006,  1297,   119]])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2400,  0.1307]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Logits: tensor([[-0.2400,  0.1307]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(output)\n",
    "print(\"Logits:\", output.logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26fc9bd0-04ec-434b-9d10-01391ef99a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4084, 0.5916]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "865199f6-36dc-4a8d-8b00-c24a6845caf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[1045, 2123, 1005, 1056, 2066, 2009, 1012]])\n",
      "Logits: tensor([[ 2.8166, -2.3199]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I don't like it.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e386a18e-a0a8-477f-a94a-6995053733bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 2123, 1005, 1056, 2066, 2009, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6094, -2.9648]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Logits: tensor([[ 3.6094, -2.9648]], grad_fn=<AddmmBackward0>)\n",
      "Outputs: SequenceClassifierOutput(loss=None, logits=tensor([[ 3.6094, -2.9648]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Logits: tensor([[ 3.6094, -2.9648]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I don't like it.\"\n",
    "\n",
    "sentence_encoded = tokenizer(sequence, return_tensors='pt')\n",
    "print(sentence_encoded)\n",
    "\n",
    "output = model(sentence_encoded[\"input_ids\"])\n",
    "print(\"Outputs:\", output)\n",
    "print(\"Logits:\", output.logits)\n",
    "\n",
    "output = model(**sentence_encoded)\n",
    "print(\"Outputs:\", output)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "813385a6-245a-48ec-9948-005ae8cb28de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 2123, 1005, 1056, 2066, 2009, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I don't like it.\"\n",
    "\n",
    "sentence_encoded = tokenizer(sequence, return_tensors='pt')\n",
    "print(sentence_encoded)\n",
    "\n",
    "#output = model(sentence_encoded[\"input_ids\"])\n",
    "#print(output)\n",
    "#print(\"Logits:\", output.logits)\n",
    "\n",
    "#output = model.generate(sentence_encoded[\"input_ids\"])\n",
    "\n",
    "# TypeError: The current model class (DistilBertForSequenceClassification) is not compatible \n",
    "# with `.generate()`, as it doesn't have a language model head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fbfdbd2d-e906-439e-a295-ae5282ed6054",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 40, 836, 470, 588, 340,  13, 703, 546, 345,  30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 10, 50257])\n",
      "tensor([[  40,  836,  470,  588,  340,   13,  703,  546,  345,   30,  198,  198,\n",
      "           40, 1101,  407, 1016,  284, 6486,   13,  314]])\n",
      "I don't like it. how about you?\n",
      "\n",
      "I'm not going to lie. I\n",
      "--------------------------------------------------\n",
      "tensor([[  40,  836,  470,  588,  340,   13,  703,  546,  345,   30,  198,  198,\n",
      "           40, 1101,  407, 1016,  284, 6486,   13,  314, 1101,  407, 1016,  284,\n",
      "         6486,   13,  314, 1101,  407, 1016,  284, 6486,   13,  314, 1101,  407,\n",
      "         1016,  284, 6486,   13,  314, 1101,  407, 1016,  284, 6486,   13,  314,\n",
      "         1101,  407, 1016,  284, 6486,   13,  314, 1101,  407, 1016,  284, 6486,\n",
      "           13,  314, 1101,  407, 1016,  284, 6486,   13,  314, 1101,  407, 1016,\n",
      "          284, 6486,   13,  314, 1101,  407, 1016,  284, 6486,   13,  314, 1101,\n",
      "          407, 1016,  284, 6486,   13,  314, 1101,  407, 1016,  284, 6486,   13,\n",
      "          314, 1101,  407, 1016,  284, 6486,   13,  314, 1101,  407, 1016,  284,\n",
      "         6486,   13]])\n",
      "I don't like it. how about you?\n",
      "\n",
      "I'm not going to lie. I'm not going to lie. I'm not going to lie. I'm not going to lie. I'm not going to lie. I'm not going to lie. I'm not going to lie. I'm not going to lie. I'm not going to lie. I'm not going to lie. I'm not going to lie. I'm not going to lie. I'm not going to lie. I'm not going to lie.\n",
      "--------------------------------------------------\n",
      "tensor([[   40,   836,   470,   588,   340,    13,   703,   546,   345,    30,\n",
      "           532,  1400,   314,   714,   986,  1867, 12248,   314,  4724,   356,\n",
      "           460,   651,  5755,   286,   262,  2356,   287,   994,   986,   475,\n",
      "           356,   765,   345,   994,   783,    13,   836,   532,  3966,   986,\n",
      "          1320,   338,  1049,     0,  1309,   340,   477, 12259,   572,   523,\n",
      "           326,   356,   836,   470,   423,   284,  1730,   832,   523,   881,\n",
      "         35358,   757,  9313,  3244,    11,   618,   257,  1021, 11406, 43622,\n",
      "           319,   530,  3211,   290,   607,  1182,   373, 17788,   355,   262,\n",
      "          2933,  7121,   530,   286,   606,   503,    11,   673,  3521,   470,\n",
      "          2407,  4929,   683,    11,   290,   287,   326,  2292,   355,   673,\n",
      "          1549,  4271, 31531,   262,  2356,    11,   772,   617,  1807,   561]])\n",
      "I don't like it. how about you? - No I could... What?! I guess we can get rid of the pain in here... but we want you here now. don - Oh... That's great! let it all settle off so that we don't have to deal through so much agony again...\" Then, when a hand landed awkwardly on one arm and her head was lowered as the boy pushed one of them out, she couldn't quite catch him, and in that position as she'd previously restrained the pain, even some thought would\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I don't like it. how about you?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sequence, return_tensors='pt')\n",
    "print(sentence_encoded)\n",
    "\n",
    "#output = model(sentence_encoded[\"input_ids\"])\n",
    "#print(output)\n",
    "#print(\"Logits:\", output.logits)\n",
    "\n",
    "output = model(sentence_encoded[\"input_ids\"])\n",
    "print(output.logits.shape)\n",
    "generated_output = model.generate(sentence_encoded[\"input_ids\"])\n",
    "print(generated_output)\n",
    "print(tokenizer.decode(generated_output[0],skip_special_tokens=True))\n",
    "#---------------\n",
    "print(\"-\" * 50)\n",
    "generated_output = model.generate(\n",
    "    sentence_encoded[\"input_ids\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=100)\n",
    "print(generated_output)\n",
    "print(tokenizer.decode(generated_output[0],skip_special_tokens=True))\n",
    "#---------------\n",
    "print(\"-\" * 50)\n",
    "generation_config = GenerationConfig(max_new_tokens=100, do_sample=True, temperature=1.5)\n",
    "generated_output = model.generate(\n",
    "    sentence_encoded[\"input_ids\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    generation_config=generation_config\n",
    ")                                     \n",
    "print(generated_output)\n",
    "print(tokenizer.decode(generated_output[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f47f62e-4a82-4b2b-a48c-31d3df5c9210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[10919,   389,   262,   642,  1266,  4113,   284,  3187,   287,  6342,\n",
      "            30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 11, 50257])\n",
      "tensor([[[ -33.8901,  -33.3173,  -36.8389,  ...,  -40.4198,  -40.1635,\n",
      "           -34.3397],\n",
      "         [ -78.0767,  -75.3813,  -80.3885,  ...,  -81.7798,  -83.6533,\n",
      "           -77.3351],\n",
      "         [-101.4724,  -99.6831, -102.9444,  ...,  -99.6192, -104.8387,\n",
      "           -98.7175],\n",
      "         ...,\n",
      "         [ -81.3855,  -82.1836,  -84.5169,  ...,  -89.5507,  -89.9530,\n",
      "           -82.3802],\n",
      "         [ -91.0738,  -91.8182,  -96.6087,  ..., -103.9660, -102.1963,\n",
      "           -93.1956],\n",
      "         [-105.6677, -106.5221, -106.3895,  ..., -115.3954, -115.3625,\n",
      "           -98.6565]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[10919,   389,   262,   642,  1266,  4113,   284,  3187,   287,  6342,\n",
      "            30,   198,   198,    16,    13,   383,  4141,  3139,   198,   198]])\n",
      "what are the 5 best places to visit in Paris?\n",
      "\n",
      "1. The French capital\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "tensor([[10919,   389,   262,   642,  1266,  4113,   284,  3187,   287,  6342,\n",
      "            30,   198,   198,    16,    13,   383,  4141,  3139,   198,   198,\n",
      "           464,  4141,  3139,   318,   262,   749,  8672,  1748,   287,   262,\n",
      "           995,    13,   632,   318,  1363,   284,   625,   352,    13,    20,\n",
      "          1510,   661,    11,   290,   318,  1363,   284,   625,   352,    13,\n",
      "            20,  1510,   661,  2877,   287,  6342,    13,   383,  1748,   318,\n",
      "          1363,   284,   625,   352,    13,    20,  1510,   661,  2877,   287,\n",
      "          6342,    13,   198,   198,    17,    13,   383,  3139,   286,   262,\n",
      "           995,   198,   198,   464,  3139,   286,   262,   995,   318,  6342,\n",
      "            13,   632,   318,  1363,   284,   625,   352,    13,    20,  1510,\n",
      "           661,    11,   290,   318,  1363,   284,   625,   352,    13,    20,\n",
      "          1510]])\n",
      "what are the 5 best places to visit in Paris?\n",
      "\n",
      "1. The French capital\n",
      "\n",
      "The French capital is the most visited city in the world. It is home to over 1.5 million people, and is home to over 1.5 million people living in Paris. The city is home to over 1.5 million people living in Paris.\n",
      "\n",
      "2. The capital of the world\n",
      "\n",
      "The capital of the world is Paris. It is home to over 1.5 million people, and is home to over 1.5 million\n",
      "--------------------------------------------------\n",
      "tensor([[10919,   389,   262,   642,  1266,  4113,   284,  3187,   287,  6342,\n",
      "            30,   383,   717,  2239,   318,   284,  1064,   257,  1295,   284,\n",
      "           651,  1497,   422,  6342,     0,   198,   198, 25681,   379,   262,\n",
      "         12690,     0,   198,   198,  3123, 23284,   260,    11,  1474,   569,\n",
      "          8270, 11635,   357,    53,  3824,     8,  1222, 20576,    70,   519,\n",
      "           710,   784,   317,  3621,  1295,   284,   651,   287,   706,  6155,\n",
      "           838, 18212,   284,   262,  9003,    13,   383,  1388,  4141,  9003,\n",
      "           318,  1004, 23284,   260,  5567,    11,   422,   569,   452,   284,\n",
      "         20576,    70,   519,   710,    13,   198,   198,  3123, 23284,   260,\n",
      "           784,  6023,  3621,  1295,   284,   651,   287,   706, 24522,   767,\n",
      "         18212,    13,   632,   338, 11282,   329, 13417,    11,  6672,   290,\n",
      "          6180]])\n",
      "what are the 5 best places to visit in Paris? The first step is to find a place to get away from Paris!\n",
      "\n",
      "Stay at the Airport!\n",
      "\n",
      "Le Havre, near Villepin (VIV) & Bourgogne – A nice place to get in after walking 10 kilometers to the airport. The main French airport is Le Havre Road, from Viv to Bourgogne.\n",
      "\n",
      "Le Havre – Another nice place to get in after hiking 7 kilometers. It's convenient for overnight, afternoon and evening\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"what are the 5 best places to visit in Paris?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sequence, return_tensors='pt')\n",
    "print(sentence_encoded)\n",
    "\n",
    "#output = model(sentence_encoded[\"input_ids\"])\n",
    "#print(output)\n",
    "#print(\"Logits:\", output.logits)\n",
    "\n",
    "output = model(sentence_encoded[\"input_ids\"])\n",
    "print(output.logits.shape)\n",
    "print(output.logits)\n",
    "#print(tokenizer.decode(output[0],skip_special_tokens=True))\n",
    "\n",
    "generated_output = model.generate(sentence_encoded[\"input_ids\"])\n",
    "print(generated_output)\n",
    "print(tokenizer.decode(generated_output[0],skip_special_tokens=True))\n",
    "print(\"-\" * 50)\n",
    "generated_output = model.generate(\n",
    "    sentence_encoded[\"input_ids\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=100)\n",
    "print(generated_output)\n",
    "print(tokenizer.decode(generated_output[0],skip_special_tokens=True))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=100, do_sample=True, temperature=1.1)\n",
    "generated_output = model.generate(\n",
    "    sentence_encoded[\"input_ids\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    generation_config=generation_config,\n",
    "    max_new_tokens=100\n",
    ")                                     \n",
    "print(generated_output)\n",
    "print(tokenizer.decode(generated_output[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "88dee119-51b4-4397-b16a-8f71b2cecc2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b9c1dcb-3fe7-42d0-bebd-13feaa8c0342",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4129d89a-d21f-4fcd-935b-f58012e1d61a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 40, 836, 470, 588, 340,  13, 703, 546, 345,  30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 10, 768])\n",
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I don't like it. how about you?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sequence, return_tensors='pt')\n",
    "print(sentence_encoded)\n",
    "\n",
    "#output = model(sentence_encoded[\"input_ids\"])\n",
    "#print(output)\n",
    "#print(\"Logits:\", output.logits)\n",
    "\n",
    "output = model(sentence_encoded[\"input_ids\"])\n",
    "print(output.last_hidden_state.shape)\n",
    "\n",
    "output = model(sentence_encoded[\"input_ids\"])\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20b5495d-da36-4124-9e26-c6e3af917204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101,  146, 1274,  112,  189, 1176, 1122,  119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3660, -0.9273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Logits: tensor([[ 0.3660, -0.9273]], grad_fn=<AddmmBackward0>)\n",
      "--------------------------------------------------\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3660, -0.9273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Logits: tensor([[ 0.3660, -0.9273]], grad_fn=<AddmmBackward0>)\n",
      "--------------------------------------------------\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3660, -0.9273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Logits: tensor([[ 0.3660, -0.9273]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I don't like it.\"\n",
    "\n",
    "#tokens = tokenizer.tokenize(sequence)\n",
    "#ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "#input_ids = torch.tensor([ids])\n",
    "#print(\"Input IDs:\", input_ids)\n",
    "\n",
    "sentence_encoded = tokenizer(sequence, return_tensors='pt')\n",
    "print(sentence_encoded)\n",
    "\n",
    "#output = model(input_ids)\n",
    "output = model(sentence_encoded[\"input_ids\"])\n",
    "print(output)\n",
    "print(\"Logits:\", output.logits)\n",
    "print(\"-\" * 50)\n",
    "# argument for model to specify explicitly, e.g.,input_ids and attention_mask\n",
    "output = model(sentence_encoded[\"input_ids\"],sentence_encoded[\"attention_mask\"] )\n",
    "print(output)\n",
    "print(\"Logits:\", output.logits)\n",
    "print(\"-\" * 50)\n",
    "# argument for model is in a dictionery form with keys input_ids and attention_mask \n",
    "output = model(**sentence_encoded)\n",
    "print(output)\n",
    "print(\"Logits:\", output.logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ffeb611c-8edf-4c1c-83c0-4f62fe3f075e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3660, -0.9273]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "62ef63f2-1027-4b61-bb32-063fa06be00a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9978170394897461}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I don't like it\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a86ef67-562a-42ec-aa9c-da5f1d7ae7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5483551025390625},\n",
       " {'label': 'LABEL_0', 'score': 0.524556040763855}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"sentiment-analysis\", model=\"bert-base-uncased\")\n",
    "classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I don't like it\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b861b1a0-3d64-4b6e-896f-f731e917d3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': \"I've been waiting for a HuggingFace course my whole life. I have never heard of anything like this before and I really wish I had! Now you have a choice, I want you to get the full experience and I will beeverything!\"}],\n",
       " [{'generated_text': \"I don't like it. The first two lines are not the ones I was trying to write it out for. I'd like to take the time for the other things to just be explained. The first line is kind of my favourite. I love\"}]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model=\"gpt2\")\n",
    "generator([\"I've been waiting for a HuggingFace course my whole life.\", \"I don't like it\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfdb8d4-8bcf-4270-aeb3-86ae479c9e69",
   "metadata": {},
   "source": [
    "##### Here is how to use this model to get the features of a given text in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b99e1573-6786-4b84-a06b-0e26f325beba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "087bf303-9625-4d89-b447-766deb7d8609",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModel.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3365fbb-f358-414d-ae5d-bf6b3b3311e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReplace me by any text you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md like.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m generated_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_output)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_output[\u001b[38;5;241m0\u001b[39m],skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/transformers/generation/utils.py:1505\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1502\u001b[0m         synced_gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[0;32m-> 1505\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;66;03m# legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;66;03m# two conditions must be met\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;66;03m# 1) the generation config must have been created from the model config (`_from_model_config` field);\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# 2) the generation config must have seen no modification since its creation (the hash is the same).\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/transformers/generation/utils.py:1287\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate_compatible_classes:\n\u001b[1;32m   1286\u001b[0m     exception_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please use one of the following classes instead: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_compatible_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1287\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[0;31mTypeError\u001b[0m: The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModel.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "generated_output = model.generate(\n",
    "    encoded_input[\"input_ids\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=100)\n",
    "print(generated_output)\n",
    "print(tokenizer.decode(generated_output[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2d28f81f-2a00-4b13-82f0-807a993522cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13,  198,  198,\n",
      "           40, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101]])\n",
      "Replace me by any text you'd like.\n",
      "\n",
      "I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, GPT2LMHeadModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "#model = AutoModel.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "generated_output = model.generate(\n",
    "    encoded_input[\"input_ids\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=100)\n",
    "print(generated_output)\n",
    "print(tokenizer.decode(generated_output[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "19e38eee-0147-4319-8a47-f28270c2f4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "--------------------------------------------------\n",
      "tensor([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13,  198,  198,\n",
      "           40, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101,  407, 1654,  611,  345,  821, 3910,  286,  428,   11,  475,\n",
      "          314, 1101]])\n",
      "Replace me by any text you'd like.\n",
      "\n",
      "I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm not sure if you're aware of this, but I'm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, GPT2LMHeadModel, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "#model = AutoModel.from_pretrained('gpt2')\n",
    "#model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "print(\"-\" * 50)\n",
    "generated_output = model.generate(\n",
    "    encoded_input[\"input_ids\"],\n",
    "    #encoded_input[\"attention_mask\"], # in case of generation, there is no need for attention_mask\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=100)\n",
    "print(generated_output)\n",
    "print(tokenizer.decode(generated_output[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7600813c-99f6-43ea-9df9-eae73b6a7613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f406c872-30a5-455b-a20d-a10cad61778b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "#pad_token_id=tokenizer.eos_token_id\n",
    "generator = pipeline('text-generation', model='gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6e860-8ae3-4006-ab57-49b0ecf7c709",
   "metadata": {},
   "source": [
    "## 3장. 사전학습 모델에 대한 미세조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b73c738-3016-4af5-a538-49876a22fe2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 1])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 2장의 예제와 동일합니다.\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 새롭게 추가된 코드입니다.\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "print(batch)\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd5846f-c04a-439b-aa33-e5d1dac4ea00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 1])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3755b1d9-f408-42cf-a333-6f977c2722a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801c44e4-3991-4784-8ff1-3ae5e0d70a34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2151, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dcf4e9a-cff6-4292-95e5-26f6e710d23e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x2b6e4af21070>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c33171-c0cf-43d8-aed9-e2c1d293f48c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5302d3d-2638-407a-948c-9ca2c47a050d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['ax', 'cola', 'mnli', 'mnli_matched', 'mnli_mismatched', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli']\nExample of usage:\n\t`load_dataset('glue', 'ax')`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m raw_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m raw_datasets\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:2523\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2519\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2520\u001b[0m )\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2523\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2536\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2538\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:2232\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2230\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[1;32m   2231\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 2232\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2246\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[1;32m   2248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:371\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/builder.py:577\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_kwargs:\n\u001b[1;32m    576\u001b[0m         example_of_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_dataset(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig name is missing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease pick one among the available configs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExample of usage:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_of_usage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         )\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    583\u001b[0m     builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Config name is missing.\nPlease pick one among the available configs: ['ax', 'cola', 'mnli', 'mnli_matched', 'mnli_mismatched', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli']\nExample of usage:\n\t`load_dataset('glue', 'ax')`"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceba0c08-cb84-4b3b-bdf6-36993fafad13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8ea11e2-3845-4cf3-984d-68d05348cb7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e64f147-e04f-483d-a53d-8a61d055b53d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c65f54-afdd-4a49-8f42-def3842c0342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3741aee5-a7fd-40de-892a-b705bc83c2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\",\n",
       " \"On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\",\n",
       " 'Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',\n",
       " 'PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][\"sentence2\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cef19a9-47a2-4347-a430-df09270a842a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=26, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd9f47b2-e3a5-45f8-8760-7e79966d57e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences_2[0].attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7532057c-e31d-49af-bde7-1de61209f357",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40b3408a-d059-4993-aa64-f440a2ff5343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2028, 6251, 1012, 102], [101, 2023, 2003, 1996, 2117, 2028, 1012, 102, 2023, 2003, 1996, 2048, 2028, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer([\"This is the first sentence.\", \"This is the second one.\"],\n",
    "                  [\"This is the one sentence.\", \"This is the two one.\"])\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92147561-fe34-4ecc-b80f-6e39eb2cfc6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [101, 2023, 2003, 1996, 2028, 6251, 1012, 102, 2023, 2003, 1996, 2048, 2028, 1012, 102]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\",\n",
    "                  \"This is the one sentence.\", \"This is the two one.\")\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b12abed5-f5ac-4292-964c-f99e43d08027",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97fdbae6-05f0-42b7-a237-6c2075a9ef69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'one',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f4a5e64-79e9-4240-8730-7af81e3dc57e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a449fb47-5411-400e-9237-9a6ca5d306d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : torch.Size([3668, 103])\n",
      "token_type_ids : torch.Size([3668, 103])\n",
      "attention_mask : torch.Size([3668, 103])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for k,v in tokenized_dataset.items():\n",
    "    v = torch.tensor(v)\n",
    "    print(k, \":\", v.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb47a9c9-8b61-49e1-b38f-ce054086cbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([3668, 103]),\n",
       " 'token_type_ids': torch.Size([3668, 103]),\n",
       " 'attention_mask': torch.Size([3668, 103])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "{k: torch.tensor(v).shape for k, v in tokenized_dataset.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ec3b9f2-6885-48fe-9d13-d8c9d21dab97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('input_ids', torch.Size([3668, 103])),\n",
       " ('token_type_ids', torch.Size([3668, 103])),\n",
       " ('attention_mask', torch.Size([3668, 103]))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "[(k, torch.tensor(v).shape) for k, v in tokenized_dataset.items()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "333043e3-d8a3-410d-9e5a-364e2bad44ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd1dcbd6-605c-446d-b494-829f5e149aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e781734-b8f6-43b3-979e-d2576768d007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f37429a1-5947-4cbf-891b-325388f076eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9f9dda23f843c8b6da78a4535c6ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6af11945-c997-4417-b9d9-04edc480f9a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06c5199f6394f9cbd4eb66a2697f921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenized_train_datasets = raw_datasets[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_train_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05dfca51-4e2b-4f30-b268-f64c37643591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3956a62d-7cde-443b-b37a-54f30d89071e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(tokenized_datasets['train']['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a93fa2e-e1c1-4a69-b5eb-85c4783eddfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac038207-e59c-466a-9f12-97f88f957aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'list'>\n",
      "{'sentence1': ['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\", 'They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .', 'Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .', 'The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .', 'Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier .', 'The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday .', 'The DVD-CCA then appealed to the state Supreme Court .'], 'sentence2': ['Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\", \"On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\", 'Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .', 'PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .', \"With the scandal hanging over Stewart 's company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier .\", 'The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 .', 'The DVD CCA appealed that decision to the U.S. Supreme Court .'], 'label': [1, 0, 1, 0, 1, 1, 0, 1], 'idx': [0, 1, 2, 3, 4, 5, 6, 7], 'input_ids': [[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], [101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102], [101, 2027, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 2006, 2238, 2184, 1010, 5378, 1996, 6636, 2005, 5096, 1010, 2002, 2794, 1012, 102, 2006, 2238, 2184, 1010, 1996, 2911, 1005, 1055, 5608, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 1010, 5378, 1996, 14792, 2005, 5096, 1012, 102], [101, 2105, 6021, 19481, 13938, 2102, 1010, 21628, 6661, 2020, 2039, 2539, 16653, 1010, 2030, 1018, 1012, 1018, 1003, 1010, 2012, 1037, 1002, 1018, 1012, 5179, 1010, 2383, 3041, 2275, 1037, 2501, 2152, 1997, 1037, 1002, 1018, 1012, 5401, 1012, 102, 21628, 6661, 5598, 2322, 16653, 1010, 2030, 1018, 1012, 1020, 1003, 1010, 2000, 2275, 1037, 2501, 5494, 2152, 2012, 1037, 1002, 1018, 1012, 5401, 1012, 102], [101, 1996, 4518, 3123, 1002, 1016, 1012, 2340, 1010, 2030, 2055, 2340, 3867, 1010, 2000, 2485, 5958, 2012, 1002, 2538, 1012, 4868, 2006, 1996, 2047, 2259, 4518, 3863, 1012, 102, 18720, 1004, 1041, 13058, 1012, 6661, 5598, 1002, 1015, 1012, 6191, 2030, 1022, 3867, 2000, 1002, 2538, 1012, 6021, 2006, 1996, 2047, 2259, 4518, 3863, 2006, 5958, 1012, 102], [101, 6599, 1999, 1996, 2034, 4284, 1997, 1996, 2095, 3333, 2321, 3867, 2013, 1996, 2168, 2558, 1037, 2095, 3041, 1012, 102, 2007, 1996, 9446, 5689, 2058, 5954, 1005, 1055, 2194, 1010, 6599, 1996, 2034, 4284, 1997, 1996, 2095, 3333, 2321, 3867, 2013, 1996, 2168, 2558, 1037, 2095, 3041, 1012, 102], [101, 1996, 17235, 2850, 4160, 2018, 1037, 4882, 5114, 1997, 2459, 1012, 2676, 1010, 2030, 1015, 1012, 1016, 3867, 1010, 5494, 2012, 1015, 1010, 19611, 1012, 2321, 2006, 5958, 1012, 102, 1996, 6627, 1011, 17958, 17235, 2850, 4160, 12490, 1012, 11814, 2594, 24356, 2382, 1012, 4805, 2685, 1010, 2030, 1016, 1012, 5840, 3867, 1010, 2000, 1015, 1010, 19611, 1012, 2321, 1012, 102], [101, 1996, 4966, 1011, 10507, 2050, 2059, 12068, 2000, 1996, 2110, 4259, 2457, 1012, 102, 1996, 4966, 10507, 2050, 12068, 2008, 3247, 2000, 1996, 1057, 1012, 1055, 1012, 4259, 2457, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "print(type(samples))\n",
    "print(type(samples['input_ids']))\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "113d9519-b13a-44d0-8dad-98624f5b75e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd6fd5a9-6c3b-4103-ab7b-f363db3b7734",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [1, 0, 1, 0, 1, 1, 0, 1], 'input_ids': [[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], [101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102], [101, 2027, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 2006, 2238, 2184, 1010, 5378, 1996, 6636, 2005, 5096, 1010, 2002, 2794, 1012, 102, 2006, 2238, 2184, 1010, 1996, 2911, 1005, 1055, 5608, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 1010, 5378, 1996, 14792, 2005, 5096, 1012, 102], [101, 2105, 6021, 19481, 13938, 2102, 1010, 21628, 6661, 2020, 2039, 2539, 16653, 1010, 2030, 1018, 1012, 1018, 1003, 1010, 2012, 1037, 1002, 1018, 1012, 5179, 1010, 2383, 3041, 2275, 1037, 2501, 2152, 1997, 1037, 1002, 1018, 1012, 5401, 1012, 102, 21628, 6661, 5598, 2322, 16653, 1010, 2030, 1018, 1012, 1020, 1003, 1010, 2000, 2275, 1037, 2501, 5494, 2152, 2012, 1037, 1002, 1018, 1012, 5401, 1012, 102], [101, 1996, 4518, 3123, 1002, 1016, 1012, 2340, 1010, 2030, 2055, 2340, 3867, 1010, 2000, 2485, 5958, 2012, 1002, 2538, 1012, 4868, 2006, 1996, 2047, 2259, 4518, 3863, 1012, 102, 18720, 1004, 1041, 13058, 1012, 6661, 5598, 1002, 1015, 1012, 6191, 2030, 1022, 3867, 2000, 1002, 2538, 1012, 6021, 2006, 1996, 2047, 2259, 4518, 3863, 2006, 5958, 1012, 102], [101, 6599, 1999, 1996, 2034, 4284, 1997, 1996, 2095, 3333, 2321, 3867, 2013, 1996, 2168, 2558, 1037, 2095, 3041, 1012, 102, 2007, 1996, 9446, 5689, 2058, 5954, 1005, 1055, 2194, 1010, 6599, 1996, 2034, 4284, 1997, 1996, 2095, 3333, 2321, 3867, 2013, 1996, 2168, 2558, 1037, 2095, 3041, 1012, 102], [101, 1996, 17235, 2850, 4160, 2018, 1037, 4882, 5114, 1997, 2459, 1012, 2676, 1010, 2030, 1015, 1012, 1016, 3867, 1010, 5494, 2012, 1015, 1010, 19611, 1012, 2321, 2006, 5958, 1012, 102, 1996, 6627, 1011, 17958, 17235, 2850, 4160, 12490, 1012, 11814, 2594, 24356, 2382, 1012, 4805, 2685, 1010, 2030, 1016, 1012, 5840, 3867, 1010, 2000, 1015, 1010, 19611, 1012, 2321, 1012, 102], [101, 1996, 4966, 1011, 10507, 2050, 2059, 12068, 2000, 1996, 2110, 4259, 2457, 1012, 102, 1996, 4966, 10507, 2050, 12068, 2008, 3247, 2000, 1996, 1057, 1012, 1055, 1012, 4259, 2457, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "197bab52-d964-46c3-997d-ae07621a12d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f42bb0be-9bb7-4679-a666-c6e660aad761",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 59, 47, 67, 59, 50, 62, 32]\n"
     ]
    }
   ],
   "source": [
    "print([len(x) for x in samples[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a3c9470-b296-4645-af86-b6444ce8f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6716fbac-5fd6-4dd2-a7d0-bd8846ef8a08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
      "          2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
      "          3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
      "          1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
      "          2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  9805,  3540, 11514,  2050,  3079, 11282,  2243,  1005,  1055,\n",
      "          2077,  4855,  1996,  4677,  2000,  3647,  4576,  1999,  2687,  2005,\n",
      "          1002,  1016,  1012,  1019,  4551,  1012,   102,  9805,  3540, 11514,\n",
      "          2050,  4149, 11282,  2243,  1005,  1055,  1999,  2786,  2005,  1002,\n",
      "          6353,  2509,  2454,  1998,  2853,  2009,  2000,  3647,  4576,  2005,\n",
      "          1002,  1015,  1012,  1022,  4551,  1999,  2687,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2027,  2018,  2405,  2019, 15147,  2006,  1996,  4274,  2006,\n",
      "          2238,  2184,  1010,  5378,  1996,  6636,  2005,  5096,  1010,  2002,\n",
      "          2794,  1012,   102,  2006,  2238,  2184,  1010,  1996,  2911,  1005,\n",
      "          1055,  5608,  2018,  2405,  2019, 15147,  2006,  1996,  4274,  1010,\n",
      "          5378,  1996, 14792,  2005,  5096,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2105,  6021, 19481, 13938,  2102,  1010, 21628,  6661,  2020,\n",
      "          2039,  2539, 16653,  1010,  2030,  1018,  1012,  1018,  1003,  1010,\n",
      "          2012,  1037,  1002,  1018,  1012,  5179,  1010,  2383,  3041,  2275,\n",
      "          1037,  2501,  2152,  1997,  1037,  1002,  1018,  1012,  5401,  1012,\n",
      "           102, 21628,  6661,  5598,  2322, 16653,  1010,  2030,  1018,  1012,\n",
      "          1020,  1003,  1010,  2000,  2275,  1037,  2501,  5494,  2152,  2012,\n",
      "          1037,  1002,  1018,  1012,  5401,  1012,   102],\n",
      "        [  101,  1996,  4518,  3123,  1002,  1016,  1012,  2340,  1010,  2030,\n",
      "          2055,  2340,  3867,  1010,  2000,  2485,  5958,  2012,  1002,  2538,\n",
      "          1012,  4868,  2006,  1996,  2047,  2259,  4518,  3863,  1012,   102,\n",
      "         18720,  1004,  1041, 13058,  1012,  6661,  5598,  1002,  1015,  1012,\n",
      "          6191,  2030,  1022,  3867,  2000,  1002,  2538,  1012,  6021,  2006,\n",
      "          1996,  2047,  2259,  4518,  3863,  2006,  5958,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6599,  1999,  1996,  2034,  4284,  1997,  1996,  2095,  3333,\n",
      "          2321,  3867,  2013,  1996,  2168,  2558,  1037,  2095,  3041,  1012,\n",
      "           102,  2007,  1996,  9446,  5689,  2058,  5954,  1005,  1055,  2194,\n",
      "          1010,  6599,  1996,  2034,  4284,  1997,  1996,  2095,  3333,  2321,\n",
      "          3867,  2013,  1996,  2168,  2558,  1037,  2095,  3041,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996, 17235,  2850,  4160,  2018,  1037,  4882,  5114,  1997,\n",
      "          2459,  1012,  2676,  1010,  2030,  1015,  1012,  1016,  3867,  1010,\n",
      "          5494,  2012,  1015,  1010, 19611,  1012,  2321,  2006,  5958,  1012,\n",
      "           102,  1996,  6627,  1011, 17958, 17235,  2850,  4160, 12490,  1012,\n",
      "         11814,  2594, 24356,  2382,  1012,  4805,  2685,  1010,  2030,  1016,\n",
      "          1012,  5840,  3867,  1010,  2000,  1015,  1010, 19611,  1012,  2321,\n",
      "          1012,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  4966,  1011, 10507,  2050,  2059, 12068,  2000,  1996,\n",
      "          2110,  4259,  2457,  1012,   102,  1996,  4966, 10507,  2050, 12068,\n",
      "          2008,  3247,  2000,  1996,  1057,  1012,  1055,  1012,  4259,  2457,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 0, 1, 0, 1, 1, 0, 1])}\n"
     ]
    }
   ],
   "source": [
    "# data_collator returns batched tensors\n",
    "batch = data_collator(samples)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7449158d-d553-404e-8bcd-dffbd3ca54ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca6eeba4-d728-4b73-831c-7973d5a4eef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db83380f0c2945e5a82fbf07e661437a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20ff4b16-099b-419b-8bd6-25cabe62f206",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e25b1f75-e4b9-4c79-a6e8-fbcde9943f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "10c826c5-f222-462c-b254-0ea403d834ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3ac4317b-438e-4b4a-8451-545140e35107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(checkpoint)\n",
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "729fbf56-10b5-481f-8a24-36a61bde8197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45a971-a6e7-4525-859b-81977c82b66b",
   "metadata": {},
   "source": [
    "2장과 달리, 사전 학습된 모델을 인스턴스화한 후 경고(warnings)가 출력되는 것을 알 수 있습니다. 이는 BERT가 문장 쌍 분류에 대해 사전 학습되지 않았기 때문에 사전 학습된 모델의 헤드(model head)를 버리고 시퀀스 분류에 적합한 새로운 헤드를 대신 추가했기 때문입니다. 경고의 내용은 일부 가중치(weights)가 사용되지 않았으며(제거된 사전 학습 헤드에 해당하는 가중치) 일부 가중치가 무작위로 초기화되었음을(새로운 헤드에 대한 가중치) 나타냅니다. 마지막 부분에서 우리가 수행하는 문장 쌍 분류에 대한 미세 조정을 해도 좋다고 설명하고 있습니다.\n",
    "\n",
    "모델(model), training_args, 학습집합 및 검증집합, data_collator 및 토크나이저 등, 지금까지 구성된 모든 개체를 전달하여 Trainer를 정의할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d2e7d8f-4cab-47f2-8e07-6abb3b9a9a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e12019-a0cb-435f-a8c4-e598fd8e4c9d",
   "metadata": {},
   "source": [
    "위에서 보듯이, 토크나이저를 전달할 때 Trainer가 사용하는 기본 data_collator는 이전에 정의된 DataCollatorWithPadding이 됩니다. 따라서 이 호출에서 data_collator=data_collator 행을 생략할 수 있습니다.\n",
    "\n",
    "데이터셋으로 모델을 미세 조정(fine-tuning)하려면 Trainer의 train() 메서드를 호출하기만 하면 됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75973d85-a9ff-4661-ad5e-b80d0d8d43ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='690' max='690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [690/690 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.381900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test-trainer/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=690, training_loss=0.3061112528261931, metrics={'train_runtime': 54.9962, 'train_samples_per_second': 200.087, 'train_steps_per_second': 12.546, 'total_flos': 428887464677760.0, 'train_loss': 0.3061112528261931, 'epoch': 3.0})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85f6264b-4140-4546-b273-04a5e18fcf42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d5da0de5-f4f3-4b96-84b9-d8a63acdc9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[-2.398008  ,  3.4064734 ],\n",
      "       [ 2.246801  , -2.780507  ],\n",
      "       [ 1.8273125 , -2.3758883 ],\n",
      "       [-2.3807433 ,  3.3468034 ],\n",
      "       [ 2.248301  , -2.744817  ],\n",
      "       [-2.2686753 ,  3.2985806 ],\n",
      "       [-1.861015  ,  2.9110258 ],\n",
      "       [-2.5374424 ,  3.5025265 ],\n",
      "       [-2.0871613 ,  3.0829833 ],\n",
      "       [-2.2024333 ,  3.1539645 ],\n",
      "       [-2.464815  ,  3.4224343 ],\n",
      "       [ 2.160247  , -2.7484548 ],\n",
      "       [ 1.1706076 , -1.4239653 ],\n",
      "       [-2.0517612 ,  3.1153507 ],\n",
      "       [-2.3355386 ,  3.3131568 ],\n",
      "       [ 1.525522  , -2.0545692 ],\n",
      "       [-2.2695704 ,  3.2934375 ],\n",
      "       [ 1.2917032 , -1.5533695 ],\n",
      "       [-2.4560006 ,  3.399235  ],\n",
      "       [ 0.2905374 ,  0.4498185 ],\n",
      "       [ 2.2141428 , -2.7959604 ],\n",
      "       [-1.6209257 ,  2.4750872 ],\n",
      "       [ 1.7065779 , -2.1190758 ],\n",
      "       [-2.189832  ,  3.193757  ],\n",
      "       [-2.5309138 ,  3.4359915 ],\n",
      "       [-1.3945458 ,  2.3992157 ],\n",
      "       [-1.0307316 ,  2.19923   ],\n",
      "       [-2.4556751 ,  3.3819814 ],\n",
      "       [-2.4506497 ,  3.4487817 ],\n",
      "       [-2.242287  ,  3.233393  ],\n",
      "       [ 1.6724688 , -2.1728213 ],\n",
      "       [-2.3960917 ,  3.361754  ],\n",
      "       [-2.2253387 ,  3.2596562 ],\n",
      "       [-1.1083258 ,  1.9405974 ],\n",
      "       [-2.2914727 ,  3.2971108 ],\n",
      "       [-2.0786011 ,  3.0468338 ],\n",
      "       [ 1.7034686 , -2.1973536 ],\n",
      "       [ 2.255655  , -2.8491845 ],\n",
      "       [-1.7362512 ,  2.696395  ],\n",
      "       [-2.4138427 ,  3.34656   ],\n",
      "       [ 2.2509081 , -2.8820767 ],\n",
      "       [-2.1737573 ,  3.1816392 ],\n",
      "       [ 2.1346304 , -2.5537226 ],\n",
      "       [ 1.9631279 , -2.4480422 ],\n",
      "       [ 0.31383154,  0.3998322 ],\n",
      "       [-2.2836852 ,  3.2485428 ],\n",
      "       [-2.2163293 ,  3.17568   ],\n",
      "       [ 2.2676454 , -2.8132796 ],\n",
      "       [-2.2586305 ,  3.3218968 ],\n",
      "       [-2.4119325 ,  3.4283216 ],\n",
      "       [-2.0338013 ,  3.0627851 ],\n",
      "       [-1.7748102 ,  2.8473365 ],\n",
      "       [-2.1756687 ,  3.2272532 ],\n",
      "       [-2.2620556 ,  3.2291856 ],\n",
      "       [-2.3015156 ,  3.3140755 ],\n",
      "       [-2.519506  ,  3.4847913 ],\n",
      "       [-0.36627164,  1.3662682 ],\n",
      "       [-2.459109  ,  3.432602  ],\n",
      "       [-2.3031602 ,  3.2928123 ],\n",
      "       [-2.263848  ,  3.303901  ],\n",
      "       [-1.584517  ,  2.3294122 ],\n",
      "       [-1.0570314 ,  2.0266352 ],\n",
      "       [-2.2097874 ,  3.261155  ],\n",
      "       [-2.089234  ,  3.1470068 ],\n",
      "       [-2.2901113 ,  3.2355556 ],\n",
      "       [ 1.6907017 , -2.278103  ],\n",
      "       [-2.405606  ,  3.3513935 ],\n",
      "       [-2.360271  ,  3.3647168 ],\n",
      "       [ 1.2513307 , -1.8037393 ],\n",
      "       [-2.1676443 ,  3.1701713 ],\n",
      "       [-2.3900054 ,  3.3416173 ],\n",
      "       [ 1.3537167 , -1.6750498 ],\n",
      "       [-2.2049427 ,  3.2503238 ],\n",
      "       [-2.2682421 ,  3.2790065 ],\n",
      "       [-1.6718411 ,  2.7980025 ],\n",
      "       [-2.178625  ,  3.170521  ],\n",
      "       [-1.5220561 ,  2.717753  ],\n",
      "       [-2.3813307 ,  3.3563    ],\n",
      "       [-2.2148895 ,  3.2213535 ],\n",
      "       [-2.3938751 ,  3.4094014 ],\n",
      "       [-1.8113424 ,  2.843125  ],\n",
      "       [-2.5895596 ,  3.581731  ],\n",
      "       [-2.323356  ,  3.3144622 ],\n",
      "       [ 1.9827507 , -2.4755695 ],\n",
      "       [-2.1235774 ,  3.165265  ],\n",
      "       [ 1.2070855 , -1.4578295 ],\n",
      "       [-2.0372717 ,  3.0675082 ],\n",
      "       [-1.2558347 ,  2.2733822 ],\n",
      "       [-2.2838311 ,  3.3049212 ],\n",
      "       [-2.279362  ,  3.2561064 ],\n",
      "       [ 2.0336127 , -2.5307949 ],\n",
      "       [-2.4825633 ,  3.4708848 ],\n",
      "       [-2.4651372 ,  3.3971112 ],\n",
      "       [-2.4596386 ,  3.4122658 ],\n",
      "       [-2.5290537 ,  3.4855742 ],\n",
      "       [-2.47613   ,  3.4163132 ],\n",
      "       [ 2.0844657 , -2.642274  ],\n",
      "       [-2.0017776 ,  3.0400567 ],\n",
      "       [-2.217859  ,  3.2124012 ],\n",
      "       [-1.9710683 ,  2.9770377 ],\n",
      "       [-2.2284596 ,  3.2448027 ],\n",
      "       [ 1.175219  , -1.656151  ],\n",
      "       [-2.1488845 ,  3.1325357 ],\n",
      "       [-2.2825532 ,  3.2772255 ],\n",
      "       [ 1.6099565 , -2.0760686 ],\n",
      "       [-2.3110816 ,  3.3046663 ],\n",
      "       [-1.9785991 ,  3.0971875 ],\n",
      "       [ 2.2492156 , -2.8274205 ],\n",
      "       [ 2.2612877 , -2.8337386 ],\n",
      "       [-2.343377  ,  3.3379118 ],\n",
      "       [-1.6499513 ,  2.8569398 ],\n",
      "       [-1.8817415 ,  3.0239625 ],\n",
      "       [-2.3816688 ,  3.1949081 ],\n",
      "       [-2.3019888 ,  3.2762592 ],\n",
      "       [-1.8671134 ,  2.9314473 ],\n",
      "       [ 1.9149991 , -2.3837802 ],\n",
      "       [-2.2767427 ,  3.2334454 ],\n",
      "       [-2.3772266 ,  3.363039  ],\n",
      "       [-2.4400117 ,  3.4262986 ],\n",
      "       [-2.418791  ,  3.4123747 ],\n",
      "       [-2.1684744 ,  3.1583447 ],\n",
      "       [-1.7810825 ,  2.844886  ],\n",
      "       [ 2.2214832 , -2.7402108 ],\n",
      "       [-2.443047  ,  3.4014664 ],\n",
      "       [-2.3174198 ,  3.2729945 ],\n",
      "       [-2.2514052 ,  3.2820802 ],\n",
      "       [-2.1481905 ,  3.1819134 ],\n",
      "       [ 2.0674489 , -2.588818  ],\n",
      "       [-2.4312584 ,  3.370788  ],\n",
      "       [-2.35998   ,  3.3748832 ],\n",
      "       [-2.0577419 ,  3.0958002 ],\n",
      "       [ 1.346647  , -1.7712531 ],\n",
      "       [-0.8065148 ,  2.0416527 ],\n",
      "       [ 0.75907886, -1.0278187 ],\n",
      "       [-1.7739282 ,  2.834091  ],\n",
      "       [-2.4227235 ,  3.3913977 ],\n",
      "       [ 2.0681    , -2.680957  ],\n",
      "       [ 1.3216488 , -1.3003203 ],\n",
      "       [-2.2976282 ,  3.2600288 ],\n",
      "       [-2.4084318 ,  3.4152849 ],\n",
      "       [-2.4542415 ,  3.3881423 ],\n",
      "       [ 1.6800928 , -2.1790242 ],\n",
      "       [ 2.18962   , -2.7806277 ],\n",
      "       [-2.330853  ,  3.3149805 ],\n",
      "       [ 2.2722883 , -2.8501053 ],\n",
      "       [-0.74350804,  1.8393328 ],\n",
      "       [-2.412778  ,  3.3948336 ],\n",
      "       [-1.2768683 ,  2.49465   ],\n",
      "       [-0.8676069 ,  2.089954  ],\n",
      "       [-2.3647997 ,  3.2782433 ],\n",
      "       [ 2.1590028 , -2.6691792 ],\n",
      "       [-2.1523974 ,  3.2119284 ],\n",
      "       [-0.70546186,  1.6649965 ],\n",
      "       [-2.4639404 ,  3.427512  ],\n",
      "       [-0.37919706,  1.4950659 ],\n",
      "       [-2.229824  ,  3.2352142 ],\n",
      "       [-2.1006474 ,  3.157005  ],\n",
      "       [-0.2646282 ,  1.3161607 ],\n",
      "       [ 1.7417285 , -2.2070858 ],\n",
      "       [-2.013736  ,  3.1037111 ],\n",
      "       [-1.8759001 ,  2.9080377 ],\n",
      "       [-2.3562732 ,  3.330894  ],\n",
      "       [-2.307135  ,  3.2572718 ],\n",
      "       [-2.1522706 ,  3.1622179 ],\n",
      "       [-2.3286192 ,  3.3089693 ],\n",
      "       [-2.1105866 ,  3.0895517 ],\n",
      "       [-1.6036171 ,  2.775275  ],\n",
      "       [ 2.1619115 , -2.6236782 ],\n",
      "       [-2.3260405 ,  3.2905593 ],\n",
      "       [ 2.1883745 , -2.757837  ],\n",
      "       [ 1.5704792 , -1.9923793 ],\n",
      "       [-1.2799269 ,  2.375645  ],\n",
      "       [-1.8064728 ,  2.9189153 ],\n",
      "       [-2.2238026 ,  3.2527122 ],\n",
      "       [ 1.3967154 , -2.0195575 ],\n",
      "       [-2.454916  ,  3.4060693 ],\n",
      "       [-2.2049978 ,  3.1716413 ],\n",
      "       [ 2.0938604 , -2.6910715 ],\n",
      "       [-2.5303638 ,  3.4448256 ],\n",
      "       [-2.5312178 ,  3.490721  ],\n",
      "       [ 1.5732753 , -2.1468585 ],\n",
      "       [-2.4821253 ,  3.3777363 ],\n",
      "       [-1.1911248 ,  2.24781   ],\n",
      "       [-2.3251429 ,  3.3308272 ],\n",
      "       [-2.1057014 ,  3.1100793 ],\n",
      "       [-1.851941  ,  2.7646577 ],\n",
      "       [ 1.7817407 , -2.2260027 ],\n",
      "       [-2.142353  ,  3.2222428 ],\n",
      "       [ 2.1325655 , -2.6955976 ],\n",
      "       [-2.1833658 ,  3.1932683 ],\n",
      "       [-2.406692  ,  3.3195112 ],\n",
      "       [ 2.2401745 , -2.752854  ],\n",
      "       [ 0.28967294,  0.3323952 ],\n",
      "       [-2.1029859 ,  3.0639768 ],\n",
      "       [-1.2610978 ,  2.567021  ],\n",
      "       [-1.8306079 ,  2.8962564 ],\n",
      "       [-2.4497135 ,  3.4036188 ],\n",
      "       [-1.6017574 ,  2.6688163 ],\n",
      "       [-2.4518223 ,  3.4096694 ],\n",
      "       [-2.346383  ,  3.3593504 ],\n",
      "       [-2.1240141 ,  3.2266452 ],\n",
      "       [-2.2889957 ,  3.2689476 ],\n",
      "       [-0.13234577,  0.9957696 ],\n",
      "       [-2.1595063 ,  3.175527  ],\n",
      "       [-2.3084464 ,  3.291356  ],\n",
      "       [ 2.1057773 , -2.6325808 ],\n",
      "       [-2.2113764 ,  3.2756226 ],\n",
      "       [-2.0809102 ,  3.1154969 ],\n",
      "       [ 1.8120115 , -2.2831185 ],\n",
      "       [-0.3087465 ,  1.1277215 ],\n",
      "       [ 0.8152761 , -1.0331715 ],\n",
      "       [-2.3916092 ,  3.379806  ],\n",
      "       [-1.4446672 ,  2.5520866 ],\n",
      "       [ 1.1977377 , -1.3600323 ],\n",
      "       [-2.353009  ,  3.3031752 ],\n",
      "       [-2.2885184 ,  3.2899382 ],\n",
      "       [-2.0584786 ,  2.982006  ],\n",
      "       [-2.4731019 ,  3.4369688 ],\n",
      "       [ 2.182731  , -2.677748  ],\n",
      "       [-2.0379407 ,  3.090509  ],\n",
      "       [-2.1121545 ,  3.0994718 ],\n",
      "       [-2.1563947 ,  3.1611738 ],\n",
      "       [-2.330466  ,  3.3224783 ],\n",
      "       [ 0.3984399 ,  0.4453876 ],\n",
      "       [-2.3865514 ,  3.4560409 ],\n",
      "       [-2.2632697 ,  3.2307465 ],\n",
      "       [-2.365553  ,  3.3071148 ],\n",
      "       [-2.0285144 ,  3.0206184 ],\n",
      "       [-2.564694  ,  3.4697716 ],\n",
      "       [-2.124773  ,  3.1829448 ],\n",
      "       [-2.3085487 ,  3.3304176 ],\n",
      "       [-2.1333115 ,  3.1424947 ],\n",
      "       [ 0.6832316 , -0.82868105],\n",
      "       [ 2.1145449 , -2.5785868 ],\n",
      "       [ 2.052975  , -2.5831232 ],\n",
      "       [ 1.7509147 , -2.3237026 ],\n",
      "       [-1.2552819 ,  2.3982546 ],\n",
      "       [ 1.9963195 , -2.4726365 ],\n",
      "       [-1.8638102 ,  2.8636928 ],\n",
      "       [-1.9307666 ,  3.0422153 ],\n",
      "       [-2.07835   ,  3.2170339 ],\n",
      "       [ 2.2171881 , -2.7667134 ],\n",
      "       [-2.052377  ,  3.1118138 ],\n",
      "       [-1.8758767 ,  3.0044613 ],\n",
      "       [-2.4773197 ,  3.4272254 ],\n",
      "       [-2.298631  ,  3.2555764 ],\n",
      "       [-2.1077116 ,  3.14121   ],\n",
      "       [-2.0392132 ,  3.1677408 ],\n",
      "       [-2.3818042 ,  3.3423698 ],\n",
      "       [-1.7131839 ,  2.8762016 ],\n",
      "       [-2.297011  ,  3.2933257 ],\n",
      "       [ 0.10487013, -0.59760773],\n",
      "       [ 0.7242474 , -1.1033758 ],\n",
      "       [-1.4479129 ,  2.4164696 ],\n",
      "       [ 1.6793064 , -2.0568612 ],\n",
      "       [ 2.2759066 , -2.850981  ],\n",
      "       [-2.5917213 ,  3.5452635 ],\n",
      "       [-2.4497478 ,  3.4117153 ],\n",
      "       [-2.2614753 ,  3.276208  ],\n",
      "       [ 0.36706376, -0.55783087],\n",
      "       [-0.11916014,  0.36427444],\n",
      "       [-0.7203656 ,  1.3303124 ],\n",
      "       [-2.125921  ,  3.1765733 ],\n",
      "       [-2.3519385 ,  3.3251746 ],\n",
      "       [-1.8299661 ,  2.8476756 ],\n",
      "       [-1.5938061 ,  2.7486649 ],\n",
      "       [ 1.9676024 , -2.463123  ],\n",
      "       [ 1.8115261 , -2.259112  ],\n",
      "       [ 2.0070188 , -2.5495205 ],\n",
      "       [-2.1338317 ,  3.138197  ],\n",
      "       [ 2.038416  , -2.648062  ],\n",
      "       [-2.356301  ,  3.3309622 ],\n",
      "       [-2.2589312 ,  3.2818437 ],\n",
      "       [-2.4318376 ,  3.397507  ],\n",
      "       [-2.3004818 ,  3.2561839 ],\n",
      "       [-2.2033994 ,  3.2549746 ],\n",
      "       [-2.1345766 ,  3.1792219 ],\n",
      "       [ 0.50923437, -0.11765601],\n",
      "       [-1.8816072 ,  2.9996197 ],\n",
      "       [ 2.1651545 , -2.696466  ],\n",
      "       [-1.8626894 ,  2.9189007 ],\n",
      "       [-1.8339875 ,  2.9588184 ],\n",
      "       [-1.2795382 ,  2.399658  ],\n",
      "       [ 2.1996698 , -2.7966197 ],\n",
      "       [ 0.72710097, -1.1678251 ],\n",
      "       [-2.3204424 ,  3.300964  ],\n",
      "       [-2.3874059 ,  3.356992  ],\n",
      "       [-1.0562537 ,  2.1879418 ],\n",
      "       [-2.4573557 ,  3.360032  ],\n",
      "       [ 0.37071458, -0.20860952],\n",
      "       [ 0.17186837, -0.43408784],\n",
      "       [ 2.2250738 , -2.7587755 ],\n",
      "       [-2.133124  ,  3.1349535 ],\n",
      "       [-2.2153838 ,  3.2150815 ],\n",
      "       [-2.0960333 ,  3.144581  ],\n",
      "       [ 2.1322834 , -2.6021235 ],\n",
      "       [ 2.219731  , -2.7820644 ],\n",
      "       [ 0.26492244, -0.37288406],\n",
      "       [-2.365193  ,  3.3425689 ],\n",
      "       [ 2.1362395 , -2.5828476 ],\n",
      "       [-2.304958  ,  3.3315744 ],\n",
      "       [-2.2634044 ,  3.2232087 ],\n",
      "       [-2.4697928 ,  3.4493601 ],\n",
      "       [ 1.5499203 , -2.1075847 ],\n",
      "       [-2.4151042 ,  3.3389955 ],\n",
      "       [-2.4987304 ,  3.502784  ],\n",
      "       [ 2.2490218 , -2.7687514 ],\n",
      "       [-2.4228616 ,  3.3632576 ],\n",
      "       [ 1.2805502 , -0.9930489 ],\n",
      "       [-0.5111092 ,  1.1529368 ],\n",
      "       [-2.4749768 ,  3.4002614 ],\n",
      "       [-2.2626767 ,  3.2460704 ],\n",
      "       [ 1.6360718 , -2.0438151 ],\n",
      "       [ 2.1236596 , -2.6697564 ],\n",
      "       [-2.358469  ,  3.3362203 ],\n",
      "       [ 1.3308184 , -1.4097444 ],\n",
      "       [ 0.452934  , -0.64954096],\n",
      "       [-2.4773958 ,  3.4430869 ],\n",
      "       [ 2.0760753 , -2.55156   ],\n",
      "       [ 2.100037  , -2.5887918 ],\n",
      "       [ 2.168027  , -2.6556823 ],\n",
      "       [ 2.257963  , -2.8629496 ],\n",
      "       [ 2.1513345 , -2.6785955 ],\n",
      "       [-1.3267547 ,  2.5390418 ],\n",
      "       [ 1.1546987 , -1.4526659 ],\n",
      "       [-2.370731  ,  3.3172734 ],\n",
      "       [-1.6494775 ,  2.7210646 ],\n",
      "       [-2.433307  ,  3.3820324 ],\n",
      "       [-2.6131487 ,  3.5563784 ],\n",
      "       [-1.0247893 ,  2.1704133 ],\n",
      "       [-2.6155925 ,  3.549152  ],\n",
      "       [-2.3868606 ,  3.35464   ],\n",
      "       [-1.2918278 ,  2.271851  ],\n",
      "       [-2.3476431 ,  3.3122754 ],\n",
      "       [-2.4393508 ,  3.4231043 ],\n",
      "       [-2.4099996 ,  3.3509426 ],\n",
      "       [-2.4201272 ,  3.3867114 ],\n",
      "       [-2.4816318 ,  3.4054902 ],\n",
      "       [ 1.7130604 , -2.2774746 ],\n",
      "       [-1.8237958 ,  2.8179808 ],\n",
      "       [-2.504563  ,  3.4966795 ],\n",
      "       [-2.4073718 ,  3.3792894 ],\n",
      "       [ 1.7565385 , -2.06792   ],\n",
      "       [-0.09128525,  1.1715506 ],\n",
      "       [-2.3666642 ,  3.3329158 ],\n",
      "       [-2.4025025 ,  3.3716493 ],\n",
      "       [-2.348859  ,  3.335453  ],\n",
      "       [-2.2069905 ,  3.210288  ],\n",
      "       [-2.2403972 ,  3.2107291 ],\n",
      "       [-2.295034  ,  3.307858  ],\n",
      "       [ 2.037279  , -2.5456374 ],\n",
      "       [-2.5136552 ,  3.4969044 ],\n",
      "       [-1.1470658 ,  1.9905164 ],\n",
      "       [-2.5312402 ,  3.50345   ],\n",
      "       [ 1.9778941 , -2.4561222 ],\n",
      "       [-1.0278399 ,  2.093584  ],\n",
      "       [-2.306335  ,  3.3185625 ],\n",
      "       [-1.9988395 ,  3.0188491 ],\n",
      "       [-2.0606062 ,  3.0246024 ],\n",
      "       [-2.307558  ,  3.2619867 ],\n",
      "       [ 1.2236756 , -1.6663082 ],\n",
      "       [-2.0710242 ,  3.1047506 ],\n",
      "       [-2.2501497 ,  3.2339725 ],\n",
      "       [ 1.1145339 , -1.7712798 ],\n",
      "       [-2.2624097 ,  3.2518735 ],\n",
      "       [-2.3602357 ,  3.3205419 ],\n",
      "       [-2.5750766 ,  3.5064895 ],\n",
      "       [-2.1618834 ,  3.2414522 ],\n",
      "       [ 2.0930593 , -2.5561938 ],\n",
      "       [ 1.9418305 , -2.4405391 ],\n",
      "       [ 1.3371408 , -2.0069525 ],\n",
      "       [-2.4471765 ,  3.415263  ],\n",
      "       [-2.1108296 ,  3.1304986 ],\n",
      "       [-1.6356485 ,  2.8034728 ],\n",
      "       [ 1.934577  , -2.382629  ],\n",
      "       [ 1.8681481 , -2.4593658 ],\n",
      "       [-1.2201161 ,  2.4087076 ],\n",
      "       [ 2.0798893 , -2.5148587 ],\n",
      "       [-2.361949  ,  3.3853323 ],\n",
      "       [-2.3851085 ,  3.4152772 ],\n",
      "       [-1.5068239 ,  2.6560905 ],\n",
      "       [-2.3496675 ,  3.324153  ],\n",
      "       [ 1.5455738 , -1.9846038 ],\n",
      "       [-2.3654876 ,  3.3286023 ],\n",
      "       [-2.0816    ,  3.1444001 ],\n",
      "       [ 2.063544  , -2.53829   ],\n",
      "       [-1.4055886 ,  2.5208879 ],\n",
      "       [ 1.3451072 , -1.7014802 ],\n",
      "       [-2.248911  ,  3.2342346 ],\n",
      "       [ 1.6918565 , -2.2848165 ],\n",
      "       [-2.5196888 ,  3.4549649 ],\n",
      "       [-1.1439214 ,  2.2401218 ],\n",
      "       [-2.2754395 ,  3.3756912 ],\n",
      "       [-2.4344277 ,  3.4305143 ],\n",
      "       [-2.103462  ,  3.1484096 ],\n",
      "       [-2.3658183 ,  3.3860693 ],\n",
      "       [-2.2535708 ,  3.216411  ],\n",
      "       [ 0.8136932 , -0.598093  ],\n",
      "       [-2.0688987 ,  3.0070336 ],\n",
      "       [-2.292199  ,  3.2986515 ],\n",
      "       [ 0.88925034, -0.6564929 ],\n",
      "       [-2.4112823 ,  3.3861966 ],\n",
      "       [-1.2349032 ,  2.4143734 ],\n",
      "       [ 2.288129  , -2.8956819 ],\n",
      "       [ 1.5624781 , -2.0549705 ],\n",
      "       [-2.2571914 ,  3.237153  ],\n",
      "       [ 0.67483217, -0.1483652 ],\n",
      "       [-2.2495286 ,  3.2728512 ]], dtype=float32), label_ids=array([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "       1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "       0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "       0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "       0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]), metrics={'test_loss': 0.5298288464546204, 'test_runtime': 1.1368, 'test_samples_per_second': 358.907, 'test_steps_per_second': 22.872})\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "694e4612-286d-49cc-955b-2587460cb26c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c2bc3b2b-a6df-4d58-ad44-2203e06a6375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408,)\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3ebc3d0b-a6ef-444a-b7a8-160f98010a32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38423/1543290331.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"mrpc\")\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8578431372549019, 'f1': 0.8989547038327527}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dd20d9f1-54f3-4107-a999-800be04404dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8e3b1461-d6a0-41b9-af15-dce11ff589e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': Value(dtype='int32', id=None),\n",
       " 'references': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "45edc4e8-aa44-4ac0-a4ef-2a25f266ee91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8578431372549019}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2e7c6c2d-78b6-4267-ad03-acb4278a5d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a734e9be-5d54-41fe-bb9f-9ca558c224b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8578431372549019,\n",
       " 'f1': 0.8989547038327527,\n",
       " 'precision': 0.8745762711864407,\n",
       " 'recall': 0.9247311827956989}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "70703f34-50be-4543-8252-bda9b7e4b0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "39e03364-1ce9-4be7-981e-4c7e82e2c363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "804e490e-9a07-496d-884d-bfa3a54fd184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='690' max='690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [690/690 00:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.368305</td>\n",
       "      <td>0.850490</td>\n",
       "      <td>0.895369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.401676</td>\n",
       "      <td>0.840686</td>\n",
       "      <td>0.891122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>0.514764</td>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.895270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory test-trainer/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=690, training_loss=0.310552049719769, metrics={'train_runtime': 47.332, 'train_samples_per_second': 232.485, 'train_steps_per_second': 14.578, 'total_flos': 428887464677760.0, 'train_loss': 0.310552049719769, 'epoch': 3.0})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2f841a81-04f1-4a36-8af5-d6cf738da8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='690' max='690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [690/690 00:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.333718</td>\n",
       "      <td>0.865196</td>\n",
       "      <td>0.906303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.457012</td>\n",
       "      <td>0.855392</td>\n",
       "      <td>0.900506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.553438</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.896907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory test-trainer/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/scratch/qualis/miniconda3/envs/genai/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=690, training_loss=0.2791967198468637, metrics={'train_runtime': 47.1469, 'train_samples_per_second': 233.398, 'train_steps_per_second': 14.635, 'total_flos': 428887464677760.0, 'train_loss': 0.2791967198468637, 'epoch': 3.0})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ad13d-c234-4602-b1f8-aaed6af4d63f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
