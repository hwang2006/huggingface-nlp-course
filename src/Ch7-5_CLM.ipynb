{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e48d1e-2a64-4b92-8eb1-dcdc9f30bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d064a36b-a1c7-423e-88ec-14ef07b5d409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\"\n",
    "\n",
    "print(any_keyword_in_string(example_1, filters),\n",
    "      any_keyword_in_string(example_2, filters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b88f04f5-21e4-470d-8abc-f84b102ae071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0\n",
    "    #batch = iter(dataset)\n",
    "    #print(batch)\n",
    "    for sample in tqdm(iter(dataset)):\n",
    "        total += 1\n",
    "        #print(sample.keys())\n",
    "        #print(sample[\"content\"])\n",
    "        if total > 10000:\n",
    "            break\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v)\n",
    "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
    "    #print(filtered_dict)\n",
    "    return Dataset.from_dict(filtered_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28cfadf5-a598-43c3-8424-e328fa1ea50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 코드 셀은 실행 시간이 매우 깁니다. 따라서 그냥 생략하고 다음으로 넘어가시기 바랍니다!\n",
    "#from datasets import load_dataset\n",
    "\n",
    "#split = \"train\"\n",
    "#filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "\n",
    "#data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
    "#filtered_data = filter_streaming_dataset(data, filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf5d9669-cd34-469e-9190-4752688082f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe5c01a8-3689-4595-bb10-e645b82239c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "587c96ea-addf-4966-807b-960a3f328486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 80000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train.shuffle(seed=12).select(range(80000)),\n",
    "        \"valid\": ds_valid.shuffle(seed=12).select(range(500))\n",
    "    }\n",
    ")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8da040d-6b17-48a2-88c4-066773a9f181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': ['henniggroup/MPInterfaces', 'nesterione/scikit-learn'],\n",
       " 'path': ['mpinterfaces/mat2d/stability/analysis.py',\n",
       "  'examples/covariance/plot_lw_vs_oas.py'],\n",
       " 'copies': ['2', '248'],\n",
       " 'size': ['5881', '2903'],\n",
       " 'content': ['from __future__ import print_function, division, unicode_literals\\n\\nimport operator\\nimport os\\n\\nimport matplotlib as mpl\\nmpl.use(\\'Agg\\')\\nimport matplotlib.pyplot as plt\\n\\nfrom pymatgen.core.structure import Structure\\nfrom pymatgen.entries.computed_entries import ComputedEntry\\nfrom pymatgen.io.vasp.outputs import Vasprun\\n#from pymatgen.analysis.phase_diagram import PDAnalyzer\\nfrom pymatgen.analysis.phase_diagram import PhaseDiagram\\n\\nfrom mpinterfaces.utils import is_converged\\nfrom mpinterfaces.mat2d import MPR\\n\\n__author__ = \"Michael Ashton\"\\n__copyright__ = \"Copyright 2017, Henniggroup\"\\n__maintainer__ = \"Michael Ashton\"\\n__email__ = \"ashtonmv@gmail.com\"\\n__status__ = \"Production\"\\n__date__ = \"March 3, 2017\"\\n\\n\\ndef get_competing_phases():\\n    \"\"\"\\n    Collect the species to which the material might decompose to.\\n\\n    Returns:\\n        A list of phases as tuples formatted as\\n        [(formula_1, Materials_Project_ID_1),\\n        (formula_2, Materials_Project_ID_2), ...]\\n    \"\"\"\\n\\n    composition = Structure.from_file(\\'POSCAR\\').composition\\n    try:\\n        energy = Vasprun(\\'vasprun.xml\\').final_energy\\n    except:\\n        energy = 100  # The function can work without a vasprun.xml\\n    entries = MPR.get_entries_in_chemsys([elt.symbol for elt in composition])\\n    my_entry = ComputedEntry(composition, energy)\\n    entries.append(my_entry)\\n\\n    #pda = PDAnalyzer(PhaseDiagram(entries))\\n    pda = PhaseDiagram(entries)\\n    decomp = pda.get_decomp_and_e_above_hull(my_entry, allow_negative=True)\\n    competing_phases = [(entry.composition.reduced_formula, entry.entry_id)\\n                        for entry in decomp[0]]\\n\\n    return competing_phases\\n\\n\\ndef get_hull_distance(competing_phase_directory=\\'../competing_phases\\'):\\n    \"\"\"\\n    Calculate the material\\'s distance to the thermodynamic hull,\\n    based on species in the Materials Project database.\\n\\n    Args:\\n        competing_phase_directory (str): absolute or relative path\\n            to the location where your competing phases have been\\n            relaxed. The default expectation is that they are stored\\n            in a directory named \\'competing_phases\\' at the same level\\n            as your material\\'s relaxation directory.\\n    Returns:\\n        float: distance (eV/atom) between the material and the\\n            hull.\\n    \"\"\"\\n\\n    finished_competitors = {}\\n    original_directory = os.getcwd()\\n    # Determine which competing phases have been relaxed in the current\\n    # framework and store them in a dictionary ({formula: entry}).\\n    if os.path.isdir(competing_phase_directory):\\n        os.chdir(competing_phase_directory)\\n        for comp_dir in [dir for dir in os.listdir(os.getcwd())\\n                         if os.path.isdir(dir) and is_converged(dir)]:\\n            vasprun = Vasprun(\\'{}/vasprun.xml\\'.format(comp_dir))\\n            composition = vasprun.final_structure.composition\\n            energy = vasprun.final_energy\\n            finished_competitors[comp_dir] = ComputedEntry(composition, energy)\\n        os.chdir(original_directory)\\n    else:\\n        raise ValueError(\\'Competing phase directory does not exist.\\')\\n\\n    composition = Structure.from_file(\\'POSCAR\\').composition\\n    try:\\n        energy = Vasprun(\\'vasprun.xml\\').final_energy\\n    except:\\n        raise ValueError(\\'This directory does not have a converged vasprun.xml\\')\\n    my_entry = ComputedEntry(composition, energy)  # 2D material\\n    entries = MPR.get_entries_in_chemsys([elt.symbol for elt in composition])\\n\\n    # If the energies of competing phases have been calculated in\\n    # the current framework, put them in the phase diagram instead\\n    # of the MP energies.\\n    for i in range(len(entries)):\\n        formula = entries[i].composition.reduced_formula\\n        if formula in finished_competitors:\\n            entries[i] = finished_competitors[formula]\\n        else:\\n            entries[i] = ComputedEntry(entries[i].composition, 100)\\n\\n    entries.append(my_entry)  # 2D material\\n\\n    #pda = PDAnalyzer(PhaseDiagram(entries))\\n    pda = PhaseDiagram(entries)\\n    decomp = pda.get_decomp_and_e_above_hull(my_entry, allow_negative=True)\\n\\n    return decomp[1]\\n\\n\\ndef plot_hull_distances(hull_distances, fmt=\\'pdf\\'):\\n    \"\"\"\\n    Create a bar graph of the formation energies of several 2D materials.\\n\\n    Args:\\n        hull_distances (dict): follow the format:\\n            {reduced_formula: hull_distance (in eV/atom)}\\n        fmt (str): matplotlib format style. Check the matplotlib\\n            docs for options.\\n    \"\"\"\\n\\n    hsize = 12 + (len(hull_distances) - 4) / 3\\n    ax = plt.figure(figsize=(hsize, 10)).gca()\\n    ax.set_ylim(0, 700)\\n    ax.set_xlim(0, len(hull_distances))\\n\\n    x_ticklabels = []\\n    i = 0\\n    for compound in sorted(hull_distances.items(), key=operator.itemgetter(1)):\\n\\n        proper_formula = \\'\\'\\n        for char in compound[0]:\\n            try:\\n                proper_formula += \\'_{}\\'.format(char)\\n            except ValueError:\\n                proper_formula += char\\n\\n        x_ticklabels.append(r\\'$\\\\mathrm{%s}$\\' % proper_formula)\\n        hull_distance = hull_distances[compound[0]] * 1000\\n\\n        # Good chance of stability\\n        if hull_distance < 100:\\n            color_code = 0.5\\n\\n        # Decent chance of stability\\n        elif hull_distance < 200:\\n            color_code = 0.71\\n\\n        # Poor chance of stability\\n        else:\\n            color_code = 0.92\\n\\n        ax.add_patch(plt.Rectangle((i + 0.1, 0), height=hull_distance,\\n                                   width=0.8, linewidth=0,\\n                                   facecolor=plt.cm.jet(color_code)))\\n        i += 1\\n\\n    ax.set_xticks([x + 0.5 for x in range(len(hull_distances))])\\n    ax.set_xticklabels(x_ticklabels, family=\\'serif\\', size=20, rotation=60)\\n    ax.set_yticklabels(ax.get_yticks(), family=\\'serif\\', size=20)\\n    ax.set_ylabel(r\\'$\\\\mathrm{E_F\\\\/(meV/atom)}$\\', size=40)\\n\\n    plt.savefig(\\'stability_plot.{}\\'.format(fmt), transparent=True)\\n',\n",
       "  '\"\"\"\\n=============================\\nLedoit-Wolf vs OAS estimation\\n=============================\\n\\nThe usual covariance maximum likelihood estimate can be regularized\\nusing shrinkage. Ledoit and Wolf proposed a close formula to compute\\nthe asymptotically optimal shrinkage parameter (minimizing a MSE\\ncriterion), yielding the Ledoit-Wolf covariance estimate.\\n\\nChen et al. proposed an improvement of the Ledoit-Wolf shrinkage\\nparameter, the OAS coefficient, whose convergence is significantly\\nbetter under the assumption that the data are Gaussian.\\n\\nThis example, inspired from Chen\\'s publication [1], shows a comparison\\nof the estimated MSE of the LW and OAS methods, using Gaussian\\ndistributed data.\\n\\n[1] \"Shrinkage Algorithms for MMSE Covariance Estimation\"\\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\\n\\n\"\"\"\\nprint(__doc__)\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom scipy.linalg import toeplitz, cholesky\\n\\nfrom sklearn.covariance import LedoitWolf, OAS\\n\\nnp.random.seed(0)\\n###############################################################################\\nn_features = 100\\n# simulation covariance matrix (AR(1) process)\\nr = 0.1\\nreal_cov = toeplitz(r ** np.arange(n_features))\\ncoloring_matrix = cholesky(real_cov)\\n\\nn_samples_range = np.arange(6, 31, 1)\\nrepeat = 100\\nlw_mse = np.zeros((n_samples_range.size, repeat))\\noa_mse = np.zeros((n_samples_range.size, repeat))\\nlw_shrinkage = np.zeros((n_samples_range.size, repeat))\\noa_shrinkage = np.zeros((n_samples_range.size, repeat))\\nfor i, n_samples in enumerate(n_samples_range):\\n    for j in range(repeat):\\n        X = np.dot(\\n            np.random.normal(size=(n_samples, n_features)), coloring_matrix.T)\\n\\n        lw = LedoitWolf(store_precision=False, assume_centered=True)\\n        lw.fit(X)\\n        lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)\\n        lw_shrinkage[i, j] = lw.shrinkage_\\n\\n        oa = OAS(store_precision=False, assume_centered=True)\\n        oa.fit(X)\\n        oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)\\n        oa_shrinkage[i, j] = oa.shrinkage_\\n\\n# plot MSE\\nplt.subplot(2, 1, 1)\\nplt.errorbar(n_samples_range, lw_mse.mean(1), yerr=lw_mse.std(1),\\n             label=\\'Ledoit-Wolf\\', color=\\'g\\')\\nplt.errorbar(n_samples_range, oa_mse.mean(1), yerr=oa_mse.std(1),\\n             label=\\'OAS\\', color=\\'r\\')\\nplt.ylabel(\"Squared error\")\\nplt.legend(loc=\"upper right\")\\nplt.title(\"Comparison of covariance estimators\")\\nplt.xlim(5, 31)\\n\\n# plot shrinkage coefficient\\nplt.subplot(2, 1, 2)\\nplt.errorbar(n_samples_range, lw_shrinkage.mean(1), yerr=lw_shrinkage.std(1),\\n             label=\\'Ledoit-Wolf\\', color=\\'g\\')\\nplt.errorbar(n_samples_range, oa_shrinkage.mean(1), yerr=oa_shrinkage.std(1),\\n             label=\\'OAS\\', color=\\'r\\')\\nplt.xlabel(\"n_samples\")\\nplt.ylabel(\"Shrinkage\")\\nplt.legend(loc=\"lower right\")\\nplt.ylim(plt.ylim()[0], 1. + (plt.ylim()[1] - plt.ylim()[0]) / 10.)\\nplt.xlim(5, 31)\\n\\nplt.show()\\n'],\n",
       " 'license': ['mit', 'bsd-3-clause']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d107347a-f3dd-4d6f-a9ad-06ac740a5427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom datasets import load_dataset, DatasetDict\\n\\nds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\\nds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\\n\\nraw_datasets = DatasetDict(\\n    {\\n        \"train\": ds_train,\\n        \"valid\": ds_valid\\n    }\\n)\\nraw_datasets\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,\n",
    "        \"valid\": ds_valid\n",
    "    }\n",
    ")\n",
    "raw_datasets\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c6a527-851f-4303-902a-7874c488e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME\n",
      "PATH\n",
      "COPIES\n",
      "SIZE\n",
      "CONTENT\n",
      "LICENSE\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba7c926c-b537-420c-a139-0c70162b84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: henniggroup/MPInterfaces\n",
      "PATH: mpinterfaces/mat2d/stability/analysis.py\n",
      "COPIES: 2\n",
      "SIZE: 5881\n",
      "CONTENT: from __future__ import print_function, division, unicode_literals\n",
      "\n",
      "import operator\n",
      "import os\n",
      "\n",
      "import matplotlib as mpl\n",
      "mpl.use('Agg')\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from pymatgen.core.structure impo\n",
      "LICENSE: mit\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dc0a05a-9d94-4a48-8a6d-b20fc8664ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: henniggroup/MPInterfaces\n",
      "PATH: mpinterfaces/mat2d/stability/analysis.py\n",
      "COPIES: 2\n",
      "SIZE: 5881\n",
      "CONTENT: from __future__ import print_function, division, unicode_literals\n",
      "\n",
      "import operator\n",
      "import os\n",
      "\n",
      "import matplotlib as mpl\n",
      "mpl.use('Agg')\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from pymatgen.core.structure impo\n",
      "LICENSE: mit\n"
     ]
    }
   ],
   "source": [
    "for k, v in raw_datasets[\"train\"][0].items():\n",
    "    print(f\"{k.upper()}: {v[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c2ef929-3694-4c51-bfb2-9d9ecd713592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: henniggroup/MPInterfaces\n",
      "PATH: mpinterfaces/mat2d/stability/analysis.py\n",
      "COPIES: 2\n",
      "SIZE: 5881\n",
      "CONTENT: from __future__ import print_function, division, unicode_literals\n",
      "\n",
      "import operator\n",
      "import os\n",
      "\n",
      "import matplotlib as mpl\n",
      "mpl.use('Agg')\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from pymatgen.core.structure impo\n",
      "LICENSE: mit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{print(f\"{k.upper()}: {v[:200]}\") for k, v in raw_datasets[\"train\"][0].items()}\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e5ad75c-6bfb-43f6-9f9f-ec2610a2824f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': 'henniggroup/MPInterfaces',\n",
       " 'path': 'mpinterfaces/mat2d/stability/analysis.py',\n",
       " 'copies': '2',\n",
       " 'size': '5881',\n",
       " 'content': 'from __future__ import print_function, division, unicode_literals\\n\\nimport operator\\nimport os\\n\\nimport matplotlib as mpl\\nmpl.use(\\'Agg\\')\\nimport matplotlib.pyplot as plt\\n\\nfrom pymatgen.core.structure import Structure\\nfrom pymatgen.entries.computed_entries import ComputedEntry\\nfrom pymatgen.io.vasp.outputs import Vasprun\\n#from pymatgen.analysis.phase_diagram import PDAnalyzer\\nfrom pymatgen.analysis.phase_diagram import PhaseDiagram\\n\\nfrom mpinterfaces.utils import is_converged\\nfrom mpinterfaces.mat2d import MPR\\n\\n__author__ = \"Michael Ashton\"\\n__copyright__ = \"Copyright 2017, Henniggroup\"\\n__maintainer__ = \"Michael Ashton\"\\n__email__ = \"ashtonmv@gmail.com\"\\n__status__ = \"Production\"\\n__date__ = \"March 3, 2017\"\\n\\n\\ndef get_competing_phases():\\n    \"\"\"\\n    Collect the species to which the material might decompose to.\\n\\n    Returns:\\n        A list of phases as tuples formatted as\\n        [(formula_1, Materials_Project_ID_1),\\n        (formula_2, Materials_Project_ID_2), ...]\\n    \"\"\"\\n\\n    composition = Structure.from_file(\\'POSCAR\\').composition\\n    try:\\n        energy = Vasprun(\\'vasprun.xml\\').final_energy\\n    except:\\n        energy = 100  # The function can work without a vasprun.xml\\n    entries = MPR.get_entries_in_chemsys([elt.symbol for elt in composition])\\n    my_entry = ComputedEntry(composition, energy)\\n    entries.append(my_entry)\\n\\n    #pda = PDAnalyzer(PhaseDiagram(entries))\\n    pda = PhaseDiagram(entries)\\n    decomp = pda.get_decomp_and_e_above_hull(my_entry, allow_negative=True)\\n    competing_phases = [(entry.composition.reduced_formula, entry.entry_id)\\n                        for entry in decomp[0]]\\n\\n    return competing_phases\\n\\n\\ndef get_hull_distance(competing_phase_directory=\\'../competing_phases\\'):\\n    \"\"\"\\n    Calculate the material\\'s distance to the thermodynamic hull,\\n    based on species in the Materials Project database.\\n\\n    Args:\\n        competing_phase_directory (str): absolute or relative path\\n            to the location where your competing phases have been\\n            relaxed. The default expectation is that they are stored\\n            in a directory named \\'competing_phases\\' at the same level\\n            as your material\\'s relaxation directory.\\n    Returns:\\n        float: distance (eV/atom) between the material and the\\n            hull.\\n    \"\"\"\\n\\n    finished_competitors = {}\\n    original_directory = os.getcwd()\\n    # Determine which competing phases have been relaxed in the current\\n    # framework and store them in a dictionary ({formula: entry}).\\n    if os.path.isdir(competing_phase_directory):\\n        os.chdir(competing_phase_directory)\\n        for comp_dir in [dir for dir in os.listdir(os.getcwd())\\n                         if os.path.isdir(dir) and is_converged(dir)]:\\n            vasprun = Vasprun(\\'{}/vasprun.xml\\'.format(comp_dir))\\n            composition = vasprun.final_structure.composition\\n            energy = vasprun.final_energy\\n            finished_competitors[comp_dir] = ComputedEntry(composition, energy)\\n        os.chdir(original_directory)\\n    else:\\n        raise ValueError(\\'Competing phase directory does not exist.\\')\\n\\n    composition = Structure.from_file(\\'POSCAR\\').composition\\n    try:\\n        energy = Vasprun(\\'vasprun.xml\\').final_energy\\n    except:\\n        raise ValueError(\\'This directory does not have a converged vasprun.xml\\')\\n    my_entry = ComputedEntry(composition, energy)  # 2D material\\n    entries = MPR.get_entries_in_chemsys([elt.symbol for elt in composition])\\n\\n    # If the energies of competing phases have been calculated in\\n    # the current framework, put them in the phase diagram instead\\n    # of the MP energies.\\n    for i in range(len(entries)):\\n        formula = entries[i].composition.reduced_formula\\n        if formula in finished_competitors:\\n            entries[i] = finished_competitors[formula]\\n        else:\\n            entries[i] = ComputedEntry(entries[i].composition, 100)\\n\\n    entries.append(my_entry)  # 2D material\\n\\n    #pda = PDAnalyzer(PhaseDiagram(entries))\\n    pda = PhaseDiagram(entries)\\n    decomp = pda.get_decomp_and_e_above_hull(my_entry, allow_negative=True)\\n\\n    return decomp[1]\\n\\n\\ndef plot_hull_distances(hull_distances, fmt=\\'pdf\\'):\\n    \"\"\"\\n    Create a bar graph of the formation energies of several 2D materials.\\n\\n    Args:\\n        hull_distances (dict): follow the format:\\n            {reduced_formula: hull_distance (in eV/atom)}\\n        fmt (str): matplotlib format style. Check the matplotlib\\n            docs for options.\\n    \"\"\"\\n\\n    hsize = 12 + (len(hull_distances) - 4) / 3\\n    ax = plt.figure(figsize=(hsize, 10)).gca()\\n    ax.set_ylim(0, 700)\\n    ax.set_xlim(0, len(hull_distances))\\n\\n    x_ticklabels = []\\n    i = 0\\n    for compound in sorted(hull_distances.items(), key=operator.itemgetter(1)):\\n\\n        proper_formula = \\'\\'\\n        for char in compound[0]:\\n            try:\\n                proper_formula += \\'_{}\\'.format(char)\\n            except ValueError:\\n                proper_formula += char\\n\\n        x_ticklabels.append(r\\'$\\\\mathrm{%s}$\\' % proper_formula)\\n        hull_distance = hull_distances[compound[0]] * 1000\\n\\n        # Good chance of stability\\n        if hull_distance < 100:\\n            color_code = 0.5\\n\\n        # Decent chance of stability\\n        elif hull_distance < 200:\\n            color_code = 0.71\\n\\n        # Poor chance of stability\\n        else:\\n            color_code = 0.92\\n\\n        ax.add_patch(plt.Rectangle((i + 0.1, 0), height=hull_distance,\\n                                   width=0.8, linewidth=0,\\n                                   facecolor=plt.cm.jet(color_code)))\\n        i += 1\\n\\n    ax.set_xticks([x + 0.5 for x in range(len(hull_distances))])\\n    ax.set_xticklabels(x_ticklabels, family=\\'serif\\', size=20, rotation=60)\\n    ax.set_yticklabels(ax.get_yticks(), family=\\'serif\\', size=20)\\n    ax.set_ylabel(r\\'$\\\\mathrm{E_F\\\\/(meV/atom)}$\\', size=40)\\n\\n    plt.savefig(\\'stability_plot.{}\\'.format(fmt), transparent=True)\\n',\n",
       " 'license': 'mit'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8df0210a-1ab1-48d0-9ca7-ac3a51725538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 20\n",
      "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 61, 128, 128, 128, 128, 128, 128, 128, 28]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")  # returns when return_length is set to True \n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e53ad89-17fc-4f68-8f6b-a52163805608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "dict_keys(['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'])\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs)) #BatchEncoding, subclass of dictionary \n",
    "#print(outputs)\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffe445a6-f278-4498-8191-15211582a86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "431ab30e-57c1-4661-9880-3f29c9b7c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cec38e6-ac9c-40f3-82ca-66fbe23ad182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['repo_name', 'path', 'copies', 'size', 'content', 'license']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38927acd-c92e-41d0-ad27-04ce5e56d30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": raw_datasets[\"train\"].select(range(100)),\n",
    "        \"valid\": raw_datasets[\"valid\"].select(range(50))\n",
    "    }\n",
    ")\n",
    "sample_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc79bb62-988a-40f8-9d13-dd68cab8dc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='huggingface-course/code-search-net-tokenizer', vocab_size=50000, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fdead59-d3db-445a-bac3-94c473a0ec21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['input_ids', 'attention_mask'], 'valid': ['input_ids', 'attention_mask']}\n",
      "1469\n",
      "1469\n",
      "{'input_ids': [973, 1289, 5964, 612, 978, 781, 63, 1619, 12, 14233, 12, 2536, 63, 35824, 173, 173, 2745, 4043, 173, 2745, 756, 173, 173, 2745, 4855, 442, 13263, 173, 862, 14, 1232, 359, 15579, 397, 173, 2745, 4855, 14, 11032, 442, 2564, 173, 173, 973, 23116, 14, 1341, 14, 4220, 978, 12332, 173, 973, 23116, 14, 4264, 14, 11921, 63, 4264, 978, 34156, 4792, 173, 973, 23116, 14, 1425, 14, 38632, 14, 4966, 978, 44178, 1071, 173, 3, 973, 23116, 14, 5188, 14, 6219, 63, 17670, 978, 38534, 25302, 173, 973, 23116, 14, 5188, 14, 6219, 63, 17670, 978, 19067, 23923, 173, 173, 973, 6800, 7789, 14, 1377, 978, 300, 63, 27052, 173, 973, 6800, 7789, 14, 803, 18, 68, 978, 627, 9743, 173, 173, 612, 3409, 612, 233, 333, 45, 42284, 3726, 524, 223, 2, 173, 612, 21975, 612, 233, 333, 7698, 1495, 11991, 12, 865, 35668, 399, 902, 2, 173, 612, 32785, 612, 233, 333, 45, 42284, 3726, 524, 223, 2, 173, 612, 3104, 612, 233, 333, 352, 524, 223, 6573, 32, 29024, 14, 1026, 2, 173, 612, 1167, 612, 233, 333, 1304, 6995, 2, 173, 612, 526, 612, 233, 333, 45, 906, 869, 12, 11991, 2, 4391, 173, 295, 549, 63, 37191, 63, 20840, 794, 232, 290, 232, 12342, 256, 7591, 292, 947, 256, 10238, 4367, 27194, 292, 14, 312, 734, 26, 222, 393, 495, 311, 17730, 442, 4193, 4463, 442, 222, 3947, 9367, 63, 17, 12, 35168, 83, 63, 8090, 63, 1119, 63, 17, 491, 222, 308, 9367, 63, 18, 12, 35168, 83, 63, 8090, 63, 1119, 63, 18, 491, 10763, 232, 290, 312, 13228, 233, 12332, 14, 973, 63, 431, 359, 2251, 40899, 2258, 7304, 232, 646, 26, 222, 5681, 233, 44178, 1071, 359, 45362, 14, 2075, 2258, 4203, 63, 6133, 232, 663, 26, 222, 5681, 233, 3038, 179, 294, 507, 753, 796, 1334, 2826, 231, 35820, 14, 2075, 232, 3261, 233, 627, 9743, 14, 322, 63, 4264, 63, 219, 63, 479, 516, 715, 929, 10656, 14, 3742, 296, 11225, 253, 13228, 535, 232, 2673, 63, 2108, 233, 34156, 4792, 8, 7304, 12, 5681, 9, 232, 3261, 14, 576, 8, 2028, 63, 2108, 9, 312, 294, 36903, 233, 38534, 25302, 8, 16542, 23923, 8, 4264, 353, 232, 38767, 233, 19067, 23923, 8, 4264, 9, 232, 19443, 233, 38767, 14, 322, 63, 29713, 63, 402, 63, 69, 63, 13743, 63, 14564, 8, 2028, 63, 2108, 12, 1613, 63, 7652, 29, 678, 9, 232, 36345, 63, 20840, 233, 3947, 2108, 14, 7304, 14, 14261, 63, 9367, 12, 1704, 14, 2108, 63, 293, 9, 531, 296, 1704, 253, 19443, 59, 16, 1730, 312, 302, 36345, 63, 20840, 4391, 173, 295, 549, 63, 14564, 63, 3314, 8, 37191, 63, 6219, 63, 2518, 567, 10161, 37191, 63, 20840, 1032, 232, 290, 232, 5391, 256, 10238, 975, 2550, 292, 256, 31094, 15957, 12, 232, 2249, 517, 7591, 253, 256, 35168, 83, 8460, 2215, 14, 312, 1169, 26, 222, 36345, 63, 6219, 63, 2518, 308, 427, 274, 4381, 385, 3193, 661, 241, 292, 256, 1868, 1548, 2827, 36345, 17730, 1054, 2085, 241, 14053, 301, 14, 507, 713, 12444, 300, 542, 2469, 602, 3394, 241, 253, 231, 1517, 3533, 269, 37191, 63, 20840, 7, 815, 256, 1496, 1764, 241, 442, 2827, 10238, 975, 18637, 1517, 14, 232, 734, 26, 222, 1005, 26, 2550, 308, 27076, 15, 4444, 9, 1946, 256, 10238, 350, 256, 241, 15957, 14, 232, 290, 312, 6470, 63, 1026, 6591, 14405, 233, 677, 232, 2405, 63, 2518, 233, 756, 14, 8291, 323, 232, 294, 7331, 947, 36345, 17730, 1054, 2085, 14053, 301, 253, 256, 966, 232, 294, 12314, 350, 2689, 2266, 253, 231, 1368, 6704, 9367, 26, 1704, 15664, 232, 264, 756, 14, 419, 14, 4893, 8, 37191, 63, 6219, 63, 2518, 274, 222, 756, 14, 9404, 8, 37191, 63, 6219, 63, 2518, 9, 222, 296, 824, 63, 679, 253, 404, 679, 296, 2563, 253, 756, 14, 6781, 8, 748, 14, 8291, 1002, 373, 264, 756, 14, 419, 14, 4893, 8, 679, 9, 350, 300, 63, 27052, 8, 679, 18954, 241, 35820, 233, 44178, 1071, 40387, 45362, 14, 2075, 721, 514, 8, 968, 63, 679, 353, 241, 13228, 233, 35820, 14, 4203, 63, 4220, 14, 7304, 241, 5681, 233, 35820, 14, 4203, 63, 6133, 241, 6470, 63, 1026, 6591, 14405, 59, 968, 63, 679, 61, 233, 34156, 4792, 8, 7304, 12, 5681, 9, 222, 756, 14, 9404, 8, 4884, 63, 2518, 9, 232, 425, 26, 222, 510, 911, 359, 6576, 24438, 4926, 1517, 1313, 339, 1084, 1888, 312, 13228, 233, 12332, 14, 973, 63, 431, 359, 2251, 40899, 2258, 7304, 232, 646, 26, 222, 5681, 233, 44178, 1071, 359, 45362, 14, 2075, 2258, 4203, 63, 6133, 232, 663, 26, 222, 510, 911, 359, 3842, 1517, 1313, 339, 1054, 231, 19259, 35820, 14, 2075, 397, 232, 2673, 63, 2108, 233, 34156, 4792, 8, 7304, 12, 5681, 9, 179, 294, 554, 36, 10238, 232, 3261, 233, 627, 9743, 14, 322, 63, 4264, 63, 219, 63, 479, 516, 715, 929, 10656, 14, 3742, 296, 11225, 253, 13228, 535, 312, 294, 647, 256, 14328, 311, 36345, 17730, 1054, 2085, 5195, 253, 232, 294, 256, 966, 12314, 12, 4124, 2266, 253, 256, 4926, 14215, 2478, 232, 294, 311, 256, 20882, 14328, 14, 232, 296, 234, 253, 1004, 8, 563, 8, 4264, 1569, 222, 7661, 233, 3261, 59, 73, 848, 7304, 14, 14261, 63, 9367, 222, 264, 7661, 253, 6470, 63, 1026, 6591, 14405, 26, 241, 3261, 59, 73, 61, 233, 6470, 63, 1026, 6591, 14405, 59, 9367, 61, 222, 425, 26, 241, 3261, 59, 73, 61, 233, 34156, 4792, 8, 4264, 59, 73, 848, 7304, 12, 3038, 9, 312, 3261, 14, 576, 8, 2028, 63, 2108, 9, 179, 294, 554, 36, 10238, 312, 294, 36903, 233, 38534, 25302, 8, 16542, 23923, 8, 4264, 353, 232, 38767, 233, 19067, 23923, 8, 4264, 9, 232, 19443, 233, 38767, 14, 322, 63, 29713, 63, 402, 63, 69, 63, 13743, 63, 14564, 8, 2028, 63, 2108, 12, 1613, 63, 7652, 29, 678, 9, 312, 302, 19443, 59, 17, 61, 4391, 173, 295, 1676, 63, 14564, 63, 9870, 8, 14564, 63, 9870, 12, 3602, 567, 4737, 1032, 232, 290, 232, 1857, 231, 4002, 1967, 311, 256, 37879, 14328, 311, 8208, 554, 36, 30570, 14, 312, 1169, 26, 222, 15957, 63, 9870, 308, 645, 274, 1950, 256, 1115, 26, 241, 420, 14261, 63, 9367, 26, 15957, 63, 3314, 308, 219, 28907, 15, 4444, 4404, 222, 3602, 308, 427, 274, 4855, 1115, 2942, 14, 2102, 256, 4855, 241, 6191, 296, 1401, 14, 232, 290, 312, 366, 708, 233, 3086, 382, 308, 563, 8, 14564, 63, 9870, 9, 415, 1163, 9, 823, 869, 232, 1003, 233, 2564, 14, 5662, 8, 9792, 2471, 72, 708, 12, 2009, 4342, 13947, 323, 232, 1003, 14, 416, 63, 8093, 8, 16, 12, 37896, 9, 232, 1003, 14, 416, 63, 8236, 8, 16, 12, 553, 8, 14564, 63, 9870, 353, 312, 548, 63, 10230, 233, 787, 232, 234, 233, 443, 232, 296, 8046, 253, 2058, 8, 14564, 63, 9870, 14, 1042, 1283, 521, 29, 5061, 14, 15505, 8, 17, 1569, 298, 1498, 63, 9367, 233, 2107, 222, 296, 1524, 253, 8046, 59, 16, 1356, 241, 646, 26, 284, 1498, 63, 9367, 793, 2526, 4255, 514, 8, 1869, 9, 241, 663, 911, 26, 284, 1498, 63, 9367, 793, 1524, 298, 548, 63, 10230, 14, 576, 8, 82, 29000, 14117, 22929, 83, 44027, 446, 1498, 63, 9367, 9, 222, 15957, 63, 3314, 233, 15957, 63, 9870, 59, 8479, 59, 16, 1730, 408, 4764, 298, 294, 28980, 17813, 311, 24989, 222, 264, 15957, 63, 3314, 659, 3038, 26, 241, 1639, 63, 552, 233, 443, 14, 21, 298, 294, 7372, 273, 17813, 311, 24989, 222, 639, 15957, 63, 3314, 659, 3694, 26, 241, 1639, 63, 552, 233, 443, 14, 12927, 298, 294, 4509, 228, 17813, 311, 24989, 222, 425, 26, 241, 1639, 63, 552, 233, 443, 14, 11223, 298, 1003, 14, 566, 63, 2993, 8, 8436, 14, 26216, 1139, 73, 382, 443, 14, 17, 12, 443, 491, 2734, 29, 14564, 63, 3314, 12, 2935, 2246, 29, 16, 14, 24, 12, 11008, 29, 16, 12, 2935, 18020, 29, 8436, 14, 5248, 14, 21190, 8, 1555, 63, 552, 1322, 222, 234, 793, 396, 312, 1003, 14, 416, 63, 14423, 929, 88, 382, 443, 14, 21, 296, 548, 253, 1004, 8, 563, 8, 14564, 63, 9870, 15695, 232, 1003, 14, 416, 63, 17598, 8, 88, 63, 10230, 12, 6102, 567, 34913, 340, 1208, 29, 2368, 12, 5760, 29, 3330, 9, 232, 1003, 14, 416, 63, 18471, 8, 1027, 14, 322, 63, 15626, 1283, 6102, 567, 34913, 340, 1208, 29, 2368, 9, 232, 1003, 14, 416, 63, 6988, 8, 82, 29000, 14117, 91, 37, 63, 38, 60, 5747, 240, 54, 15, 4444, 4404, 7349, 1208, 29, 4586, 9, 312, 2564, 14, 11160, 359, 36911, 63, 1389, 32886, 514, 8, 3638, 491, 16679, 29, 678, 9, 173], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['from', 'Ġ__', 'future', '__', 'Ġimport', 'Ġprint', '_', 'function', ',', 'Ġdivision', ',', 'Ġunicode', '_', 'literals', 'Ċ', 'Ċ', 'import', 'Ġoperator', 'Ċ', 'import', 'Ġos', 'Ċ', 'Ċ', 'import', 'Ġmatplotlib', 'Ġas', 'Ġmpl', 'Ċ', 'mpl', '.', 'use', \"('\", 'Agg', \"')\", 'Ċ', 'import', 'Ġmatplotlib', '.', 'pyplot', 'Ġas', 'Ġplt', 'Ċ', 'Ċ', 'from', 'Ġpymatgen', '.', 'core', '.', 'structure', 'Ġimport', 'ĠStructure', 'Ċ', 'from', 'Ġpymatgen', '.', 'entries', '.', 'computed', '_', 'entries', 'Ġimport', 'ĠComputed', 'Entry', 'Ċ', 'from', 'Ġpymatgen', '.', 'io', '.', 'vasp', '.', 'outputs', 'Ġimport', 'ĠVasp', 'run', 'Ċ', '#', 'from', 'Ġpymatgen', '.', 'analysis', '.', 'phase', '_', 'diagram', 'Ġimport', 'ĠPD', 'Analyzer', 'Ċ', 'from', 'Ġpymatgen', '.', 'analysis', '.', 'phase', '_', 'diagram', 'Ġimport', 'ĠPhase', 'Diagram', 'Ċ', 'Ċ', 'from', 'Ġmp', 'interfaces', '.', 'utils', 'Ġimport', 'Ġis', '_', 'converged', 'Ċ', 'from', 'Ġmp', 'interfaces', '.', 'mat', '2', 'd', 'Ġimport', 'ĠM', 'PR', 'Ċ', 'Ċ', '__', 'author', '__', 'Ġ=', 'Ġ\"', 'M', 'ichael', 'ĠAs', 'ht', 'on', '\"', 'Ċ', '__', 'copyright', '__', 'Ġ=', 'Ġ\"', 'Copy', 'right', 'Ġ2017', ',', 'ĠH', 'enn', 'ig', 'group', '\"', 'Ċ', '__', 'maintainer', '__', 'Ġ=', 'Ġ\"', 'M', 'ichael', 'ĠAs', 'ht', 'on', '\"', 'Ċ', '__', 'email', '__', 'Ġ=', 'Ġ\"', 'as', 'ht', 'on', 'mv', '@', 'gmail', '.', 'com', '\"', 'Ċ', '__', 'status', '__', 'Ġ=', 'Ġ\"', 'Pro', 'duction', '\"', 'Ċ', '__', 'date', '__', 'Ġ=', 'Ġ\"', 'M', 'arch', 'Ġ3', ',', 'Ġ2017', '\"', 'ĊĊ', 'Ċ', 'def', 'Ġget', '_', 'competing', '_', 'phases', '():', 'ĊĠĠĠ', 'Ġ\"\"\"', 'ĊĠĠĠ', 'ĠCollect', 'Ġthe', 'Ġspecies', 'Ġto', 'Ġwhich', 'Ġthe', 'Ġmaterial', 'Ġmight', 'Ġdecompose', 'Ġto', '.', 'ĊĊĠĠĠ', 'ĠReturns', ':', 'ĊĠĠĠĠĠĠĠ', 'ĠA', 'Ġlist', 'Ġof', 'Ġphases', 'Ġas', 'Ġtuples', 'Ġformatted', 'Ġas', 'ĊĠĠĠĠĠĠĠ', 'Ġ[(', 'formula', '_', '1', ',', 'ĠMaterial', 's', '_', 'Project', '_', 'ID', '_', '1', '),', 'ĊĠĠĠĠĠĠĠ', 'Ġ(', 'formula', '_', '2', ',', 'ĠMaterial', 's', '_', 'Project', '_', 'ID', '_', '2', '),', 'Ġ...]', 'ĊĠĠĠ', 'Ġ\"\"\"', 'ĊĊĠĠĠ', 'Ġcomposition', 'Ġ=', 'ĠStructure', '.', 'from', '_', 'file', \"('\", 'PO', 'SCAR', \"').\", 'composition', 'ĊĠĠĠ', 'Ġtry', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġenergy', 'Ġ=', 'ĠVasp', 'run', \"('\", 'vasprun', '.', 'xml', \"').\", 'final', '_', 'energy', 'ĊĠĠĠ', 'Ġexcept', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġenergy', 'Ġ=', 'Ġ100', 'Ġ', 'Ġ#', 'ĠThe', 'Ġfunction', 'Ġcan', 'Ġwork', 'Ġwithout', 'Ġa', 'Ġvasprun', '.', 'xml', 'ĊĠĠĠ', 'Ġentries', 'Ġ=', 'ĠM', 'PR', '.', 'get', '_', 'entries', '_', 'in', '_', 'che', 'ms', 'ys', '([', 'elt', '.', 'symbol', 'Ġfor', 'Ġelt', 'Ġin', 'Ġcomposition', '])', 'ĊĠĠĠ', 'Ġmy', '_', 'entry', 'Ġ=', 'ĠComputed', 'Entry', '(', 'composition', ',', 'Ġenergy', ')', 'ĊĠĠĠ', 'Ġentries', '.', 'append', '(', 'my', '_', 'entry', ')', 'ĊĊĠĠĠ', 'Ġ#', 'pda', 'Ġ=', 'ĠPD', 'Analyzer', '(', 'Phase', 'Diagram', '(', 'entries', '))', 'ĊĠĠĠ', 'Ġpda', 'Ġ=', 'ĠPhase', 'Diagram', '(', 'entries', ')', 'ĊĠĠĠ', 'Ġdecomp', 'Ġ=', 'Ġpda', '.', 'get', '_', 'decomp', '_', 'and', '_', 'e', '_', 'above', '_', 'hull', '(', 'my', '_', 'entry', ',', 'Ġallow', '_', 'negative', '=', 'True', ')', 'ĊĠĠĠ', 'Ġcompeting', '_', 'phases', 'Ġ=', 'Ġ[(', 'entry', '.', 'composition', '.', 'reduced', '_', 'formula', ',', 'Ġentry', '.', 'entry', '_', 'id', ')', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġfor', 'Ġentry', 'Ġin', 'Ġdecomp', '[', '0', ']]', 'ĊĊĠĠĠ', 'Ġreturn', 'Ġcompeting', '_', 'phases', 'ĊĊ', 'Ċ', 'def', 'Ġget', '_', 'hull', '_', 'distance', '(', 'competing', '_', 'phase', '_', 'directory', \"='\", '../', 'competing', '_', 'phases', \"'):\", 'ĊĠĠĠ', 'Ġ\"\"\"', 'ĊĠĠĠ', 'ĠCalculate', 'Ġthe', 'Ġmaterial', \"'s\", 'Ġdistance', 'Ġto', 'Ġthe', 'Ġthermodynamic', 'Ġhull', ',', 'ĊĠĠĠ', 'Ġbased', 'Ġon', 'Ġspecies', 'Ġin', 'Ġthe', 'ĠMaterial', 's', 'ĠProject', 'Ġdatabase', '.', 'ĊĊĠĠĠ', 'ĠArgs', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġcompeting', '_', 'phase', '_', 'directory', 'Ġ(', 'str', '):', 'Ġabsolute', 'Ġor', 'Ġrelative', 'Ġpath', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġto', 'Ġthe', 'Ġlocation', 'Ġwhere', 'Ġyour', 'Ġcompeting', 'Ġphases', 'Ġhave', 'Ġbeen', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġrelax', 'ed', '.', 'ĠThe', 'Ġdefault', 'Ġexpectation', 'Ġis', 'Ġthat', 'Ġthey', 'Ġare', 'Ġstored', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġin', 'Ġa', 'Ġdirectory', 'Ġnamed', \"Ġ'\", 'competing', '_', 'phases', \"'\", 'Ġat', 'Ġthe', 'Ġsame', 'Ġlevel', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġas', 'Ġyour', 'Ġmaterial', \"'s\", 'Ġrelaxation', 'Ġdirectory', '.', 'ĊĠĠĠ', 'ĠReturns', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġfloat', ':', 'Ġdistance', 'Ġ(', 'eV', '/', 'atom', ')', 'Ġbetween', 'Ġthe', 'Ġmaterial', 'Ġand', 'Ġthe', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġhull', '.', 'ĊĠĠĠ', 'Ġ\"\"\"', 'ĊĊĠĠĠ', 'Ġfinished', '_', 'com', 'pet', 'itors', 'Ġ=', 'Ġ{}', 'ĊĠĠĠ', 'Ġoriginal', '_', 'directory', 'Ġ=', 'Ġos', '.', 'getcwd', '()', 'ĊĠĠĠ', 'Ġ#', 'ĠDetermine', 'Ġwhich', 'Ġcompeting', 'Ġphases', 'Ġhave', 'Ġbeen', 'Ġrelax', 'ed', 'Ġin', 'Ġthe', 'Ġcurrent', 'ĊĠĠĠ', 'Ġ#', 'Ġframework', 'Ġand', 'Ġstore', 'Ġthem', 'Ġin', 'Ġa', 'Ġdictionary', 'Ġ({', 'formula', ':', 'Ġentry', '}).', 'ĊĠĠĠ', 'Ġif', 'Ġos', '.', 'path', '.', 'isdir', '(', 'competing', '_', 'phase', '_', 'directory', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġos', '.', 'chdir', '(', 'competing', '_', 'phase', '_', 'directory', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġfor', 'Ġcomp', '_', 'dir', 'Ġin', 'Ġ[', 'dir', 'Ġfor', 'Ġdir', 'Ġin', 'Ġos', '.', 'listdir', '(', 'os', '.', 'getcwd', '())', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġif', 'Ġos', '.', 'path', '.', 'isdir', '(', 'dir', ')', 'Ġand', 'Ġis', '_', 'converged', '(', 'dir', ')]:', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġvasprun', 'Ġ=', 'ĠVasp', 'run', \"('{}/\", 'vasprun', '.', 'xml', \"'.\", 'format', '(', 'comp', '_', 'dir', '))', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġcomposition', 'Ġ=', 'Ġvasprun', '.', 'final', '_', 'structure', '.', 'composition', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġenergy', 'Ġ=', 'Ġvasprun', '.', 'final', '_', 'energy', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġfinished', '_', 'com', 'pet', 'itors', '[', 'comp', '_', 'dir', ']', 'Ġ=', 'ĠComputed', 'Entry', '(', 'composition', ',', 'Ġenergy', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġos', '.', 'chdir', '(', 'original', '_', 'directory', ')', 'ĊĠĠĠ', 'Ġelse', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġraise', 'ĠValueError', \"('\", 'Com', 'peting', 'Ġphase', 'Ġdirectory', 'Ġdoes', 'Ġnot', 'Ġexist', \".')\", 'ĊĊĠĠĠ', 'Ġcomposition', 'Ġ=', 'ĠStructure', '.', 'from', '_', 'file', \"('\", 'PO', 'SCAR', \"').\", 'composition', 'ĊĠĠĠ', 'Ġtry', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġenergy', 'Ġ=', 'ĠVasp', 'run', \"('\", 'vasprun', '.', 'xml', \"').\", 'final', '_', 'energy', 'ĊĠĠĠ', 'Ġexcept', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġraise', 'ĠValueError', \"('\", 'This', 'Ġdirectory', 'Ġdoes', 'Ġnot', 'Ġhave', 'Ġa', 'Ġconverged', 'Ġvasprun', '.', 'xml', \"')\", 'ĊĠĠĠ', 'Ġmy', '_', 'entry', 'Ġ=', 'ĠComputed', 'Entry', '(', 'composition', ',', 'Ġenergy', ')', 'Ġ', 'Ġ#', 'Ġ2', 'D', 'Ġmaterial', 'ĊĠĠĠ', 'Ġentries', 'Ġ=', 'ĠM', 'PR', '.', 'get', '_', 'entries', '_', 'in', '_', 'che', 'ms', 'ys', '([', 'elt', '.', 'symbol', 'Ġfor', 'Ġelt', 'Ġin', 'Ġcomposition', '])', 'ĊĊĠĠĠ', 'Ġ#', 'ĠIf', 'Ġthe', 'Ġenergies', 'Ġof', 'Ġcompeting', 'Ġphases', 'Ġhave', 'Ġbeen', 'Ġcalculated', 'Ġin', 'ĊĠĠĠ', 'Ġ#', 'Ġthe', 'Ġcurrent', 'Ġframework', ',', 'Ġput', 'Ġthem', 'Ġin', 'Ġthe', 'Ġphase', 'Ġdiagram', 'Ġinstead', 'ĊĠĠĠ', 'Ġ#', 'Ġof', 'Ġthe', 'ĠMP', 'Ġenergies', '.', 'ĊĠĠĠ', 'Ġfor', 'Ġi', 'Ġin', 'Ġrange', '(', 'len', '(', 'entries', ')):', 'ĊĠĠĠĠĠĠĠ', 'Ġformula', 'Ġ=', 'Ġentries', '[', 'i', '].', 'composition', '.', 'reduced', '_', 'formula', 'ĊĠĠĠĠĠĠĠ', 'Ġif', 'Ġformula', 'Ġin', 'Ġfinished', '_', 'com', 'pet', 'itors', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġentries', '[', 'i', ']', 'Ġ=', 'Ġfinished', '_', 'com', 'pet', 'itors', '[', 'formula', ']', 'ĊĠĠĠĠĠĠĠ', 'Ġelse', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġentries', '[', 'i', ']', 'Ġ=', 'ĠComputed', 'Entry', '(', 'entries', '[', 'i', '].', 'composition', ',', 'Ġ100', ')', 'ĊĊĠĠĠ', 'Ġentries', '.', 'append', '(', 'my', '_', 'entry', ')', 'Ġ', 'Ġ#', 'Ġ2', 'D', 'Ġmaterial', 'ĊĊĠĠĠ', 'Ġ#', 'pda', 'Ġ=', 'ĠPD', 'Analyzer', '(', 'Phase', 'Diagram', '(', 'entries', '))', 'ĊĠĠĠ', 'Ġpda', 'Ġ=', 'ĠPhase', 'Diagram', '(', 'entries', ')', 'ĊĠĠĠ', 'Ġdecomp', 'Ġ=', 'Ġpda', '.', 'get', '_', 'decomp', '_', 'and', '_', 'e', '_', 'above', '_', 'hull', '(', 'my', '_', 'entry', ',', 'Ġallow', '_', 'negative', '=', 'True', ')', 'ĊĊĠĠĠ', 'Ġreturn', 'Ġdecomp', '[', '1', ']', 'ĊĊ', 'Ċ', 'def', 'Ġplot', '_', 'hull', '_', 'distances', '(', 'hull', '_', 'distances', ',', 'Ġfmt', \"='\", 'pdf', \"'):\", 'ĊĠĠĠ', 'Ġ\"\"\"', 'ĊĠĠĠ', 'ĠCreate', 'Ġa', 'Ġbar', 'Ġgraph', 'Ġof', 'Ġthe', 'Ġformation', 'Ġenergies', 'Ġof', 'Ġseveral', 'Ġ2', 'D', 'Ġmaterials', '.', 'ĊĊĠĠĠ', 'ĠArgs', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġhull', '_', 'distances', 'Ġ(', 'dict', '):', 'Ġfollow', 'Ġthe', 'Ġformat', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġ{', 'reduced', '_', 'formula', ':', 'Ġhull', '_', 'distance', 'Ġ(', 'in', 'ĠeV', '/', 'atom', ')}', 'ĊĠĠĠĠĠĠĠ', 'Ġfmt', 'Ġ(', 'str', '):', 'Ġmatplotlib', 'Ġformat', 'Ġstyle', '.', 'ĠCheck', 'Ġthe', 'Ġmatplotlib', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġdocs', 'Ġfor', 'Ġoptions', '.', 'ĊĠĠĠ', 'Ġ\"\"\"', 'ĊĊĠĠĠ', 'Ġh', 'size', 'Ġ=', 'Ġ12', 'Ġ+', 'Ġ(', 'len', '(', 'hull', '_', 'distances', ')', 'Ġ-', 'Ġ4', ')', 'Ġ/', 'Ġ3', 'ĊĠĠĠ', 'Ġax', 'Ġ=', 'Ġplt', '.', 'figure', '(', 'figsize', '=(', 'h', 'size', ',', 'Ġ10', ')).', 'gca', '()', 'ĊĠĠĠ', 'Ġax', '.', 'set', '_', 'ylim', '(', '0', ',', 'Ġ700', ')', 'ĊĠĠĠ', 'Ġax', '.', 'set', '_', 'xlim', '(', '0', ',', 'Ġlen', '(', 'hull', '_', 'distances', '))', 'ĊĊĠĠĠ', 'Ġx', '_', 'ticklabels', 'Ġ=', 'Ġ[]', 'ĊĠĠĠ', 'Ġi', 'Ġ=', 'Ġ0', 'ĊĠĠĠ', 'Ġfor', 'Ġcompound', 'Ġin', 'Ġsorted', '(', 'hull', '_', 'distances', '.', 'items', '(),', 'Ġkey', '=', 'operator', '.', 'itemgetter', '(', '1', ')):', 'ĊĊĠĠĠĠĠĠĠ', 'Ġproper', '_', 'formula', 'Ġ=', \"Ġ''\", 'ĊĠĠĠĠĠĠĠ', 'Ġfor', 'Ġchar', 'Ġin', 'Ġcompound', '[', '0', ']:', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġtry', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġproper', '_', 'formula', 'Ġ+=', \"Ġ'_\", \"{}'.\", 'format', '(', 'char', ')', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġexcept', 'ĠValueError', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġproper', '_', 'formula', 'Ġ+=', 'Ġchar', 'ĊĊĠĠĠĠĠĠĠ', 'Ġx', '_', 'ticklabels', '.', 'append', '(', 'r', \"'$\\\\\", 'mathrm', '{%', 's', \"}$'\", 'Ġ%', 'Ġproper', '_', 'formula', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġhull', '_', 'distance', 'Ġ=', 'Ġhull', '_', 'distances', '[', 'compound', '[', '0', ']]', 'Ġ*', 'Ġ1000', 'ĊĊĠĠĠĠĠĠĠ', 'Ġ#', 'ĠGood', 'Ġchance', 'Ġof', 'Ġstability', 'ĊĠĠĠĠĠĠĠ', 'Ġif', 'Ġhull', '_', 'distance', 'Ġ<', 'Ġ100', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġcolor', '_', 'code', 'Ġ=', 'Ġ0', '.', '5', 'ĊĊĠĠĠĠĠĠĠ', 'Ġ#', 'ĠDec', 'ent', 'Ġchance', 'Ġof', 'Ġstability', 'ĊĠĠĠĠĠĠĠ', 'Ġelif', 'Ġhull', '_', 'distance', 'Ġ<', 'Ġ200', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġcolor', '_', 'code', 'Ġ=', 'Ġ0', '.', '71', 'ĊĊĠĠĠĠĠĠĠ', 'Ġ#', 'ĠPo', 'or', 'Ġchance', 'Ġof', 'Ġstability', 'ĊĠĠĠĠĠĠĠ', 'Ġelse', ':', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġcolor', '_', 'code', 'Ġ=', 'Ġ0', '.', '92', 'ĊĊĠĠĠĠĠĠĠ', 'Ġax', '.', 'add', '_', 'patch', '(', 'plt', '.', 'Rectangle', '((', 'i', 'Ġ+', 'Ġ0', '.', '1', ',', 'Ġ0', '),', 'Ġheight', '=', 'hull', '_', 'distance', ',', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġwidth', '=', '0', '.', '8', ',', 'Ġlinewidth', '=', '0', ',', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġfacecolor', '=', 'plt', '.', 'cm', '.', 'jet', '(', 'color', '_', 'code', ')))', 'ĊĠĠĠĠĠĠĠ', 'Ġi', 'Ġ+=', 'Ġ1', 'ĊĊĠĠĠ', 'Ġax', '.', 'set', '_', 'xticks', '([', 'x', 'Ġ+', 'Ġ0', '.', '5', 'Ġfor', 'Ġx', 'Ġin', 'Ġrange', '(', 'len', '(', 'hull', '_', 'distances', '))])', 'ĊĠĠĠ', 'Ġax', '.', 'set', '_', 'xticklabels', '(', 'x', '_', 'ticklabels', ',', 'Ġfamily', \"='\", 'serif', \"',\", 'Ġsize', '=', '20', ',', 'Ġrotation', '=', '60', ')', 'ĊĠĠĠ', 'Ġax', '.', 'set', '_', 'yticklabels', '(', 'ax', '.', 'get', '_', 'yticks', '(),', 'Ġfamily', \"='\", 'serif', \"',\", 'Ġsize', '=', '20', ')', 'ĊĠĠĠ', 'Ġax', '.', 'set', '_', 'ylabel', '(', 'r', \"'$\\\\\", 'mathrm', '{', 'E', '_', 'F', '\\\\', '/(', 'me', 'V', '/', 'atom', ')}', \"$',\", 'Ġsize', '=', '40', ')', 'ĊĊĠĠĠ', 'Ġplt', '.', 'savefig', \"('\", 'stability', '_', 'plot', \".{}'.\", 'format', '(', 'fmt', '),', 'Ġtransparent', '=', 'True', ')', 'Ċ']\n",
      "eos_token_id: 0\n"
     ]
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element['content'],\n",
    "        truncation=False, \n",
    "        #max_length=context_length,\n",
    "        #return_overflowing_tokens=True,\n",
    "        #return_length=True,\n",
    "    )\n",
    "    #input_batch = []\n",
    "    #for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
    "    #    if length == context_length:\n",
    "    #        input_batch.append(input_ids)\n",
    "    \n",
    "    #outputs[\"chunked_input_ids\"] = input_batch\n",
    "    #return {\"input_ids\": input_batch}\n",
    "    return outputs\n",
    "\n",
    "tokenized_datasets_sample = sample_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "print(tokenized_datasets_sample.column_names)\n",
    "print(len(tokenized_datasets_sample[\"train\"][\"input_ids\"][0]))\n",
    "print(len(tokenized_datasets_sample[\"train\"][\"attention_mask\"][0]))\n",
    "print(tokenized_datasets_sample[\"train\"][0])\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_datasets_sample[\"train\"][\"input_ids\"][0]))\n",
    "print(f\"eos_token_id: {tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "465deb3a-875f-4687-928a-c4e1d7cd2220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1469, 924, 436, 543, 6127, 637, 682, 4334, 6269, 712, 4182, 11323, 1347, 1893, 1632, 1352, 1148, 534, 486, 2239, 449, 1047, 1071, 354, 1638, 3630, 1452, 844, 525, 4066, 989, 1172, 2588, 1906, 2422, 1625, 2777, 1640, 2232, 781, 2387, 3639, 382, 563, 4014, 507, 1160, 618, 5342, 2169, 962, 756, 326, 1158, 4507, 596, 1209, 434, 803, 5150, 645, 8046, 26806, 483, 461, 1652, 1248, 476, 1230, 1457, 6731, 1293, 1863, 4411, 3983, 500, 740, 844, 1400, 983, 8941, 5088, 1061, 3723, 6185, 7302, 450, 14115, 1289, 997, 862, 634, 1509, 8118, 1974, 1033, 2075, 1447, 5754, 1107]\n"
     ]
    }
   ],
   "source": [
    "print([len(i) for i in tokenized_datasets_sample[\"train\"][\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e0f4451-9a70-4753-8814-e187b7533c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024, 924, 436, 543, 1024, 637, 682, 1024, 1024, 712, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 534, 486, 1024, 449, 1024, 1024, 354, 1024, 1024, 1024, 844, 525, 1024, 989, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 781, 1024, 1024, 382, 563, 1024, 507, 1024, 618, 1024, 1024, 962, 756, 326, 1024, 1024, 596, 1024, 434, 803, 1024, 645, 1024, 1024, 483, 461, 1024, 1024, 476, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 500, 740, 844, 1024, 983, 1024, 1024, 1024, 1024, 1024, 1024, 450, 1024, 1024, 997, 862, 634, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element['content'],\n",
    "        truncation=True,\n",
    "        #max_length=context_length,\n",
    "        #return_overflowing_tokens=True,\n",
    "        #return_length=True,\n",
    "    )\n",
    "    #input_batch =  [\n",
    "    #    input_ids for length, input_ids in zip(outputs['length'], outputs['input_ids']) if length == context_length\n",
    "    #]\n",
    "    \n",
    "    #return {\"input_ids\": input_batch}\n",
    "    return outputs\n",
    "\n",
    "tokenized_datasets = sample_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "print([len(i) for i in tokenized_datasets[\"train\"][\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efb2cc5b-fbeb-42a3-abc7-b41604a44f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1954\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1338\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element['content'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch =  [\n",
    "        input_ids for length, input_ids in zip(outputs['length'], outputs['input_ids']) if length == context_length\n",
    "    ]\n",
    "    \n",
    "    return {\"input_ids\": input_batch}\n",
    "    #return outputs\n",
    "\n",
    "tokenized_datasets = sample_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f38b762a-d3d5-4215-a8d4-503970d679ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1954\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1338\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element['content'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch =  [\n",
    "        outputs[\"input_ids\"] for length in outputs[\"length\"] if length == context_length\n",
    "    ]\n",
    "    \n",
    "    return {\"input_ids\": input_batch}\n",
    "    #return outputs\n",
    "\n",
    "tokenized_datasets = sample_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c266b90b-c14b-44c1-8f57-9510af04e787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef tokenize(element):\\n    outputs = tokenizer(\\n        element[\\'content\\'],\\n        truncation=True,\\n        max_length=context_length,\\n        return_overflowing_tokens=True,\\n        return_length=True,\\n    )\\n    input_batch = []\\n    for length, input_ids in zip(outputs[\\'length\\'], outputs[\\'input_ids\\']):\\n        if length == context_length:\\n            input_batch.append(input_ids)\\n    return {\"input_ids\": input_batch}\\n\\n\\ntokenized_datasets = raw_datasets.map(\\n    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\\ntokenized_datasets\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It takes about 05:08\n",
    "\"\"\"\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element['content'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "tokenized_datasets\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd389985-8d51-4f03-960b-e9b6386ea528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 16, 36, 64]\n",
      "[0, 1, 4, 27, 16, 125, 36, 343, 64, 729]\n"
     ]
    }
   ],
   "source": [
    "print([ x**2 for x in range(10) if x%2==0])\n",
    "print([ x**2 if x%2==0 else x**3 for x in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "552bd8b8-aa5e-40a1-ad12-8e4fee2a41f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 2203760\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 12763\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It takes about 01:40 using list comprehension \n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element['content'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch =  [\n",
    "        input_ids for length, input_ids in zip(outputs['length'], outputs['input_ids']) if length == context_length\n",
    "    ]\n",
    "\n",
    "    #input_batch =  [\n",
    "    #    outputs[\"input_ids\"] for length in outputs[\"length\"] if length == context_length\n",
    "    #]\n",
    "    #print(len(input_batch))\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2be86b4-6225-4aa1-aa4b-a9e75e6c75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c991792-5316-49f9-a100-db903e620531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe7c017e-1e73-4662-a47e-1582bc2566f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e605419-18bd-4c14-84e5-e175f834a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(tokenizer))\n",
    "#print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ecb0440-4c85-43a6-8139-ae27aab2a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='huggingface-course/code-search-net-tokenizer', vocab_size=50000, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tokenizer))\n",
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "792a8b65-ff6a-4808-8b6f-194bd720710a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.39.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aaeecf1-580c-4373-930d-8fb670311219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n",
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4da93f60-a012-4d80-b085-8a1e843a69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e161d195-84be-4588-aff1-ab9bea3bfd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.2M parameters\n",
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 128,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01c0669c-25cc-4969-a95a-5a1d35936608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50000, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba51ac8e-28ef-4009-80f3-0836e901238f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='huggingface-course/code-search-net-tokenizer', vocab_size=50000, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96f38322-dae4-4974-936f-0e3fab69d374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pad token\n",
      "GPT2TokenizerFast(name_or_path='huggingface-course/code-search-net-tokenizer', vocab_size=50000, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token == None: \n",
    "    print (\"No pad token\") \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5e434da-51ef-43de-a510-641fe5880baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f6a6342-a49b-4d76-a387-af9c9d5a6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 128]\n",
      "[{'input_ids': [973, 1289, 5964, 612, 978, 781, 63, 1619, 12, 14233, 12, 2536, 63, 35824, 173, 173, 2745, 4043, 173, 2745, 756, 173, 173, 2745, 4855, 442, 13263, 173, 862, 14, 1232, 359, 15579, 397, 173, 2745, 4855, 14, 11032, 442, 2564, 173, 173, 973, 23116, 14, 1341, 14, 4220, 978, 12332, 173, 973, 23116, 14, 4264, 14, 11921, 63, 4264, 978, 34156, 4792, 173, 973, 23116, 14, 1425, 14, 38632, 14, 4966, 978, 44178, 1071, 173, 3, 973, 23116, 14, 5188, 14, 6219, 63, 17670, 978, 38534, 25302, 173, 973, 23116, 14, 5188, 14, 6219, 63, 17670, 978, 19067, 23923, 173, 173, 973, 6800, 7789, 14, 1377, 978, 300, 63, 27052, 173, 973, 6800, 7789, 14, 803, 18, 68, 978, 627, 9743, 173, 173, 612, 3409, 612, 233]}, {'input_ids': [333, 45, 42284, 3726, 524, 223, 2, 173, 612, 21975, 612, 233, 333, 7698, 1495, 11991, 12, 865, 35668, 399, 902, 2, 173, 612, 32785, 612, 233, 333, 45, 42284, 3726, 524, 223, 2, 173, 612, 3104, 612, 233, 333, 352, 524, 223, 6573, 32, 29024, 14, 1026, 2, 173, 612, 1167, 612, 233, 333, 1304, 6995, 2, 173, 612, 526, 612, 233, 333, 45, 906, 869, 12, 11991, 2, 4391, 173, 295, 549, 63, 37191, 63, 20840, 794, 232, 290, 232, 12342, 256, 7591, 292, 947, 256, 10238, 4367, 27194, 292, 14, 312, 734, 26, 222, 393, 495, 311, 17730, 442, 4193, 4463, 442, 222, 3947, 9367, 63, 17, 12, 35168, 83, 63, 8090, 63, 1119, 63, 17, 491, 222, 308, 9367, 63, 18, 12, 35168, 83]}]\n"
     ]
    }
   ],
   "source": [
    "token_dataset = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "print([len(i[\"input_ids\"]) for i in token_dataset])\n",
    "print([tokenized_datasets[\"train\"][i] for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "529c3b50-7a3d-4aff-b1c3-eb8d1dddc4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([2, 128])\n",
      "attention_mask shape: torch.Size([2, 128])\n",
      "labels shape: torch.Size([2, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  973,  1289,  5964,   612,   978,   781,    63,  1619,    12, 14233,\n",
       "            12,  2536,    63, 35824,   173,   173,  2745,  4043,   173,  2745,\n",
       "           756,   173,   173,  2745,  4855,   442, 13263,   173,   862,    14,\n",
       "          1232,   359, 15579,   397,   173,  2745,  4855,    14, 11032,   442,\n",
       "          2564,   173,   173,   973, 23116,    14,  1341,    14,  4220,   978,\n",
       "         12332,   173,   973, 23116,    14,  4264,    14, 11921,    63,  4264,\n",
       "           978, 34156,  4792,   173,   973, 23116,    14,  1425,    14, 38632,\n",
       "            14,  4966,   978, 44178,  1071,   173,     3,   973, 23116,    14,\n",
       "          5188,    14,  6219,    63, 17670,   978, 38534, 25302,   173,   973,\n",
       "         23116,    14,  5188,    14,  6219,    63, 17670,   978, 19067, 23923,\n",
       "           173,   173,   973,  6800,  7789,    14,  1377,   978,   300,    63,\n",
       "         27052,   173,   973,  6800,  7789,    14,   803,    18,    68,   978,\n",
       "           627,  9743,   173,   173,   612,  3409,   612,   233],\n",
       "        [  333,    45, 42284,  3726,   524,   223,     2,   173,   612, 21975,\n",
       "           612,   233,   333,  7698,  1495, 11991,    12,   865, 35668,   399,\n",
       "           902,     2,   173,   612, 32785,   612,   233,   333,    45, 42284,\n",
       "          3726,   524,   223,     2,   173,   612,  3104,   612,   233,   333,\n",
       "           352,   524,   223,  6573,    32, 29024,    14,  1026,     2,   173,\n",
       "           612,  1167,   612,   233,   333,  1304,  6995,     2,   173,   612,\n",
       "           526,   612,   233,   333,    45,   906,   869,    12, 11991,     2,\n",
       "          4391,   173,   295,   549,    63, 37191,    63, 20840,   794,   232,\n",
       "           290,   232, 12342,   256,  7591,   292,   947,   256, 10238,  4367,\n",
       "         27194,   292,    14,   312,   734,    26,   222,   393,   495,   311,\n",
       "         17730,   442,  4193,  4463,   442,   222,  3947,  9367,    63,    17,\n",
       "            12, 35168,    83,    63,  8090,    63,  1119,    63,    17,   491,\n",
       "           222,   308,  9367,    63,    18,    12, 35168,    83]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  973,  1289,  5964,   612,   978,   781,    63,  1619,    12, 14233,\n",
       "            12,  2536,    63, 35824,   173,   173,  2745,  4043,   173,  2745,\n",
       "           756,   173,   173,  2745,  4855,   442, 13263,   173,   862,    14,\n",
       "          1232,   359, 15579,   397,   173,  2745,  4855,    14, 11032,   442,\n",
       "          2564,   173,   173,   973, 23116,    14,  1341,    14,  4220,   978,\n",
       "         12332,   173,   973, 23116,    14,  4264,    14, 11921,    63,  4264,\n",
       "           978, 34156,  4792,   173,   973, 23116,    14,  1425,    14, 38632,\n",
       "            14,  4966,   978, 44178,  1071,   173,     3,   973, 23116,    14,\n",
       "          5188,    14,  6219,    63, 17670,   978, 38534, 25302,   173,   973,\n",
       "         23116,    14,  5188,    14,  6219,    63, 17670,   978, 19067, 23923,\n",
       "           173,   173,   973,  6800,  7789,    14,  1377,   978,   300,    63,\n",
       "         27052,   173,   973,  6800,  7789,    14,   803,    18,    68,   978,\n",
       "           627,  9743,   173,   173,   612,  3409,   612,   233],\n",
       "        [  333,    45, 42284,  3726,   524,   223,     2,   173,   612, 21975,\n",
       "           612,   233,   333,  7698,  1495, 11991,    12,   865, 35668,   399,\n",
       "           902,     2,   173,   612, 32785,   612,   233,   333,    45, 42284,\n",
       "          3726,   524,   223,     2,   173,   612,  3104,   612,   233,   333,\n",
       "           352,   524,   223,  6573,    32, 29024,    14,  1026,     2,   173,\n",
       "           612,  1167,   612,   233,   333,  1304,  6995,     2,   173,   612,\n",
       "           526,   612,   233,   333,    45,   906,   869,    12, 11991,     2,\n",
       "          4391,   173,   295,   549,    63, 37191,    63, 20840,   794,   232,\n",
       "           290,   232, 12342,   256,  7591,   292,   947,   256, 10238,  4367,\n",
       "         27194,   292,    14,   312,   734,    26,   222,   393,   495,   311,\n",
       "         17730,   442,  4193,  4463,   442,   222,  3947,  9367,    63,    17,\n",
       "            12, 35168,    83,    63,  8090,    63,  1119,    63,    17,   491,\n",
       "           222,   308,  9367,    63,    18,    12, 35168,    83]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "222acf13-badf-401e-8411-e0575950dea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  973,  1289,  5964,   612,   978,   781,    63,  1619,    12, 14233,\n",
      "           12,  2536,    63, 35824,   173,   173,  2745,  4043,   173,  2745,\n",
      "          756,   173,   173,  2745,  4855,   442, 13263,   173,   862,    14,\n",
      "         1232,   359, 15579,   397,   173,  2745,  4855,    14, 11032,   442,\n",
      "         2564,   173,   173,   973, 23116,    14,  1341,    14,  4220,   978,\n",
      "        12332,   173,   973, 23116,    14,  4264,    14, 11921,    63,  4264,\n",
      "          978, 34156,  4792,   173,   973, 23116,    14,  1425,    14, 38632,\n",
      "           14,  4966,   978, 44178,  1071,   173,     3,   973, 23116,    14,\n",
      "         5188,    14,  6219,    63, 17670,   978, 38534, 25302,   173,   973,\n",
      "        23116,    14,  5188,    14,  6219,    63, 17670,   978, 19067, 23923,\n",
      "          173,   173,   973,  6800,  7789,    14,  1377,   978,   300,    63,\n",
      "        27052,   173,   973,  6800,  7789,    14,   803,    18,    68,   978,\n",
      "          627,  9743,   173,   173,   612,  3409,   612,   233])\n",
      "tensor([  973,  1289,  5964,   612,   978,   781,    63,  1619,    12, 14233,\n",
      "           12,  2536,    63, 35824,   173,   173,  2745,  4043,   173,  2745,\n",
      "          756,   173,   173,  2745,  4855,   442, 13263,   173,   862,    14,\n",
      "         1232,   359, 15579,   397,   173,  2745,  4855,    14, 11032,   442,\n",
      "         2564,   173,   173,   973, 23116,    14,  1341,    14,  4220,   978,\n",
      "        12332,   173,   973, 23116,    14,  4264,    14, 11921,    63,  4264,\n",
      "          978, 34156,  4792,   173,   973, 23116,    14,  1425,    14, 38632,\n",
      "           14,  4966,   978, 44178,  1071,   173,     3,   973, 23116,    14,\n",
      "         5188,    14,  6219,    63, 17670,   978, 38534, 25302,   173,   973,\n",
      "        23116,    14,  5188,    14,  6219,    63, 17670,   978, 19067, 23923,\n",
      "          173,   173,   973,  6800,  7789,    14,  1377,   978,   300,    63,\n",
      "        27052,   173,   973,  6800,  7789,    14,   803,    18,    68,   978,\n",
      "          627,  9743,   173,   173,   612,  3409,   612,   233])\n"
     ]
    }
   ],
   "source": [
    "#make sure thatout[\"input_ids\"][0] == out[\"labels\"][0]\n",
    "print(out[\"input_ids\"][0])\n",
    "print(out[\"labels\"][0])\n",
    "\n",
    "#Shifting the inputs and labels to align them happens inside the model, \n",
    "#so the data collator just copies the inputs to create the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbdd1e1b-1421-4a3a-bbb7-a0f2992711b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967e678dd68e47a9ba3a7d9335f18ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65b603b6-bf9a-4cb2-b5cb-f3755b8322dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2203760"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e57e766-1cd3-4667-b041-8891824e8050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    logging_steps=500,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=1000,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9f1d030-6a01-49ff-af86-94b338601e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06b03d28-84ab-4c2d-bd5e-e1d0a90be6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2203760"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b224721-87ed-48b6-a2e7-e1a83c29ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.push_to_hub(commit_message=\"Training complete\", tags=\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48e2f170-5979-486a-ad1c-b1ba2aac7c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model= \"hwang2006/codeparrot-ds\", device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3ca2f07-4417-423e-bdd9-fb782eb9ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# create some data\n",
      "x = np.random.randn(100)\n",
      "y = np.random.randn(100)\n",
      "\n",
      "# create scatter plot with x, y\n",
      "fig, ax = plt.subplots()\n",
      "ax[0].\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecfbf831-282e-419c-8984-429c4552570b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# create some data\n",
      "x = np.random.randn(100)\n",
      "y = np.random.randn(100)\n",
      "\n",
      "# create dataframe from x and y\n",
      "x_flat = np.linspace(0,20,1000)\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create dataframe from x and y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec866426-89be-463a-a266-77699bf2ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# import random forest regressor from scikit-learn\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "# fit random forest model with 300 estimators on X, y:\n",
      "t = RandomForestRegressor(max_depth=5)\n",
      "X_\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "# import random forest regressor from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# fit random forest model with 300 estimators on X, y:\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=3)[2][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e228b0-b493-4021-a37c-90b25a8460d3",
   "metadata": {},
   "source": [
    "위의 몇 가지 예를 보면 우리 모델이 Python 데이터 과학 스택의 구문 중 일부를 학습한 것 같습니다. 물론 실제 세계에 모델을 배포하기 전에 더 철저하게 평가해야 하겠지요. 그러나 때로는 주어진 사용 사례에 필요한 성능을 달성하기 위해 모델 학습시에 다양한 방법을 도입해야 합니다. 예를 들어, 배치 크기를 동적으로 업데이트하거나 잘못된 예제를 즉시 건너뛰는 조건부 학습 루프를 갖고 싶다면 어떻게 해야 할까요? 한 가지 선택지는 Trainer를 서브 클래스로 만들고 필요한 변경 사항을 추가하는 것이지만 때로는 아예 처음부터 학습 루프를 작성하는 것이 더 간단합니다. 여기서 🤗Accelerate가 등장합니다.\n",
    "\n",
    "### 🤗Accelerate를 이용한 학습\n",
    "우리는 지금까지 Trainer로 모델을 학습하는 방법을 보았습니다. 이 방법에서도 몇가지 사용자 지정 설정 변경을 할 수 있었습니다. 그러나 때때로 우리는 학습 루프에 대한 완전한 제어를 원하거나 약간의 예외적인 변경을 원합니다. 이 경우 🤗Accelerate는 훌륭한 선택이며 이 섹션에서는 이를 사용하여 모델을 학습시키는 단계를 살펴보겠습니다. 작업을 더 흥미롭게 만들기 위해 우리는 또한 학습 루프에 약간의 변형을 주겠습니다.\n",
    "\n",
    "우리는 주로 데이터 과학 라이브러리에 대한 합리적인 자동 완성에 관심이 있기 때문에 이러한 라이브러리를 더 많이 사용하는 학습 샘플에 더 많은 가중치를 부여하는 것이 합리적입니다. matplotlib.pyplot, pandas, sklearn의 가장 빈번한 import 이름인 plt, pd, sk 및 fit, predict와 같은 키워드와 fit/predict 패턴을 사용하여 이러한 예를 쉽게 식별할 수 있습니다. 이것들이 각각 하나의 토큰으로 표현된다면 입력 시퀀스에서 발생하는지 쉽게 확인할 수 있습니다. 토큰에는 공백 접두어가 있을 수 있으므로 토크나이저 어휘에서도 해당 버전을 확인할 것입니다. 이 방법이 작동하는지 확인하기 위해 여러 토큰으로 분할되어야 하는 하나의 테스트 토큰을 추가합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da0f8f75-597e-4431-ba5c-1ca38ad0e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8436]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer([\"plt\"]).input_ids[0]\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b84d8fb-209d-4bd8-b46a-46c9bcaf3275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8436]\n",
      "[4289]\n",
      "[1201]\n",
      "[2770]\n",
      "[5431]\n",
      "[2564]\n",
      "[2604]\n",
      "[2110]\n",
      "[2872]\n",
      "[4969]\n",
      "[1824, 1824]\n",
      "Keyword has not single token: testtest\n",
      "[[8436], [4289], [1201], [2770], [5431], [2564], [2604], [2110], [2872], [4969]]\n"
     ]
    }
   ],
   "source": [
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"plt\",\n",
    "    \"pd\",\n",
    "    \"sk\",\n",
    "    \"fit\",\n",
    "    \"predict\",\n",
    "    \" plt\",\n",
    "    \" pd\",\n",
    "    \" sk\",\n",
    "    \" fit\",\n",
    "    \" predict\",\n",
    "    \"testtest\",\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    print(ids)\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids)\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")\n",
    "\n",
    "print(keytoken_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "832275d8-8bb2-4129-8e6a-0840b1b6f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8436]\n",
      "[4289]\n",
      "[1201]\n",
      "[2770]\n",
      "[5431]\n",
      "[2564]\n",
      "[2604]\n",
      "[2110]\n",
      "[2872]\n",
      "[4969]\n",
      "[1824, 1824]\n",
      "Keyword has not single token: testtest\n",
      "[[8436], [4289], [1201], [2770], [5431], [2564], [2604], [2110], [2872], [4969]]\n"
     ]
    }
   ],
   "source": [
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"plt\",\n",
    "    \"pd\",\n",
    "    \"sk\",\n",
    "    \"fit\",\n",
    "    \"predict\",\n",
    "    \" plt\",\n",
    "    \" pd\",\n",
    "    \" sk\",\n",
    "    \" fit\",\n",
    "    \" predict\",\n",
    "    \"testtest\",\n",
    "]:\n",
    "    ids = tokenizer(keyword)[\"input_ids\"]\n",
    "    print(ids)\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids)\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")\n",
    "\n",
    "print(keytoken_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2501743-ba2d-415d-8ab5-abb1a0e3030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8436], [4289], [1201], [2770], [5431], [2564], [2604], [2110], [2872], [4969]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keywords = [\"plt\",\"pd\",\"sk\",\"fit\",\"predict\",\" plt\",\" pd\",\" sk\",\" fit\",\" predict\"]\n",
    "keytoken_ids = [tokenizer(k).input_ids for k in keywords]\n",
    "print(keytoken_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc095b8b-89db-4f17-b263-6602d9204b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8436, 4289, 1201, 2770, 5431, 2564, 2604, 2110, 2872, 4969]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "keywords = [\"plt\",\"pd\",\"sk\",\"fit\",\"predict\",\" plt\",\" pd\",\" sk\",\" fit\",\" predict\"]\n",
    "keytoken_ids = [tokenizer(k).input_ids[0] for k in keywords]\n",
    "keytoken_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f9c0b-e656-4347-b490-2c94481e4224",
   "metadata": {},
   "source": [
    "이제 입력 시퀀스, 로짓 및 방금 입력으로 선택한 키 토큰을 사용하는 사용자 지정 손실 함수를 작성할 수 있습니다. 먼저 로짓과 입력을 정렬해야 합니다. **다음 토큰이 현재 토큰의 레이블이기 때문에 오른쪽으로 1만큼 이동한 입력 시퀀스가 레이블**을 형성합니다. 모델은 어쨌든 첫 번째 토큰에 대한 예측을 하지 않기 때문에 입력 시퀀스의 두 번째 토큰에서 레이블을 시작하여 이를 달성할 수 있습니다. 그런 다음 전체 입력 시퀀스를 따르는 토큰에 대한 레이블이 없기 때문에 **마지막 로짓을 잘라**냅니다. 이를 통해 샘플당 손실을 계산하고 각 샘플에서 모든 키워드의 발생을 계산할 수 있습니다. 마지막으로 발생 횟수를 가중치로 사용하여 모든 샘플에 대한 가중 평균을 계산합니다. 키워드가 없는 샘플을 모두 버리고 싶지 않기 때문에 **가중치에 1을 더합니다 (weights = alpha * (1.0 + weights))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d7fb370-7c6a-49e6-806b-cd2bf24fb5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  3,  4],\n",
      "        [ 6,  1,  8],\n",
      "        [10, 11, 12]])\n",
      "tensor([[ 2,  3,  4],\n",
      "        [ 6,  1,  8],\n",
      "        [10, 11, 12]])\n",
      "tensor([ 9, 10, 11, 12])\n"
     ]
    }
   ],
   "source": [
    "inputss = torch.tensor([[1, 2, 3, 4], [5, 6, 1, 8], [9,10,11,12]] )\n",
    "print(inputss[..., 1:])\n",
    "shift_labels = inputss[..., 1:].contiguous()\n",
    "print(shift_labels)\n",
    "shift_labels = inputss[..., -1, :].contiguous()\n",
    "print(shift_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1eff3de1-80e0-4c91-82ef-55ab1443a708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([8., 8., 8.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2, 3, 4)\n",
    "print(a)\n",
    "a.sum(axis=[0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30dbec4e-f01a-42fb-8a31-8028e6e1daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "[tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0.]]), tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]]), tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]])]\n",
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.]]])\n",
      "tensor([2., 1., 1.])\n",
      "tensor([3., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "key = [1, 2, 12]\n",
    "#[i for i in key]\n",
    "print(inputss.shape)\n",
    "print([(inputss == i).float() for i in key]) # list of 3 tensors of size([3,4])\n",
    "print(torch.stack([(inputss == i).float() for i in key])) # shape = (3, 3, 4)\n",
    "weight = torch.stack([(inputss == i).float() for i in key]).sum(axis=[0, 2])\n",
    "print(weight)\n",
    "alpha = 1.0\n",
    "weight = alpha * (1.0 + weight)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3673b00b-57af-4423-ac5d-2c8981a696a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0.]]), tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]]), tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]])]\n"
     ]
    }
   ],
   "source": [
    "print([(inputss == i).float() for i in key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7db12f5f-6564-422e-a554-298ea418ba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4]) torch.Size([1])\n",
      "tensor([[ True, False, False, False],\n",
      "        [False, False,  True, False],\n",
      "        [False, False, False, False]])\n",
      "torch.Size([3, 4]) torch.Size([1])\n",
      "tensor([[False,  True, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "torch.Size([3, 4]) torch.Size([1])\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False,  True]])\n",
      "[tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0.]]), tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]]), tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]])]\n"
     ]
    }
   ],
   "source": [
    "key2 = torch.tensor([[1], [2], [12]])\n",
    "for i in key2:\n",
    "    print(inputss.shape, i.shape)\n",
    "    print(inputss == i)\n",
    "print([(inputss == i).float() for i in key2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "577bbacd-4dd5-4529-b44a-52d78282a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    #assuming batch_size is equal to 4\n",
    "    #print(inputs.shape, logits.shape) \n",
    "    #torch.Size([4, 128]) torch.Size([4, 128, 50000])\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous() # first token removed\n",
    "    #shift_labels = logits[..., :-1, :].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous() # last logit removed\n",
    "    # 토큰당 손실값 계산\n",
    "    #print(shift_labels.shape, shift_logits.shape) \n",
    "    #torch.Size([4, 127]) torch.Size([4, 127, 50000])\n",
    "    \n",
    "    loss_fct = CrossEntropyLoss(reduce=False)\n",
    "    #print(shift_logits.view(-1, shift_logits.size(-1)).shape, shift_labels.view(-1).shape)\n",
    "    # torch.Size([508, 50000]) torch.Size([508])\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    #print(loss.shape) # torch.Size([508]) \n",
    "    #print(shift_logits.size(0),shift_logits.size(1), shift_logits.size(-1) ) # 4 127 50000\n",
    "    # 샘플당 손실값을 resize하고 평균화\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    #print(loss_per_sample.shape) #torch.Size([4])\n",
    "    # Calculate and scale weighting\n",
    "    \n",
    "    #print(torch.stack([(inputs == kt).float() for kt in keytoken_ids]).shape) #torch.Size([10, 4, 128])\n",
    "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(axis=[0, 2])\n",
    "    # [(inputs == kt).float() for kt in keytoken_ids] is frequences of keytoken_ids in inputs of [4,128]\n",
    "    #print(weights.shape)  #torch.Size([4]) #weight per each samples in batch\n",
    "    #print(weights) # frequeces of keytokens in each samples in the batch \n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss # torch.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2dd81f50-8e22-4aad-b0bd-981182d79de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_ids = inputs.get(\"input_ids\")\n",
    "        #print(input_ids.shape)\n",
    "        #print(keytoken_ids)\n",
    "        outputs = model(input_ids)\n",
    "        #print([(input_ids == kt).float() for kt in keytoken_ids])\n",
    "        loss = keytoken_weighted_loss(input_ids, outputs.logits, keytoken_ids)\n",
    "        return (loss, outputs) if return_outputs else loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7b800652-3007-479d-8542-ec638b6b58f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "mytrainer = MyTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n",
    "\n",
    "#mytrainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "637cf6e6-d8cb-497a-ad72-84477613ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mytrainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c963ee2e-be31-45c2-96c5-8ebec9a8713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], \n",
    "                              batch_size=batch_size,\n",
    "                              #batch_size=4,\n",
    "                              shuffle=True,\n",
    "                             )\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"valid\"], \n",
    "                             batch_size=batch_size,\n",
    "                             #batch_size=4,\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19f14366-7b30-4612-8cf0-57abb7d79e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68868\n",
      "399\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e0e5ccc-af1d-40e7-8281-54878e847925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for n, p in model.named_parameters():\n",
    "#    print(n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb8b5fa2-a2d3-411e-8c4e-6f323eaf3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        #{\"params\": len(params_with_wd), \"weight_decay\": weight_decay},\n",
    "        #{\"params\": len(params_without_wd), \"weight_decay\": 0.0},\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "#get_grouped_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "370645b9-6bce-4739-92bd-5e9145d4646c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_grouped_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8e29c117-b32b-4d09-baf0-45d887b66cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight']\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "np_list = []\n",
    "for n, p in model.named_parameters():\n",
    "    np_list.append(n)\n",
    "\n",
    "print(len(np_list))\n",
    "print(np_list[:5])\n",
    "\n",
    "param_list = []\n",
    "for param in model.parameters():\n",
    "    param_list.append(param)\n",
    "\n",
    "print(len(param_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e4dc6afc-db12-4a85-88f4-2ebca5783ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Size: 118.49Mib\n",
      "Parameter Size: 118.49Mib\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parameter Size: {sum([p.numel() for p in model.parameters()])/1024**2:.2f}Mib\")\n",
    "print(f\"Parameter Size: {sum([p.numel() for n,p in model.named_parameters()])/1024**2:.2f}Mib\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "50d2bbbf-0939-4f0f-afa3-394d9e6f2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d9867bc0-f9bc-4e5f-a2b1-64262534f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "65e818d1-454c-4197-b4d7-439c8ffe33da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "#accelerator = Accelerator(fp16=True)\n",
    "accelerator = Accelerator(mixed_precision = 'fp16')\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e05d829d-b15e-4243-9f93-135ef69953fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.972250938415527, 58235.53125)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# original version ERROR \n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather(outputs.loss)) \n",
    "    print(len(losses)) #399 \n",
    "    #loss = torch.mean(torch.cat(losses)) # ERROR\n",
    "    loss = torch.mean(torch.tensor(losses))\n",
    "    #print(loss)\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2a5e49c8-da55-4ec7-be86-5070ff8ed835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_247149/1157565489.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  losses.append(accelerator.gather(torch.tensor(outputs.loss)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.972250938415527, 58235.53125)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        #print(outputs.loss)\n",
    "        #losses.append(accelerator.gather(outputs.loss))\n",
    "        losses.append(accelerator.gather(torch.tensor(outputs.loss)))\n",
    "        #print(len(losses))\n",
    "    #print(losses)\n",
    "    #loss = torch.mean(torch.cat(losses))\n",
    "    loss = torch.mean(torch.tensor(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3214b426-a422-47c1-ad75-70884e4c6839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 2203760\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 12763\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0cd430b9-6a4b-45ee-a278-98468a882e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.972254753112793, 58235.75390625)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        #print(batch[\"input_ids\"].shape) #torch.Size([32, 128])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        loss = outputs.loss\n",
    "        batch_size = len(batch[\"input_ids\"])\n",
    "        #print(batch_size)\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "        \n",
    "        #losses.append(accelerator.gather(outputs.loss.item()))\n",
    "        #losses.append(accelerator.gather(torch.tensor(outputs.loss)))\n",
    "        #print(outputs.loss.shape)\n",
    "    #print(len(losses)) #list of batched averaged losses list #399\n",
    "    #print([len(i) for i in losses]) # [32, 32, 32,....,27] , 32 * 398 + 27 = 12763\n",
    "    losses = torch.cat(losses) \n",
    "    loss = torch.mean(losses)\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()\n",
    "\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9ceb36c0-cbc0-4003-b799-c7b6cb5963fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids'])\n",
      "torch.Size([32, 128, 50000])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "print(batch.keys()) #dict_keys(['input_ids'])\n",
    "\n",
    "inputs = batch[\"input_ids\"]\n",
    "outputs = model(inputs) \n",
    "print(outputs.logits.shape) #torch.Size([4, 128, 50000])\n",
    "#print(outputs.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0d72277d-559c-46c1-a6c6-c749dc547785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128]) torch.Size([32, 128, 50000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.8073, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "print(inputs.shape, logits.shape)\n",
    "loss = keytoken_weighted_loss(inputs, logits, keytoken_ids)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f6221011-aa2e-486b-bf43-adbed211feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c21135f-effb-432d-aade-f0be0a88d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "#model_name = \"codeparrot-ds-accelerate\"\n",
    "#repo_name = get_full_repo_name(model_name)\n",
    "#repo_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "21eef93b-5a2a-40e3-bd79-d0db9529eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"codeparrot-ds-accelerate\"\n",
    "#repo = Repository(output_dir, clone_from=repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "414b0da0-d9c5-48d2-a743-2c3eaa6907e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.972254753112793, 58235.75390625)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "94a23878-bc72-44d7-9ab7-09b4ffe1dcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68868\n"
     ]
    }
   ],
   "source": [
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84e39095-6194-41c2-9a55-0238fe90b73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd147e216c804232976bbd3604c972c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 6e-06, 'samples': 3200, 'steps': 12, 'loss/train': 14.614526748657227}\n",
      "{'lr': 1.2e-05, 'samples': 6400, 'steps': 24, 'loss/train': 11.365421295166016}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m samples_per_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train_epochs):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_dataloader, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), total\u001b[38;5;241m=\u001b[39mnum_training_steps):\n\u001b[1;32m     14\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     15\u001b[0m         loss \u001b[38;5;241m=\u001b[39m keytoken_weighted_loss(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], logits, keytoken_ids)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/data_loader.py:462\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 462\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:2814\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2814\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2815\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:2810\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/arrow_dataset.py:2795\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2794\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2795\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2797\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/formatting/formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/formatting/formatting.py:400\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/formatting/torch_formatter.py:106\u001b[0m, in \u001b[0;36mTorchFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping:\n\u001b[0;32m--> 106\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m    108\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(batch)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/formatting/formatting.py:164\u001b[0m, in \u001b[0;36mNumpyArrowExtractor.extract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {col: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arrow_array_to_numpy(pa_table[col]) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names}\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/formatting/formatting.py:164\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {col: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arrow_array_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names}\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/formatting/formatting.py:196\u001b[0m, in \u001b[0;36mNumpyArrowExtractor._arrow_array_to_numpy\u001b[0;34m(self, pa_array)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    191\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m (x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m array[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(x))\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m array\n\u001b[1;32m    194\u001b[0m     ):\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "#eval_steps = 5000\n",
    "eval_steps = 500\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "samples_per_step = batch_size * gradient_accumulation_steps\n",
    "samples_per_step = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(enumerate(train_dataloader, start=1), total=num_training_steps):\n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"lr\": lr_scheduler.get_lr()[0],\n",
    "                    \"samples\": step * batch_size,\n",
    "                    #\"samples\": samples_per_step,\n",
    "                    \"steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item(),\n",
    "                    #\"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        samples_per_step += len(batch[\"input_ids\"])\n",
    "        #print(samples_per_step)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1 \n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                #repo.push_to_hub(\n",
    "                #    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                #)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ddfeb-1b8a-49f3-9f52-3b4d5136693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install threadpoolctl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a89e00db-7edf-4ffc-89d0-d81a1b48beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='203' max='8608' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 203/8608 09:14 < 6:26:28, 0.36 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>8.552600</td>\n",
       "      <td>6.588633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.791800</td>\n",
       "      <td>5.074188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 85\u001b[0m\n\u001b[1;32m     56\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     57\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodeparrot-ds\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m#push_to_hub=True,\u001b[39;00m\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     76\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     77\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     78\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     83\u001b[0m )\n\u001b[0;32m---> 85\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/trainer.py:2085\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2086\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/data_loader.py:462\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 462\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/data/data_collator.py:761\u001b[0m, in \u001b[0;36mDataCollatorForLanguageModeling.torch_call\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtorch_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, examples: List[Union[List[\u001b[38;5;28mint\u001b[39m], Any, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;66;03m# Handle dict or lists with proper padding and conversion to tensor.\u001b[39;00m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[0;32m--> 761\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m         batch \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    766\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_to_multiple_of)\n\u001b[1;32m    767\u001b[0m         }\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3369\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3366\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3367\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:224\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    220\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:759\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    756\u001b[0m     value \u001b[38;5;241m=\u001b[39m [value]\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 759\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m tensor\n",
      "File \u001b[0;32m/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:721\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train.shuffle(seed=12).select(range(80000)),\n",
    "        \"valid\": ds_valid.shuffle(seed=12).select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element['content'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch =  [\n",
    "        input_ids for length, input_ids in zip(outputs['length'], outputs['input_ids']) if length == context_length\n",
    "    ]\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    #eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    #logging_steps=500,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5000,\n",
    "    fp16=True,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#trainer.push_to_hub(commit_message=\"Training complete\", tags=\"text-generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddc092-7635-441c-8f27-f2a016f05e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf80578f36d4aff8dd4846d6b008943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 4.9500000000000004e-05, 'samples': 25600, 'steps': 99, 'loss/train': 9.978860855102539}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train.shuffle(seed=12).select(range(80000)),\n",
    "        \"valid\": ds_valid.shuffle(seed=12).select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['input_ids'],\n",
    "#        num_rows: 2203760\n",
    "#    })\n",
    "#    valid: Dataset({\n",
    "#        features: ['input_ids'],\n",
    "#        num_rows: 12763\n",
    "#    })\n",
    "#})\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element['content'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch =  [\n",
    "        input_ids for length, input_ids in zip(outputs['length'], outputs['input_ids']) if length == context_length\n",
    "    ]\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "keywords = [\"plt\",\"pd\",\"sk\",\"fit\",\"predict\",\" plt\",\" pd\",\" sk\",\" fit\",\" predict\"]\n",
    "keytoken_ids = [tokenizer(k).input_ids[0] for k in keywords]\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    shift_labels = inputs[..., 1:].contiguous() # first token removed\n",
    "    #shift_labels = logits[..., :-1, :].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous() # last logit removed\n",
    "    # 토큰당 손실값 계산\n",
    "    loss_fct = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # 샘플당 손실값을 resize하고 평균화\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(axis=[0, 2])\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], \n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                    )\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"valid\"], \n",
    "                             batch_size=batch_size,\n",
    "                    )\n",
    "\n",
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        #print(batch[\"input_ids\"].shape) #torch.Size([32, 128])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        loss = outputs.loss\n",
    "        batch_size = len(batch)\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "        #losses.append(accelerator.gather(outputs.loss.item()))\n",
    "        #print(outputs.loss.shape)\n",
    "    #print(losses)\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    #loss = torch.mean(torch.tensor(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "#from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "#model_name = \"codeparrot-ds-accelerate\"\n",
    "#repo_name = get_full_repo_name(model_name)\n",
    "#repo_name\n",
    "\n",
    "output_dir = \"codeparrot-ds-accelerate\"\n",
    "#repo = Repository(output_dir, clone_from=repo_name)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 500\n",
    "#eval_steps = 10\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "samples_per_step = batch_size * gradient_accumulation_steps\n",
    "samples_per_step = batch_size \n",
    "\n",
    "num_training_steps = num_train_epochs * len(train_dataloader)  // gradient_accumulation_steps\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    #for step, batch in tqdm(enumerate(train_dataloader, start=1), total=num_training_steps):\n",
    "    for step, batch in enumerate(train_dataloader, start=1):    \n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        if step % 800 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"lr\": lr_scheduler.get_lr()[0],\n",
    "                    \"samples\": step * samples_per_step,\n",
    "                    \"steps\": completed_steps,\n",
    "                    #\"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                    \"loss/train\": loss.item(),\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        #samples_per_step += len(batch)\n",
    "        if step % gradient_accumulation_steps == 0:    \n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "            progress_bar.update(1)\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                #repo.push_to_hub(\n",
    "                #    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                #)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700881b4-60ab-49b3-b199-dd25bfb46f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af4136e87844f76a3abc341b4463dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 4.9500000000000004e-05, 'samples': 25600, 'steps': 99, 'loss/train': 9.382041931152344}\n",
      "{'loss/eval': 6.661674499511719, 'perplexity': 781.8590698242188}\n",
      "[2024-05-17 08:10:35,664] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "{'lr': 9.95e-05, 'samples': 51200, 'steps': 199, 'loss/train': 6.215837478637695}\n",
      "{'loss/eval': 5.175014019012451, 'perplexity': 176.79910278320312}\n",
      "{'lr': 0.0001495, 'samples': 76800, 'steps': 299, 'loss/train': 6.40376091003418}\n",
      "{'loss/eval': 4.529634952545166, 'perplexity': 92.7247085571289}\n",
      "{'lr': 0.00019950000000000002, 'samples': 102400, 'steps': 399, 'loss/train': 6.088466644287109}\n",
      "{'loss/eval': 4.12180757522583, 'perplexity': 61.670616149902344}\n",
      "{'lr': 0.0002495, 'samples': 128000, 'steps': 499, 'loss/train': 6.685642242431641}\n",
      "{'loss/eval': 3.8094990253448486, 'perplexity': 45.12782669067383}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train.shuffle(seed=12).select(range(80000)),\n",
    "        \"valid\": ds_valid.shuffle(seed=12).select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['input_ids'],\n",
    "#        num_rows: 2203760\n",
    "#    })\n",
    "#    valid: Dataset({\n",
    "#        features: ['input_ids'],\n",
    "#        num_rows: 12763\n",
    "#    })\n",
    "#})\n",
    "\n",
    "def get_dataloaders(accelerator: Accelerator, batch_size: int = 32):\n",
    "    \n",
    "    context_length = 128\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "    def tokenize(element):\n",
    "        outputs = tokenizer(\n",
    "            element['content'],\n",
    "            truncation=True,\n",
    "            max_length=context_length,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_length=True,\n",
    "        )\n",
    "        input_batch =  [\n",
    "            input_ids for length, input_ids in zip(outputs['length'], outputs['input_ids']) if length == context_length\n",
    "        ]\n",
    "        return {\"input_ids\": input_batch}\n",
    "\n",
    "    # Apply the method we just defined to all the examples in all the splits of the dataset\n",
    "    # starting with the main process first:\n",
    "    with accelerator.main_process_first():\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    train_dataloader = DataLoader(tokenized_datasets[\"train\"], \n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                        )\n",
    "    eval_dataloader = DataLoader(tokenized_datasets[\"valid\"], \n",
    "                             batch_size=batch_size,\n",
    "                        )\n",
    "\n",
    "    return train_dataloader, eval_dataloader, tokenizer\n",
    "\n",
    "def training_function():\n",
    "\n",
    "    batch_size = 32\n",
    "    \n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")    \n",
    "    #accelerator = Accelerator() \n",
    "    train_dataloader, eval_dataloader, tokenizer = get_dataloaders(accelerator, batch_size=batch_size)\n",
    "\n",
    "    keywords = [\"plt\",\"pd\",\"sk\",\"fit\",\"predict\",\" plt\",\" pd\",\" sk\",\" fit\",\" predict\"]\n",
    "    keytoken_ids = [tokenizer(k).input_ids[0] for k in keywords]\n",
    "\n",
    "    def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "        shift_labels = inputs[..., 1:].contiguous() # first token removed\n",
    "        #shift_labels = logits[..., :-1, :].contiguous()\n",
    "        shift_logits = logits[..., :-1, :].contiguous() # last logit removed\n",
    "        # 토큰당 손실값 계산\n",
    "        #loss_fct = CrossEntropyLoss(reduce=False)\n",
    "        loss_fct = CrossEntropyLoss(reduction='none')\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        # 샘플당 손실값을 resize하고 평균화\n",
    "        loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "        # Calculate and scale weighting\n",
    "        weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(axis=[0, 2])\n",
    "        weights = alpha * (1.0 + weights)\n",
    "        # Calculate weighted average\n",
    "        weighted_loss = (loss_per_sample * weights).mean()\n",
    "        return weighted_loss\n",
    "\n",
    "    weight_decay = 0.1\n",
    "\n",
    "    def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "        params_with_wd, params_without_wd = [], []\n",
    "        for n, p in model.named_parameters():\n",
    "            if any(nd in n for nd in no_decay):\n",
    "                params_without_wd.append(p)\n",
    "            else:\n",
    "                params_with_wd.append(p)\n",
    "        return [\n",
    "            {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "            {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "\n",
    "    def evaluate():\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            #print(batch[\"input_ids\"].shape) #torch.Size([32, 128])\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "            loss = outputs.loss\n",
    "            batch_size = len(batch[\"input_ids\"])\n",
    "            losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "            #losses.append(accelerator.gather(outputs.loss.item()))\n",
    "            #print(outputs.loss.shape)\n",
    "            #print(len(losses)) #1,2,3,....100 ==> 12763 / (32 * 4) = 99.7\n",
    "        #print(losses)\n",
    "        loss = torch.mean(torch.cat(losses))\n",
    "        #loss = torch.mean(torch.tensor(losses))\n",
    "        try:\n",
    "            perplexity = torch.exp(loss)\n",
    "        except OverflowError:\n",
    "            perplexity = float(\"inf\")\n",
    "        return loss.item(), perplexity.item()\n",
    "\n",
    "    context_length = 128\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        vocab_size=len(tokenizer),\n",
    "        n_ctx=context_length,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    model = GPT2LMHeadModel(config)\n",
    "\n",
    "    optimizer = AdamW(get_grouped_params(model), lr=5e-4)\n",
    "\n",
    "    \n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "    \n",
    "    gradient_accumulation_steps = 8\n",
    "    \n",
    "    num_train_epochs = 1\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    #num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch // gradient_accumulation_steps\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=1000,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    #from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "    #model_name = \"codeparrot-ds-accelerate\"\n",
    "    #repo_name = get_full_repo_name(model_name)\n",
    "    #repo_name\n",
    "\n",
    "    output_dir = \"codeparrot-ds-accelerate\"\n",
    "    #repo = Repository(output_dir, clone_from=repo_name)\n",
    "\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    gradient_accumulation_steps = 8\n",
    "    #eval_steps = 5000\n",
    "    eval_steps = 100\n",
    "\n",
    "    model.train()\n",
    "    completed_steps = 0\n",
    "    #samples_per_step = batch_size * gradient_accumulation_steps\n",
    "    samples_per_step = batch_size\n",
    "\n",
    "    num_training_steps = num_train_epochs * len(train_dataloader)  // gradient_accumulation_steps\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    #print(f\"length of train_dataloader: {len(train_dataloader)}\") # 17217\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        #for step, batch in tqdm(enumerate(train_dataloader, start=1), total=num_training_steps):\n",
    "        for step, batch in enumerate(train_dataloader, start=1):    \n",
    "            logits = model(batch[\"input_ids\"]).logits\n",
    "            loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "            if step % 800 == 0: #eval_step * gradient_accumulation_steps\n",
    "                accelerator.print(\n",
    "                    {\n",
    "                        \"lr\": lr_scheduler.get_lr()[0],\n",
    "                        \"samples\": step * samples_per_step,\n",
    "                        \"steps\": completed_steps,\n",
    "                        #\"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                        \"loss/train\": loss.item(),\n",
    "                    }\n",
    "                )\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            #samples_per_step += len(batch)\n",
    "            if step % gradient_accumulation_steps == 0:    \n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                completed_steps += 1\n",
    "                progress_bar.update(1)\n",
    "            if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "                eval_loss, perplexity = evaluate()\n",
    "                accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "                model.train()\n",
    "                accelerator.wait_for_everyone()\n",
    "                unwrapped_model = accelerator.unwrap_model(model)\n",
    "                unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "                if accelerator.is_main_process:\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "                    #repo.push_to_hub(\n",
    "                    #    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                    #)\n",
    "\n",
    "#from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "#model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "#repo_name = get_full_repo_name(model_name)\n",
    "#repo_name\n",
    "#repo = Repository(output_dir, clone_from=repo_name)\n",
    "#notebook_launcher(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=1)\n",
    " \n",
    "#def main():\n",
    "#    parser = argparse.ArgumentParser(description=\"Simple example of training script.\")\n",
    "#    parser.add_argument(\n",
    "#        \"--mixed_precision\",\n",
    "#        type=str,\n",
    "#        default=None,\n",
    "#        choices=[\"no\", \"fp16\", \"bf16\", \"fp8\"],\n",
    "#        help=\"Whether to use mixed precision. Choose\"\n",
    "#        \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n",
    "#        \"and an Nvidia Ampere GPU.\",\n",
    "#    )\n",
    "#    parser.add_argument(\"--cpu\", action=\"store_true\", help=\"If passed, will train on the CPU.\")\n",
    "#    args = parser.parse_args()\n",
    "#    config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 16}\n",
    "#    training_function(config, args)\n",
    "#\n",
    "#\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8af3d-d561-4f8b-bd1b-f26b62180508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
