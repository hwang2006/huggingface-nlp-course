{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d336f78b-d3ae-4aeb-82a2-3b44e24281c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 210173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c22a7f-6a8c-4c59-848a-a2ad8e9b1ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available kernels:\n",
      "  python3        /scratch/qualis/miniconda3/envs/transformer/share/jupyter/kernels/python3\n",
      "  alpaca         /home01/qualis/.local/share/jupyter/kernels/alpaca\n",
      "  genai          /home01/qualis/.local/share/jupyter/kernels/genai\n",
      "  transformer    /home01/qualis/.local/share/jupyter/kernels/transformer\n"
     ]
    }
   ],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66744fe0-91f2-4a1c-afeb-c70628f0de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_datasets[\"train\"].select(range(1000)).train_test_split(train_size=0.9, seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcddb51-2aeb-44b9-8fe0-188ced3b443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56aaac06-44e0-4ba8-9175-20bf7760dca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9eec2a-0628-4a70-b66f-36002256b7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c713c344-d79b-475f-8e0e-9e38179e7629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Default to expanded threads',\n",
       " 'fr': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b430d5-f6c7-4890-8d08-c005eb38ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c60a9fd-5c13-47e3-bb2c-250da953ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2119fabc-c633-4274-a2b0-5538fe27be25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.37.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#!pip show transformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2799f70f-168f-4470-94f7-0336a9f66296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "103d6ddc-1ac3-43be-a1eb-0aaab2abcdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Par dÃ©faut pour les threads Ã©largis'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ee77b3-9359-4215-a3e0-f165b30b52c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',\n",
       " 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][172][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79f25e40-4465-4edc-b030-9bb906398ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\n",
    "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47f34c35-b51f-475e-91d5-77264cda0582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': \"Otherwise, forward us the backtraces when the email pops-up (you'll see). If you get a crash often you will get it fixed for sure if you forward a really good backtrace to us. See the next sections for assistance there.\", 'fr': 'Sinon, envoyez -nous les logs quand le courrier Ã©lectronique apparaÃ®t (vous verrez). Si & amarok; plante souvent, le bogue sera corrigÃ© si vous nous envoyez un bon log. Voir la section suivante pour savoir comment obtenir un bon log.'}\n",
      "\n",
      "[{'translation_text': \"Sinon, faites-nous suivre les backtraces lorsque l'email s'affiche (vous verrez). Si vous obtenez souvent un crash, vous l'obtiendrez corrigÃ© pour Ãªtre sÃ»r que vous nous transmettez une trÃ¨s bonne backtrace. Consultez les prochaines sections pour de l'aide lÃ -bas.\"}]\n"
     ]
    }
   ],
   "source": [
    "for text in raw_datasets[\"train\"][\"translation\"]:\n",
    "    if \" email\" in text[\"en\"]:\n",
    "        print(f\"{text}\\n\")\n",
    "        print(translator(text[\"en\"]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69390d16-1030-4510-89b0-66ada18dc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c16ccf71-c389-456a-8fe5-600048b8df64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence)\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(fr_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b73359c7-ad54-4213-8e71-c27718acae52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Default', 'â–to', 'â–expanded', 'â–thread', 's', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcccefab-fd6d-498f-a111-64b9b76c95c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Par', 'â–dÃ©faut', ',', 'â–dÃ©velopper', 'â–les', 'â–fils', 'â–de', 'â–discussion', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "187e934f-8105-409f-bf70-826d1f72cd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Par', 'â–dÃ©', 'f', 'aut', ',', 'â–dÃ©', 've', 'lop', 'per', 'â–les', 'â–fil', 's', 'â–de', 'â–discussion', '</s>']\n",
      "['â–Par', 'â–dÃ©faut', ',', 'â–dÃ©velopper', 'â–les', 'â–fils', 'â–de', 'â–discussion', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(fr_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d631cb26-130b-4c6c-a84a-901897203c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a2d05e1-6abe-4abf-a629-fd01ae415624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c754143-ac5e-495b-a3e4-75647ad0c493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=fr_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cf64243-2e86-4824-b2bd-16ab20193cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Par', 'â–dÃ©', 'f', 'aut', ',', 'â–dÃ©', 've', 'lop', 'per', 'â–les', 'â–fil', 's', 'â–de', 'â–discussion', '</s>']\n",
      "['â–Par', 'â–dÃ©faut', ',', 'â–dÃ©velopper', 'â–les', 'â–fils', 'â–de', 'â–discussion', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(fr_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7534e351-52a9-46cd-95f8-6ce56d93925b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    target = tokenizer(fr_sentence)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41606066-262a-46ce-b4f0-4cae1b0a9ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-fr', vocab_size=59514, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59513: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f909f3c-b217-448e-996f-4b570a970e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [47591, 12, 9842, 19634, 9, 0]\n",
      "text: Default to expanded threads</s>\n",
      "tokens: ['â–Default', 'â–to', 'â–expanded', 'â–thread', 's', '</s>']\n"
     ]
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "model_inputs = tokenizer(en_sentence, max_length=max_input_length, truncation=True)\n",
    "print(f\"input_ids: {model_inputs['input_ids']}\")\n",
    "print(f\"text: {tokenizer.decode(model_inputs['input_ids'])}\")\n",
    "print(f\"tokens: {tokenizer.convert_ids_to_tokens(model_inputs['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a699fbc-f314-4205-b9bd-c705fbb5f5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [47591, 12, 9842, 19634, 9, 0]\n",
      "text: Default to expanded threads</s>\n",
      "tokens: ['â–Default', 'â–to', 'â–expanded', 'â–thread', 's', '</s>']\n"
     ]
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "model_inputs = tokenizer(en_sentence, max_length=max_input_length, padding = \"max_length\", truncation=True) # len(input_ids): 128\n",
    "model_inputs = tokenizer(en_sentence, max_length=max_input_length, padding = True, truncation=True) #len(input_ids):6  \n",
    "print(f\"input_ids: {model_inputs['input_ids']}\")\n",
    "print(f\"text: {tokenizer.decode(model_inputs['input_ids'])}\")\n",
    "print(f\"tokens: {tokenizer.convert_ids_to_tokens(model_inputs['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a2690ad-4852-4430-8cce-383456edd584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-fr', vocab_size=59514, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59513: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7976e2b1-b35e-4b80-a0aa-d5ee44dbe8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '152754',\n",
       " 'translation': {'en': 'Default to expanded threads',\n",
       "  'fr': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dec852b-ef74-4174-af28-0a6e12a60fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    #print(f\"examples: {examples['translation']}\")\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # íƒ€ê²Ÿì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ì…‹ì—…\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    #print(f\"labels: {labels}\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "479dde03-0733-42cc-9df6-8f4f2cdc7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "        #inputs, text_target=targets, max_length=max_length, padding=True, truncation=True\n",
    "    )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4daa0734-d483-436a-b571-17ab17783149",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07f9f07c-bac3-4432-8689-02de0ec88c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[34378, 226, 5783, 32, 200, 12, 3647, 4, 1223, 1628, 117, 4923, 23608, 3, 1789, 2942, 20059, 301, 548, 301, 331, 30, 117, 4923, 12, 4, 1528, 668, 3, 5734, 212, 9319, 30, 4, 4923, 57, 5487, 30, 4, 6, 32712, 25, 7243, 1160, 12, 621, 42, 4, 1156, 3009, 3, 0], [47591, 12, 9842, 19634, 9, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'labels': [[60, 7418, 5244, 8234, 740, 4993, 8, 6471, 5, 2218, 29, 193, 2220, 742, 3, 4366, 14237, 14, 6, 16600, 301, 548, 301, 331, 5, 193, 24275, 17, 8, 668, 6142, 3, 33640, 36, 81, 6, 5411, 2709, 9376, 22, 24275, 59, 36, 19, 9376, 153, 402, 29033, 13774, 402, 29033, 416, 27, 8, 4034, 4888, 3, 0], [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]]}\n",
      "[52, 6, 15, 3, 12, 7, 128, 59, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][:2])\n",
    "print([len(elem) for elem in tokenized_datasets[\"train\"][:10][\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3eadf6e-2a78-4383-89a6-847867321eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[34378, 226, 5783, 32, 200, 12, 3647, 4, 1223, 1628, 117, 4923, 23608, 3, 1789, 2942, 20059, 301, 548, 301, 331, 30, 117, 4923, 12, 4, 1528, 668, 3, 5734, 212, 9319, 30, 4, 4923, 57, 5487, 30, 4, 6, 32712, 25, 7243, 1160, 12, 621, 42, 4, 1156, 3009, 3, 0], [47591, 12, 9842, 19634, 9, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'labels': [[60, 7418, 5244, 8234, 740, 4993, 8, 6471, 5, 2218, 29, 193, 2220, 742, 3, 4366, 14237, 14, 6, 16600, 301, 548, 301, 331, 5, 193, 24275, 17, 8, 668, 6142, 3, 33640, 36, 81, 6, 5411, 2709, 9376, 22, 24275, 59, 36, 19, 9376, 153, 402, 29033, 13774, 402, 29033, 416, 27, 8, 4034, 4888, 3, 0], [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = preprocess_function(split_datasets[\"train\"][:2])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3fcab25-b267-4829-9995-f07405672bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "897d5ff1-1f37-4346-a6f5-9395dc9e67da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(59514, 512, padding_idx=59513)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=59514, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220aa439-66b0-4d2e-9ffa-ad5b025e0963",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ì½œë ˆì´ì…˜ (Data Collation)\r\n",
    "ë™ì  ë°°ì¹˜ ì²˜ë¦¬(Dynamic batching)ë¥¼ ìœ„í•œ íŒ¨ë”©ì„ ì²˜ë¦¬í•˜ë ¤ë©´ ë°ì´í„° ì½œë ˆì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ ê²½ìš° 3ì¥ì—ì„œì™€ ê°™ì´ DataCollatorWithPaddingì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ ì´ ë©”ì„œë“œëŠ” ì…ë ¥(input IDs, attention mask ë° token type IDs)ì— ëŒ€í•´ì„œë§Œ íŒ¨ë”©ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë ˆì´ë¸” ì—­ì‹œ ë ˆì´ë¸”ì— ìˆëŠ” ìµœëŒ€ ê¸¸ì´ë¡œ ì±„ì›Œì ¸ì•¼ í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ë ˆì´ë¸”ì„ ì±„ìš°ëŠ”ë° ì‚¬ìš©ë˜ëŠ” íŒ¨ë”© ê°’ì€ í† í¬ë‚˜ì´ì €ì˜ íŒ¨ë”© í† í°ì´ ì•„ë‹ˆë¼ -100ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë˜ì•¼ íŒ¨ë”©ëœ ê°’ì´ ì†ì‹¤ ê³„ì‚°ì—ì„œ ë¬´ì‹œë©ë‹ˆë‹¤.\r\n",
    "\r\n",
    "ì´ ì‘ì—…ì€ ëª¨ë‘ DataCollatorForSeq2Seqì— ì˜í•´ ìˆ˜í–‰ë©ë‹ˆë‹¤. DataCollatorWithPaddingê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, DataCollatorForSeq2SeqëŠ” ì…ë ¥ì„ ì „ì²˜ë¦¬í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” tokenizerëŠ” ë¬¼ë¡  ëª¨ë¸ ìì²´ë„ ë§¤ê°œë³€ìˆ˜ë¡œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤. ëª¨ë¸ë„ ì…ë ¥ë°›ëŠ” ì´ìœ ëŠ” ì´ ë°ì´í„° ì½œë ˆì´í„°ê°€ ì‹œì‘ ë¶€ë¶„ì— íŠ¹ìˆ˜ í† í°ì´ ë¶™ì–´ ìˆëŠ”, ë ˆì´ë¸” ì‹œí€€ìŠ¤ë¥¼ ìš°ì¸¡ìœ¼ë¡œ ì‹œí”„íŠ¸(shift)í•œ ë²„ì „ì¸ ë””ì½”ë” input IDsë¥¼ ì¤€ë¹„í•˜ëŠ” ì—­í• ë„ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆ**ë‹¤. ì´ ì‹œí”„íŠ¸(shift) ë°©ë²•ì€ ì•„í‚¤í…ì²˜ë§ˆë‹¤ ì•½ê°„ì”© ë‹¤ë¥´ê¸° ë•Œë¬¸ì— DataCollatorForSeq2SeqëŠ” ëª¨ë¸ ê°ì²´ë¥¼ ì•Œì•„ì•¼ í•©**.l)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29a2ad88-a56d-441f-86e5-1dfeb903f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b9d6453-2349-49fc-8a3a-bfdbf1261fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}, {'input_ids': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]}]\n"
     ]
    }
   ],
   "source": [
    "data_collator_inputs = [tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n",
    "print(data_collator_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7e1f3ec-ecc9-49e6-800e-7015338c490a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[47591,    12,  9842, 19634,     9,     0, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "         28149,   139, 33712, 25218,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "           550,  7032,  5821,  7907, 12649,     0]]), 'decoder_input_ids': tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513],\n",
       "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "           817,   550,  7032,  5821,  7907, 12649]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_collator_inputs = [tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n",
    "#print(data_collator_inputs)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)]) # return batched tensor\n",
    "batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "106af827-20be-4fd2-ad1b-1c6cdac5b245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[47591,    12,  9842, 19634,     9,     0, 59513, 59513, 59513, 59513,\n",
      "         59513, 59513, 59513, 59513, 59513]])\n",
      "tensor([[ 577, 5891,    2, 3184,   16, 2542,    5, 1710,    0, -100, -100, -100,\n",
      "         -100, -100, -100, -100]])\n",
      "tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
      "         59513, 59513, 59513, 59513, 59513, 59513]])\n"
     ]
    }
   ],
   "source": [
    "# note the shift in the decoder_input_ids \n",
    "print(batch[\"input_ids\"][:1]) # 59513 is padded in the input_ids\n",
    "print(batch[\"labels\"][:1]) # note that -100 is padded in the labels, i.e. decoder target\n",
    "print(batch[\"decoder_input_ids\"][:1]) # 59513 is padded in the decode inputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08a3eb56-283a-4fd7-b51b-0ba041691873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}, {'input_ids': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]}]\n"
     ]
    }
   ],
   "source": [
    "print([tokenized_datasets[\"train\"][i] for i in range(1, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e778d5e4-672f-46ef-aa8c-785ef312c995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': '& Lauri. Watts. mail;', 'fr': '& Lauri. Watts. mail;'},\n",
       " {'en': 'ROLES_OF_TRANSLATORS', 'fr': '& traducteurJeromeBlanc;'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][\"translation\"][1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e91c8254-21de-48b0-92b3-bf763e2ee19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[47591, 12, 9842, 19634, 9, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]]}\n",
      "[{'input_ids': [[47591, 12, 9842, 19634, 9, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]]}]\n",
      "{'input_ids': [47591, 12, 9842, 19634, 9, 0, 1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0, 1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]}\n",
      "{'input_ids': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]}\n",
      "*sample_datasets: input_ids attention_mask labels\n",
      "*sample_datasets.values():  [[47591, 12, 9842, 19634, 9, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0]] [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]] [[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]]\n",
      "([47591, 12, 9842, 19634, 9, 0], [1, 1, 1, 1, 1, 1], [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0])\n",
      "([1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0])\n",
      "[{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}, {'input_ids': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]}]\n"
     ]
    }
   ],
   "source": [
    "sample_datasets = tokenized_datasets[\"train\"][1:3]\n",
    "print(sample_datasets)\n",
    "sample_datasets_list = [sample_datasets]\n",
    "print(sample_datasets_list)\n",
    "print({k: sum(sample_datasets[k], []) for k in sample_datasets.keys()})\n",
    "#print(sample_datasets[0])\n",
    "print(tokenized_datasets[\"train\"][2])\n",
    "print(\"*sample_datasets:\", *sample_datasets)\n",
    "print(\"*sample_datasets.values(): \", *sample_datasets.values())\n",
    "for t in zip(*sample_datasets.values()):\n",
    "    print(t)\n",
    "print([dict(zip(sample_datasets, t)) for t in zip(*sample_datasets.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3e54b1a-d705-440c-bbae-25354be9022c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "       [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "          817,   550,  7032,  5821,  7907, 12649,     0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2c151d9-b255-4f24-bc74-85da02f254ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513],\n",
       "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "           817,   550,  7032,  5821,  7907, 12649]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d69ef7d7-5923-454f-8a56-11985369ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder input:  <pad> Par dÃ©faut, dÃ©velopper les fils de discussion</s> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "decoder output:  Par dÃ©faut, dÃ©velopper les fils de discussion</s> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "print(\"decoder input: \", tokenizer.decode([59513,   577,  5891,  2,  3184,  16,  2542,     5,  1710,     0,\n",
    "         59513, 59513, 59513, 59513, 59513, 59513]))\n",
    "print(\"decoder output: \", tokenizer.decode([  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "527fa368-14db-44a7-afbb-e2b46a469bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder input:  <pad> *. ui *. UI_BAR_Fichiers interface utilisateur\n",
      "decoder output:  *. ui *. UI_BAR_Fichiers interface utilisateur</s>\n"
     ]
    }
   ],
   "source": [
    "print(\"decoder input: \", tokenizer.decode([59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
    "           817,   550,  7032,  5821,  7907, 12649]))\n",
    "print(\"decoder output: \", tokenizer.decode([ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
    "          817,   550,  7032,  5821,  7907, 12649,     0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c6aed31-60d5-45c4-8136-6fcd92b9fc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-fr', vocab_size=59514, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59513: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8670bb10-505b-46c2-88ee-4ef01f278f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]\n",
      "[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bb43b14-6dc2-4003-8871-36b8a58c48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4aea3f70-58f0-4386-a34d-43227bcdad5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35207/200089639.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "398098a5-1654-44bb-af2f-691350e75a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 46.750469682990165,\n",
       " 'counts': [11, 6, 4, 3],\n",
       " 'totals': [12, 11, 10, 9],\n",
       " 'precisions': [91.66666666666667,\n",
       "  54.54545454545455,\n",
       "  40.0,\n",
       "  33.333333333333336],\n",
       " 'bp': 0.9200444146293233,\n",
       " 'sys_len': 12,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f7c31e0-310c-437a-8773-30e4494a0549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1.683602693167689,\n",
       " 'counts': [1, 0, 0, 0],\n",
       " 'totals': [4, 3, 2, 1],\n",
       " 'precisions': [25.0, 16.666666666666668, 12.5, 12.5],\n",
       " 'bp': 0.10539922456186433,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This This This This\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0b9556d-d8c7-4068-b535-f0571d04893a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [2, 1, 0, 0],\n",
       " 'totals': [2, 1, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 0.004086771438464067,\n",
       " 'sys_len': 2,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This plugin\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba302787-6f84-4ad0-bc4b-4acb819af82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-fr', vocab_size=59514, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59513: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20e8e8a5-6b78-4f63-b9c6-8bfd23e0f57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\\n\\nfrom datasets import load_dataset, load_metric\\n\\nraw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\\n#split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\\nsplit_datasets = raw_datasets[\"train\"].select(range(10000)).train_test_split(train_size=0.9, seed=20)\\nsplit_datasets[\"validation\"] = split_datasets.pop(\"test\")\\n\\nmodel_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\\n#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\n\\nmax_input_length = 128\\nmax_target_length = 128\\n\\n\\ndef preprocess_function(examples):\\n    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\\n    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\\n\\n    # íƒ€ê²Ÿì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ì…‹ì—…\\n    with tokenizer.as_target_tokenizer():\\n        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\\n\\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\\n    return model_inputs\\n\\ntokenized_datasets = split_datasets.map(\\n    preprocess_function,\\n    batched=True,\\n    remove_columns=split_datasets[\"train\"].column_names,\\n)\\n\\nprint(tokenized_datasets)\\n#DatasetDict({\\n#    train: Dataset({\\n#        features: [\\'input_ids\\', \\'attention_mask\\', \\'labels\\'],\\n#        num_rows: 189155\\n#    })\\n#    validation: Dataset({\\n#        features: [\\'input_ids\\', \\'attention_mask\\', \\'labels\\'],\\n#        num_rows: 21018\\n#    })\\n#})\\n\\nfrom transformers import DataCollatorForSeq2Seq\\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\\n\\nfrom transformers import AutoModelForSeq2SeqLM\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\\n\\nfrom torch.utils.data import DataLoader\\n\\n#tokenized_datasets.set_format(\"torch\")\\ntrain_dataloader = DataLoader(\\n    tokenized_datasets[\"train\"],\\n    shuffle=True,\\n    collate_fn=data_collator,\\n    batch_size=8,\\n)\\neval_dataloader = DataLoader(\\n    tokenized_datasets[\"validation\"], \\n    collate_fn=data_collator, \\n    batch_size=8\\n)\\n\\noptimizer = AdamW(model.parameters(), lr=2e-5)\\n\\nfor batch in train_dataloader:\\n    break\\nprint(batch.keys()) #dict_keys([\\'input_ids\\', \\'attention_mask\\', \\'labels\\', \\'decoder_input_ids\\'])\\n#print(batch)\\n\\n#outputs = model(**batch) \\n#print(outputs.logits.shape) #torch.Size([8, 39, 59514])\\n#print(outputs.loss)\\n\\n#predictions = outputs.logits.argmax(-1) \\n#print(f\"predictions shape: {predictions.shape}\") #torch.Size([8, 39])\\n#print(f\"preds: {predictions}\")\\n#print(f\"labels: {batch[\\'labels\\']}\")\\n\\n#decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\\n#print(decoded_preds)\\n\\n#print(tokenizer.decode(predictions[0].tolist()))\\n#output = outputs.logits[0,:,:]\\n#print(output.shape)\\n#tokenizer.decode\\n\\nimport numpy as np\\n\\ndef compute_metrics(preds, labels):\\n    print(labels)\\n    #preds, labels = eval_preds\\n    # ëª¨ë¸ì´ ì˜ˆì¸¡ ë¡œì§“(logits)ì™¸ì— ë‹¤ë¥¸ ê²ƒì„ ë¦¬í„´í•˜ëŠ” ê²½ìš°.\\n    if isinstance(preds, tuple):\\n        preds = preds[0]\\n\\n    #print(f\"prediction: {preds.shape}, {preds}\") #torch.Size([8, 39]), batch size == 8\\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\\n\\n    print(decoded_preds)\\n\\n    #print(f\"labels : {labels.shape}, {type(labels)}\") #torch.Size([8, 39]), batch size == 8\\n    # -100ì€ ê±´ë„ˆë›´ë‹¤.\\n    #labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\\n    labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\\n\\n    #print(labels)\\n    \\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\\n\\n    print(decoded_labels)\\n\\n    # ë‹¨ìˆœ í›„ì²˜ë¦¬\\n    decoded_preds = [pred.strip() for pred in decoded_preds]\\n    decoded_labels = [[label.strip()] for label in decoded_labels]\\n    print(decoded_preds)\\n    print(decoded_labels)\\n## decoded_preds\\n#[\"L\\'Ã©diteur d\\'entrÃ©e pour pour les champs de L L L L\", \\'Taille du cylindre Taille Taille \\n#Taille Taille Taille Taille Taille Taille Taille Taille Taille Taille\\', \\'Allemandvieen \\n#deryerenAllemagnehÃ©) prÃ©sident le raisonss de la #ids.\\', \\'La fenÃªtre de marche La La La La La \\n#La La La La La La\\', \\'Ctrl; P Fichier Imprimer... C C C C C C C C\\', \\'Le mode Veurs de sources; \\n#V VPL.. Le Le Le\\', \\'Utilisation de & kkuickshow; Utilisation Utilisation Utilisation\\', \\n#\\'Types de champs Types Types Types Types Types Types Types Types Types Types Types Types \\n#Types Types\\']\\n## decoded_labels    \\n#[[\"L\\'Ã©diteur d\\'entrÃ©es pour les champs Tableau\"], [\\'Taille de cylindre\\'], \\n#[\\'Jan Willem van de Meent (Adios), pour les icÃ´nes de & krusader;\\'], [\\'La fenÃªtre de progression\\'],\\n#[\\'Ctrl; P Fichier Imprimer...\\'], [\\'Le mode Ã‰diteur de source et & VPL;.\\'],\\n#[\\'Utilisation de & kuickshow;\\'], [\\'Types de champs\\']]\\n    \\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\\n    return {\"bleu\": result[\"score\"]}\\n\\n\\nprint(batch[\"labels\"])\\n\\nscores = compute_metrics(predictions, batch[\"labels\"])\\nprint(scores)\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "#split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets = raw_datasets[\"train\"].select(range(10000)).train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # íƒ€ê²Ÿì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ì…‹ì—…\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "print(tokenized_datasets)\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 189155\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 21018\n",
    "#    })\n",
    "#})\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "print(batch.keys()) #dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n",
    "#print(batch)\n",
    "\n",
    "#outputs = model(**batch) \n",
    "#print(outputs.logits.shape) #torch.Size([8, 39, 59514])\n",
    "#print(outputs.loss)\n",
    "\n",
    "#predictions = outputs.logits.argmax(-1) \n",
    "#print(f\"predictions shape: {predictions.shape}\") #torch.Size([8, 39])\n",
    "#print(f\"preds: {predictions}\")\n",
    "#print(f\"labels: {batch['labels']}\")\n",
    "\n",
    "#decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#print(decoded_preds)\n",
    "\n",
    "#print(tokenizer.decode(predictions[0].tolist()))\n",
    "#output = outputs.logits[0,:,:]\n",
    "#print(output.shape)\n",
    "#tokenizer.decode\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    print(labels)\n",
    "    #preds, labels = eval_preds\n",
    "    # ëª¨ë¸ì´ ì˜ˆì¸¡ ë¡œì§“(logits)ì™¸ì— ë‹¤ë¥¸ ê²ƒì„ ë¦¬í„´í•˜ëŠ” ê²½ìš°.\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    #print(f\"prediction: {preds.shape}, {preds}\") #torch.Size([8, 39]), batch size == 8\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    print(decoded_preds)\n",
    "\n",
    "    #print(f\"labels : {labels.shape}, {type(labels)}\") #torch.Size([8, 39]), batch size == 8\n",
    "    # -100ì€ ê±´ë„ˆë›´ë‹¤.\n",
    "    #labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    #print(labels)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    print(decoded_labels)\n",
    "\n",
    "    # ë‹¨ìˆœ í›„ì²˜ë¦¬\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    print(decoded_preds)\n",
    "    print(decoded_labels)\n",
    "## decoded_preds\n",
    "#[\"L'Ã©diteur d'entrÃ©e pour pour les champs de L L L L\", 'Taille du cylindre Taille Taille \n",
    "#Taille Taille Taille Taille Taille Taille Taille Taille Taille Taille', 'Allemandvieen \n",
    "#deryerenAllemagnehÃ©) prÃ©sident le raisonss de la #ids.', 'La fenÃªtre de marche La La La La La \n",
    "#La La La La La La', 'Ctrl; P Fichier Imprimer... C C C C C C C C', 'Le mode Veurs de sources; \n",
    "#V VPL.. Le Le Le', 'Utilisation de & kkuickshow; Utilisation Utilisation Utilisation', \n",
    "#'Types de champs Types Types Types Types Types Types Types Types Types Types Types Types \n",
    "#Types Types']\n",
    "## decoded_labels    \n",
    "#[[\"L'Ã©diteur d'entrÃ©es pour les champs Tableau\"], ['Taille de cylindre'], \n",
    "#['Jan Willem van de Meent (Adios), pour les icÃ´nes de & krusader;'], ['La fenÃªtre de progression'],\n",
    "#['Ctrl; P Fichier Imprimer...'], ['Le mode Ã‰diteur de source et & VPL;.'],\n",
    "#['Utilisation de & kuickshow;'], ['Types de champs']]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "\n",
    "print(batch[\"labels\"])\n",
    "\n",
    "scores = compute_metrics(predictions, batch[\"labels\"])\n",
    "print(scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a244d09-ae85-4008-b289-5bd2d95dae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_values, max_indices = torch.max(output, dim=1)\n",
    "#print(max_values)\n",
    "#print(max_indices.tolist())\n",
    "#print(tokenizer.convert_ids_to_tokens(max_indices.tolist()))\n",
    "#print(tokenizer.decode(max_indices.tolist()))\n",
    "#print(tokenizer.decode(max_indices.tolist(), skip_special_tokens=True))\n",
    "#print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c42b0cb5-e045-4575-8cfa-7e8ccda99dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # ëª¨ë¸ì´ ì˜ˆì¸¡ ë¡œì§“(logits)ì™¸ì— ë‹¤ë¥¸ ê²ƒì„ ë¦¬í„´í•˜ëŠ” ê²½ìš°.\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # -100ì€ ê±´ë„ˆë›´ë‹¤.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ë‹¨ìˆœ í›„ì²˜ë¦¬\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d03e5c24-e32c-42ca-a027-cff774831666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cad01950-e726-4b6a-a4c6-8413f2d82f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ad850fe-e8fa-4740-8090-82c3db199bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-en-to-fr\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    #push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e396607-25da-4b5a-b868-69422078ef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 189155\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 21018\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "print(tokenized_datasets[\"train\"])\n",
    "print(tokenized_datasets[\"validation\"])\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a90e351f-bc6f-40a0-9efc-4574ce009e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5def8861-443e-40ee-90f3-b9c21ab13777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b1af3d3-75be-4a4d-a512-4da8fe32d29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='166' max='83' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [83/83 29:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6985208988189697,\n",
       " 'eval_bleu': 39.27124165416069,\n",
       " 'eval_runtime': 534.5806,\n",
       " 'eval_samples_per_second': 39.317,\n",
       " 'eval_steps_per_second': 0.155}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "161af7b0-dc6e-4e0d-8965-3f5068c9ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4434' max='4434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4434/4434 12:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.976100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.915100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4434, training_loss=1.0253028990074917, metrics={'train_runtime': 769.6242, 'train_samples_per_second': 737.327, 'train_steps_per_second': 5.761, 'total_flos': 1.6505690950139904e+16, 'train_loss': 1.0253028990074917, 'epoch': 3.0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5112f-c507-4d2f-8856-48903d2d23eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f56b59-dae6-41c0-b83d-82238ac79b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.push_to_hub(tags=\"tanslation\", commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62967fa2-ae7a-4321-8870-35fa0b110a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"hwang2006/marian-finetuned-kde4-en-to-fr\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb3601-7a10-417c-9149-13bf309d9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e40c5-84f8-4c7a-9f15-46a1980679b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator(\n",
    "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6dc4a8-2764-46a7-965a-120f8e45e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in raw_datasets[\"train\"][\"translation\"]:\n",
    "    if \" email\" in text[\"en\"]:\n",
    "        print(f\"{text}\\n\")\n",
    "        print(translator(text[\"en\"]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429650c0-bffc-424a-8a5a-a3407934a382",
   "metadata": {},
   "source": [
    "### \r\n",
    "ë§ì¶¤í˜• í•™ìŠµ ë£¨í”„ (Custom Training Loop\n",
    ")\r\n",
    "ì´ì œ ì „ì²´ í•™ìŠµ ë£¨í”„(full training loop)ë¥¼ ì‚´í´ë³´ê³  í•„ìš”í•œ ë¶€ë¶„ì„ ì‰½ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 7.2ì™€ 7.3ì—ì„œ í–ˆë˜ ê²ƒê³¼ ë§¤ìš° ìœ ì‚¬í•  ê²ƒì…ë‹ˆë‹¤.\r\n",
    "\r\n",
    "í•™ìŠµì„ ìœ„í•œ ëª¨ë“  ì‚¬í•­ ì¤€ë¹„í•˜ê¸°\r\n",
    "ì•„ë˜ ë‚´ìš©ì€ ì´ë¯¸ ëª‡ë²ˆì”© ê³µë¶€í–ˆìœ¼ë¯€ë¡œ ì½”ë“œë¥¼ ë§¤ìš° ë¹ ë¥´ê²Œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ì…‹ì„ \"torch\" í˜•ì‹ìœ¼ë¡œ ì„¤ì •í•œ í›„ ë°ì´í„°ì…‹ì—ì„œ DataLoaderë¥¼ ë¹Œë“œí•˜ì—¬ PyTorch í…ì„œë¥¼ ì–»ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eec006-733a-4e8d-989d-ea06038d32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577241a-c6b2-41e2-ade2-ea60f4a79001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ceb297-a99a-47e6-91e6-5632a0500e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c0df8-80bc-4ab6-a0e0-afe5c395f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d7eec-9e7c-4eeb-bb2f-38962f188a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc4ced-03f2-4a89-b723-28f262761c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "#model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "#repo_name = get_full_repo_name(model_name)\n",
    "#repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4661ae4-ef61-4342-8bb0-52f2236416cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "#repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4dacd8-7c2d-4bea-9170-908ea85fa31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7ada2-3689-4d6d-a449-22023a6eb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # í•™ìŠµ\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # í‰ê°€\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # ì˜ˆì¸¡ê³¼ ë ˆì´ë¸”ì„ ëª¨ìœ¼ê¸° ì „ì— í•¨ê»˜ íŒ¨ë”© ìˆ˜í–‰\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # ì €ì¥ ë° ì—…ë¡œë“œ\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        #repo.push_to_hub(\n",
    "        #    commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        #)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a70d60-500e-44a5-907a-146083875fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 199664\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 10509\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6575/850021938.py:61: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='658' max='329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [329/329 21:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training: {'eval_loss': 1.7459030151367188, 'eval_bleu': 38.9906643720949, 'eval_runtime': 309.8045, 'eval_samples_per_second': 33.921, 'eval_steps_per_second': 1.062}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18720' max='18720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18720/18720 47:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.995100</td>\n",
       "      <td>0.924919</td>\n",
       "      <td>49.642413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.891300</td>\n",
       "      <td>0.869644</td>\n",
       "      <td>52.180058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.827100</td>\n",
       "      <td>0.854542</td>\n",
       "      <td>52.685193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='329' max='329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [329/329 04:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training: {'eval_loss': 0.8545421957969666, 'eval_bleu': 52.64871528841124, 'eval_runtime': 327.0346, 'eval_samples_per_second': 32.134, 'eval_steps_per_second': 1.006, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "#raw_datasets = raw_datasets[\"train\"].select(range(1000))\n",
    "#split_datasets = raw_datasets[\"train\"].select(range(10000)).train_test_split(train_size=0.90, seed=20)\n",
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.95, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n",
    "print(split_datasets)\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # íƒ€ê²Ÿì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ì…‹ì—…\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "#print(tokenized_datasets)\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 189155\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 21018\n",
    "#    })\n",
    "#})\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    #print(preds.shape, labels.shape) # (500, 512) (500, 512)\n",
    "    # ëª¨ë¸ì´ ì˜ˆì¸¡ ë¡œì§“(logits)ì™¸ì— ë‹¤ë¥¸ ê²ƒì„ ë¦¬í„´í•˜ëŠ” ê²½ìš°.\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # -100ì€ ê±´ë„ˆë›´ë‹¤.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id) # replace -100 with pad_token\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ë‹¨ìˆœ í›„ì²˜ë¦¬\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    \n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-en-to-fr\",\n",
    "    #evaluation_strategy=\"no\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    #per_device_train_batch_size=32,\n",
    "    #per_device_eval_batch_size=64,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"before training: {trainer.evaluate(max_length=max_target_length)}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(f\"after training: {trainer.evaluate(max_length=max_target_length)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018fa328-2d70-49c9-a53e-a812254deb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 199664\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10509\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49868b1e764f4755a0bad9860b399370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27613347103949eea041f3ed57e89c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, BLEU score: 51.39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7df1601804044dd993bb8f0aa9fa2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, BLEU score: 53.02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2a3fc579ee49e5ae925d0c31cff5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, BLEU score: 53.90\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "#split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets = raw_datasets[\"train\"].shuffle(seed=20).select(range(10000)).train_test_split(train_size=0.95, seed=20)\n",
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.95, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # íƒ€ê²Ÿì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ì…‹ì—…\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "print(tokenized_datasets)\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 189155\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 21018\n",
    "#    })\n",
    "#})\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "#from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "#model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "#repo_name = get_full_repo_name(model_name)\n",
    "#repo_name\n",
    "\n",
    "output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "#repo = Repository(output_dir, clone_from=repo_name)\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    #print(f\"postprocess: {predictions.shape}, {labels.shape}\")\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # í•™ìŠµ\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # í‰ê°€\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "    #for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        #print(generated_tokens.shape, labels.shape)\n",
    "\n",
    "        # ì˜ˆì¸¡ê³¼ ë ˆì´ë¸”ì„ ëª¨ìœ¼ê¸° ì „ì— í•¨ê»˜ íŒ¨ë”© ìˆ˜í–‰\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # ì €ì¥ ë° ì—…ë¡œë“œ\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        #repo.push_to_hub(\n",
    "        #    commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        #)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc68360-fba2-4136-8cc1-21bd1a3219fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061676fb-a526-4864-8c97-ae8290986e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "from transformers import AdamW # try out with torch.optim.AdamW\n",
    "#from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "#split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets = raw_datasets[\"train\"].shuffle(seed=20).select(range(10000)).train_test_split(train_size=0.95, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def get_dataloaders(accelerator: Accelerator, model, batch_size: int = 32):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    max_input_length = 128\n",
    "    max_target_length = 128\n",
    "\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "        targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "        # íƒ€ê²Ÿì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ì…‹ì—…\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    # Apply the method we just defined to all the examples in all the splits of the dataset\n",
    "    # starting with the main process first:\n",
    "    with accelerator.main_process_first():\n",
    "        tokenized_datasets = split_datasets.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=split_datasets[\"train\"].column_names,\n",
    "        )\n",
    "\n",
    "    #print(tokenized_datasets)\n",
    "    #DatasetDict({\n",
    "    #    train: Dataset({\n",
    "    #        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "    #        num_rows: 189155\n",
    "    #    })\n",
    "    #    validation: Dataset({\n",
    "    #        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "    #        num_rows: 21018\n",
    "    #    })\n",
    "    #})\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    #tokenized_datasets.set_format(\"torch\")\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation\"], \n",
    "        collate_fn=data_collator, \n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, eval_dataloader, tokenizer\n",
    "\n",
    "\n",
    "#def training_function(config, args):\n",
    "def training_function():\n",
    "\n",
    "    batch_size = 8\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "    train_dataloader, eval_dataloader, tokenizer = get_dataloaders(accelerator, model, batch_size=batch_size)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    num_train_epochs = 3\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "\n",
    "    def postprocess(predictions, labels):\n",
    "        #print(f\"postprocess: {predictions.shape}, {labels.shape}\")\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "        # Replace -100 in the labels as we can't decode them\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "        return decoded_preds, decoded_labels\n",
    "\n",
    "    #metric = load_metric(\"sacrebleu\")\n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        # í•™ìŠµ\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # í‰ê°€\n",
    "        model.eval()\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "        #for batch in eval_dataloader:\n",
    "            with torch.no_grad():\n",
    "                generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                    batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=128,\n",
    "                )\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            #print(generated_tokens.shape, labels.shape)\n",
    "            #torch.Size([8, 15]) torch.Size([8, 17])\n",
    "            #torch.Size([8, 13]) torch.Size([8, 12])\n",
    "            #torch.Size([8, 32]) torch.Size([8, 28])\n",
    "            #torch.Size([8, 61]) torch.Size([8, 59])\n",
    "\n",
    "            # ì˜ˆì¸¡ê³¼ ë ˆì´ë¸”ì„ ëª¨ìœ¼ê¸° ì „ì— í•¨ê»˜ íŒ¨ë”© ìˆ˜í–‰\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "            #print(f\"after padding: {generated_tokens.shape}, {labels.shape}\")\n",
    "            #after padding: torch.Size([8, 61]), torch.Size([8, 59])\n",
    "            #after padding: torch.Size([8, 61]), torch.Size([8, 59])\n",
    "            #after padding: torch.Size([8, 61]), torch.Size([8, 59])\n",
    "            #after padding: torch.Size([8, 61]), torch.Size([8, 59])\n",
    "            \n",
    "\n",
    "            predictions_gathered = accelerator.gather(generated_tokens)\n",
    "            labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "            #print(f\"after gathered: {predictions_gathered.shape}, {labels_gathered.shape}\")\n",
    "            #after gathered: torch.Size([32, 61]), torch.Size([32, 59])\n",
    "            #after gathered: torch.Size([32, 61]), torch.Size([32, 59])\n",
    "            #after gathered: torch.Size([32, 61]), torch.Size([32, 59])\n",
    "            #after gathered: torch.Size([32, 61]), torch.Size([32, 59])\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "            metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "        results = metric.compute()\n",
    "        #print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "        accelerator.print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "        # ì €ì¥ ë° ì—…ë¡œë“œ\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            #repo.push_to_hub(\n",
    "            #    commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "            #)\n",
    "\n",
    "#from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "#model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "#repo_name = get_full_repo_name(model_name)\n",
    "#repo_name\n",
    "#repo = Repository(output_dir, clone_from=repo_name)\n",
    "#notebook_launcher(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb1fc8-f36d-440c-89da-a773b894a1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
