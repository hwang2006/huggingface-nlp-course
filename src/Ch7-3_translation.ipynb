{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d336f78b-d3ae-4aeb-82a2-3b44e24281c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 210173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c22a7f-6a8c-4c59-848a-a2ad8e9b1ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available kernels:\n",
      "  python3        /scratch/qualis/miniconda3/envs/transformer/share/jupyter/kernels/python3\n",
      "  alpaca         /home01/qualis/.local/share/jupyter/kernels/alpaca\n",
      "  genai          /home01/qualis/.local/share/jupyter/kernels/genai\n",
      "  transformer    /home01/qualis/.local/share/jupyter/kernels/transformer\n"
     ]
    }
   ],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66744fe0-91f2-4a1c-afeb-c70628f0de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_datasets[\"train\"].select(range(1000)).train_test_split(train_size=0.9, seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcddb51-2aeb-44b9-8fe0-188ced3b443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56aaac06-44e0-4ba8-9175-20bf7760dca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9eec2a-0628-4a70-b66f-36002256b7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c713c344-d79b-475f-8e0e-9e38179e7629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Default to expanded threads',\n",
       " 'fr': 'Par défaut, développer les fils de discussion'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b430d5-f6c7-4890-8d08-c005eb38ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c60a9fd-5c13-47e3-bb2c-250da953ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2119fabc-c633-4274-a2b0-5538fe27be25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.37.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#!pip show transformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2799f70f-168f-4470-94f7-0336a9f66296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "103d6ddc-1ac3-43be-a1eb-0aaab2abcdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Par défaut pour les threads élargis'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ee77b3-9359-4215-a3e0-f165b30b52c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',\n",
       " 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][172][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79f25e40-4465-4edc-b030-9bb906398ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\n",
    "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47f34c35-b51f-475e-91d5-77264cda0582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': \"Otherwise, forward us the backtraces when the email pops-up (you'll see). If you get a crash often you will get it fixed for sure if you forward a really good backtrace to us. See the next sections for assistance there.\", 'fr': 'Sinon, envoyez -nous les logs quand le courrier électronique apparaît (vous verrez). Si & amarok; plante souvent, le bogue sera corrigé si vous nous envoyez un bon log. Voir la section suivante pour savoir comment obtenir un bon log.'}\n",
      "\n",
      "[{'translation_text': \"Sinon, faites-nous suivre les backtraces lorsque l'email s'affiche (vous verrez). Si vous obtenez souvent un crash, vous l'obtiendrez corrigé pour être sûr que vous nous transmettez une très bonne backtrace. Consultez les prochaines sections pour de l'aide là-bas.\"}]\n"
     ]
    }
   ],
   "source": [
    "for text in raw_datasets[\"train\"][\"translation\"]:\n",
    "    if \" email\" in text[\"en\"]:\n",
    "        print(f\"{text}\\n\")\n",
    "        print(translator(text[\"en\"]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69390d16-1030-4510-89b0-66ada18dc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c16ccf71-c389-456a-8fe5-600048b8df64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence)\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(fr_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b73359c7-ad54-4213-8e71-c27718acae52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Default', '▁to', '▁expanded', '▁thread', 's', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcccefab-fd6d-498f-a111-64b9b76c95c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "187e934f-8105-409f-bf70-826d1f72cd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Par', '▁dé', 'f', 'aut', ',', '▁dé', 've', 'lop', 'per', '▁les', '▁fil', 's', '▁de', '▁discussion', '</s>']\n",
      "['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(fr_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d631cb26-130b-4c6c-a84a-901897203c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a2d05e1-6abe-4abf-a629-fd01ae415624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c754143-ac5e-495b-a3e4-75647ad0c493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=fr_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cf64243-2e86-4824-b2bd-16ab20193cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Par', '▁dé', 'f', 'aut', ',', '▁dé', 've', 'lop', 'per', '▁les', '▁fil', 's', '▁de', '▁discussion', '</s>']\n",
      "['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(fr_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7534e351-52a9-46cd-95f8-6ce56d93925b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    target = tokenizer(fr_sentence)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41606066-262a-46ce-b4f0-4cae1b0a9ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-fr', vocab_size=59514, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59513: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f909f3c-b217-448e-996f-4b570a970e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [47591, 12, 9842, 19634, 9, 0]\n",
      "text: Default to expanded threads</s>\n",
      "tokens: ['▁Default', '▁to', '▁expanded', '▁thread', 's', '</s>']\n"
     ]
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "model_inputs = tokenizer(en_sentence, max_length=max_input_length, truncation=True)\n",
    "print(f\"input_ids: {model_inputs['input_ids']}\")\n",
    "print(f\"text: {tokenizer.decode(model_inputs['input_ids'])}\")\n",
    "print(f\"tokens: {tokenizer.convert_ids_to_tokens(model_inputs['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a699fbc-f314-4205-b9bd-c705fbb5f5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [47591, 12, 9842, 19634, 9, 0]\n",
      "text: Default to expanded threads</s>\n",
      "tokens: ['▁Default', '▁to', '▁expanded', '▁thread', 's', '</s>']\n"
     ]
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "model_inputs = tokenizer(en_sentence, max_length=max_input_length, padding = \"max_length\", truncation=True) # len(input_ids): 128\n",
    "model_inputs = tokenizer(en_sentence, max_length=max_input_length, padding = True, truncation=True) #len(input_ids):6  \n",
    "print(f\"input_ids: {model_inputs['input_ids']}\")\n",
    "print(f\"text: {tokenizer.decode(model_inputs['input_ids'])}\")\n",
    "print(f\"tokens: {tokenizer.convert_ids_to_tokens(model_inputs['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a2690ad-4852-4430-8cce-383456edd584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-fr', vocab_size=59514, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59513: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7976e2b1-b35e-4b80-a0aa-d5ee44dbe8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '152754',\n",
       " 'translation': {'en': 'Default to expanded threads',\n",
       "  'fr': 'Par défaut, développer les fils de discussion'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dec852b-ef74-4174-af28-0a6e12a60fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    #print(f\"examples: {examples['translation']}\")\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # 타겟을 위한 토크나이저 셋업\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    #print(f\"labels: {labels}\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "479dde03-0733-42cc-9df6-8f4f2cdc7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "        #inputs, text_target=targets, max_length=max_length, padding=True, truncation=True\n",
    "    )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4daa0734-d483-436a-b571-17ab17783149",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07f9f07c-bac3-4432-8689-02de0ec88c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[34378, 226, 5783, 32, 200, 12, 3647, 4, 1223, 1628, 117, 4923, 23608, 3, 1789, 2942, 20059, 301, 548, 301, 331, 30, 117, 4923, 12, 4, 1528, 668, 3, 5734, 212, 9319, 30, 4, 4923, 57, 5487, 30, 4, 6, 32712, 25, 7243, 1160, 12, 621, 42, 4, 1156, 3009, 3, 0], [47591, 12, 9842, 19634, 9, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'labels': [[60, 7418, 5244, 8234, 740, 4993, 8, 6471, 5, 2218, 29, 193, 2220, 742, 3, 4366, 14237, 14, 6, 16600, 301, 548, 301, 331, 5, 193, 24275, 17, 8, 668, 6142, 3, 33640, 36, 81, 6, 5411, 2709, 9376, 22, 24275, 59, 36, 19, 9376, 153, 402, 29033, 13774, 402, 29033, 416, 27, 8, 4034, 4888, 3, 0], [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]]}\n",
      "[52, 6, 15, 3, 12, 7, 128, 59, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][:2])\n",
    "print([len(elem) for elem in tokenized_datasets[\"train\"][:10][\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3eadf6e-2a78-4383-89a6-847867321eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[34378, 226, 5783, 32, 200, 12, 3647, 4, 1223, 1628, 117, 4923, 23608, 3, 1789, 2942, 20059, 301, 548, 301, 331, 30, 117, 4923, 12, 4, 1528, 668, 3, 5734, 212, 9319, 30, 4, 4923, 57, 5487, 30, 4, 6, 32712, 25, 7243, 1160, 12, 621, 42, 4, 1156, 3009, 3, 0], [47591, 12, 9842, 19634, 9, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'labels': [[60, 7418, 5244, 8234, 740, 4993, 8, 6471, 5, 2218, 29, 193, 2220, 742, 3, 4366, 14237, 14, 6, 16600, 301, 548, 301, 331, 5, 193, 24275, 17, 8, 668, 6142, 3, 33640, 36, 81, 6, 5411, 2709, 9376, 22, 24275, 59, 36, 19, 9376, 153, 402, 29033, 13774, 402, 29033, 416, 27, 8, 4034, 4888, 3, 0], [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = preprocess_function(split_datasets[\"train\"][:2])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3fcab25-b267-4829-9995-f07405672bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "897d5ff1-1f37-4346-a6f5-9395dc9e67da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(59514, 512, padding_idx=59513)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=59514, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220aa439-66b0-4d2e-9ffa-ad5b025e0963",
   "metadata": {},
   "source": [
    "### 데이터 콜레이션 (Data Collation)\r\n",
    "동적 배치 처리(Dynamic batching)를 위한 패딩을 처리하려면 데이터 콜레이터가 필요합니다. 이 경우 3장에서와 같이 DataCollatorWithPadding을 사용할 수 없습니다. 왜냐하면 이 메서드는 입력(input IDs, attention mask 및 token type IDs)에 대해서만 패딩을 수행하기 때문입니다. 레이블 역시 레이블에 있는 최대 길이로 채워져야 합니다. 그리고 앞서 언급했듯이 레이블을 채우는데 사용되는 패딩 값은 토크나이저의 패딩 토큰이 아니라 -100이어야 합니다. 그래야 패딩된 값이 손실 계산에서 무시됩니다.\r\n",
    "\r\n",
    "이 작업은 모두 DataCollatorForSeq2Seq에 의해 수행됩니다. DataCollatorWithPadding과 마찬가지로, DataCollatorForSeq2Seq는 입력을 전처리하는데 사용되는 tokenizer는 물론 모델 자체도 매개변수로 입력받습니다. 모델도 입력받는 이유는 이 데이터 콜레이터가 시작 부분에 특수 토큰이 붙어 있는, 레이블 시퀀스를 우측으로 시프트(shift)한 버전인 디코더 input IDs를 준비하는 역할도 하기 때문입니**다. 이 시프트(shift) 방법은 아키텍처마다 약간씩 다르기 때문에 DataCollatorForSeq2Seq는 모델 객체를 알아야 합**.l)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29a2ad88-a56d-441f-86e5-1dfeb903f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b9d6453-2349-49fc-8a3a-bfdbf1261fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}, {'input_ids': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]}]\n"
     ]
    }
   ],
   "source": [
    "data_collator_inputs = [tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n",
    "print(data_collator_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7e1f3ec-ecc9-49e6-800e-7015338c490a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[47591,    12,  9842, 19634,     9,     0, 59513, 59513, 59513, 59513,\n",
       "         59513, 59513, 59513, 59513, 59513],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "         28149,   139, 33712, 25218,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "           550,  7032,  5821,  7907, 12649,     0]]), 'decoder_input_ids': tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513],\n",
       "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "           817,   550,  7032,  5821,  7907, 12649]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_collator_inputs = [tokenized_datasets[\"train\"][i] for i in range(1, 3)]\n",
    "#print(data_collator_inputs)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)]) # return batched tensor\n",
    "batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "106af827-20be-4fd2-ad1b-1c6cdac5b245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[47591,    12,  9842, 19634,     9,     0, 59513, 59513, 59513, 59513,\n",
      "         59513, 59513, 59513, 59513, 59513]])\n",
      "tensor([[ 577, 5891,    2, 3184,   16, 2542,    5, 1710,    0, -100, -100, -100,\n",
      "         -100, -100, -100, -100]])\n",
      "tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
      "         59513, 59513, 59513, 59513, 59513, 59513]])\n"
     ]
    }
   ],
   "source": [
    "# note the shift in the decoder_input_ids \n",
    "print(batch[\"input_ids\"][:1]) # 59513 is padded in the input_ids\n",
    "print(batch[\"labels\"][:1]) # note that -100 is padded in the labels, i.e. decoder target\n",
    "print(batch[\"decoder_input_ids\"][:1]) # 59513 is padded in the decode inputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08a3eb56-283a-4fd7-b51b-0ba041691873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}, {'input_ids': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]}]\n"
     ]
    }
   ],
   "source": [
    "print([tokenized_datasets[\"train\"][i] for i in range(1, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e778d5e4-672f-46ef-aa8c-785ef312c995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': '& Lauri. Watts. mail;', 'fr': '& Lauri. Watts. mail;'},\n",
       " {'en': 'ROLES_OF_TRANSLATORS', 'fr': '& traducteurJeromeBlanc;'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][\"translation\"][1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e91c8254-21de-48b0-92b3-bf763e2ee19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[47591, 12, 9842, 19634, 9, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]]}\n",
      "[{'input_ids': [[47591, 12, 9842, 19634, 9, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]]}]\n",
      "{'input_ids': [47591, 12, 9842, 19634, 9, 0, 1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0, 1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]}\n",
      "{'input_ids': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]}\n",
      "*sample_datasets: input_ids attention_mask labels\n",
      "*sample_datasets.values():  [[47591, 12, 9842, 19634, 9, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0]] [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]] [[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]]\n",
      "([47591, 12, 9842, 19634, 9, 0], [1, 1, 1, 1, 1, 1], [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0])\n",
      "([1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0])\n",
      "[{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}, {'input_ids': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 28149, 139, 33712, 25218, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]}]\n"
     ]
    }
   ],
   "source": [
    "sample_datasets = tokenized_datasets[\"train\"][1:3]\n",
    "print(sample_datasets)\n",
    "sample_datasets_list = [sample_datasets]\n",
    "print(sample_datasets_list)\n",
    "print({k: sum(sample_datasets[k], []) for k in sample_datasets.keys()})\n",
    "#print(sample_datasets[0])\n",
    "print(tokenized_datasets[\"train\"][2])\n",
    "print(\"*sample_datasets:\", *sample_datasets)\n",
    "print(\"*sample_datasets.values(): \", *sample_datasets.values())\n",
    "for t in zip(*sample_datasets.values()):\n",
    "    print(t)\n",
    "print([dict(zip(sample_datasets, t)) for t in zip(*sample_datasets.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3e54b1a-d705-440c-bbae-25354be9022c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "       [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "          817,   550,  7032,  5821,  7907, 12649,     0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2c151d9-b255-4f24-bc74-85da02f254ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513],\n",
       "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "           817,   550,  7032,  5821,  7907, 12649]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d69ef7d7-5923-454f-8a56-11985369ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder input:  <pad> Par défaut, développer les fils de discussion</s> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "decoder output:  Par défaut, développer les fils de discussion</s> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "print(\"decoder input: \", tokenizer.decode([59513,   577,  5891,  2,  3184,  16,  2542,     5,  1710,     0,\n",
    "         59513, 59513, 59513, 59513, 59513, 59513]))\n",
    "print(\"decoder output: \", tokenizer.decode([  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "527fa368-14db-44a7-afbb-e2b46a469bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder input:  <pad> *. ui *. UI_BAR_Fichiers interface utilisateur\n",
      "decoder output:  *. ui *. UI_BAR_Fichiers interface utilisateur</s>\n"
     ]
    }
   ],
   "source": [
    "print(\"decoder input: \", tokenizer.decode([59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
    "           817,   550,  7032,  5821,  7907, 12649]))\n",
    "print(\"decoder output: \", tokenizer.decode([ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
    "          817,   550,  7032,  5821,  7907, 12649,     0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c6aed31-60d5-45c4-8136-6fcd92b9fc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-fr', vocab_size=59514, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59513: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8670bb10-505b-46c2-88ee-4ef01f278f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]\n",
      "[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bb43b14-6dc2-4003-8871-36b8a58c48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4aea3f70-58f0-4386-a34d-43227bcdad5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35207/200089639.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "398098a5-1654-44bb-af2f-691350e75a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 46.750469682990165,\n",
       " 'counts': [11, 6, 4, 3],\n",
       " 'totals': [12, 11, 10, 9],\n",
       " 'precisions': [91.66666666666667,\n",
       "  54.54545454545455,\n",
       "  40.0,\n",
       "  33.333333333333336],\n",
       " 'bp': 0.9200444146293233,\n",
       " 'sys_len': 12,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f7c31e0-310c-437a-8773-30e4494a0549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1.683602693167689,\n",
       " 'counts': [1, 0, 0, 0],\n",
       " 'totals': [4, 3, 2, 1],\n",
       " 'precisions': [25.0, 16.666666666666668, 12.5, 12.5],\n",
       " 'bp': 0.10539922456186433,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This This This This\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0b9556d-d8c7-4068-b535-f0571d04893a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [2, 1, 0, 0],\n",
       " 'totals': [2, 1, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 0.004086771438464067,\n",
       " 'sys_len': 2,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This plugin\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba302787-6f84-4ad0-bc4b-4acb819af82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-fr', vocab_size=59514, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t59513: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20e8e8a5-6b78-4f63-b9c6-8bfd23e0f57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\\n\\nfrom datasets import load_dataset, load_metric\\n\\nraw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\\n#split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\\nsplit_datasets = raw_datasets[\"train\"].select(range(10000)).train_test_split(train_size=0.9, seed=20)\\nsplit_datasets[\"validation\"] = split_datasets.pop(\"test\")\\n\\nmodel_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\\n#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\n\\nmax_input_length = 128\\nmax_target_length = 128\\n\\n\\ndef preprocess_function(examples):\\n    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\\n    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\\n\\n    # 타겟을 위한 토크나이저 셋업\\n    with tokenizer.as_target_tokenizer():\\n        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\\n\\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\\n    return model_inputs\\n\\ntokenized_datasets = split_datasets.map(\\n    preprocess_function,\\n    batched=True,\\n    remove_columns=split_datasets[\"train\"].column_names,\\n)\\n\\nprint(tokenized_datasets)\\n#DatasetDict({\\n#    train: Dataset({\\n#        features: [\\'input_ids\\', \\'attention_mask\\', \\'labels\\'],\\n#        num_rows: 189155\\n#    })\\n#    validation: Dataset({\\n#        features: [\\'input_ids\\', \\'attention_mask\\', \\'labels\\'],\\n#        num_rows: 21018\\n#    })\\n#})\\n\\nfrom transformers import DataCollatorForSeq2Seq\\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\\n\\nfrom transformers import AutoModelForSeq2SeqLM\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\\n\\nfrom torch.utils.data import DataLoader\\n\\n#tokenized_datasets.set_format(\"torch\")\\ntrain_dataloader = DataLoader(\\n    tokenized_datasets[\"train\"],\\n    shuffle=True,\\n    collate_fn=data_collator,\\n    batch_size=8,\\n)\\neval_dataloader = DataLoader(\\n    tokenized_datasets[\"validation\"], \\n    collate_fn=data_collator, \\n    batch_size=8\\n)\\n\\noptimizer = AdamW(model.parameters(), lr=2e-5)\\n\\nfor batch in train_dataloader:\\n    break\\nprint(batch.keys()) #dict_keys([\\'input_ids\\', \\'attention_mask\\', \\'labels\\', \\'decoder_input_ids\\'])\\n#print(batch)\\n\\n#outputs = model(**batch) \\n#print(outputs.logits.shape) #torch.Size([8, 39, 59514])\\n#print(outputs.loss)\\n\\n#predictions = outputs.logits.argmax(-1) \\n#print(f\"predictions shape: {predictions.shape}\") #torch.Size([8, 39])\\n#print(f\"preds: {predictions}\")\\n#print(f\"labels: {batch[\\'labels\\']}\")\\n\\n#decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\\n#print(decoded_preds)\\n\\n#print(tokenizer.decode(predictions[0].tolist()))\\n#output = outputs.logits[0,:,:]\\n#print(output.shape)\\n#tokenizer.decode\\n\\nimport numpy as np\\n\\ndef compute_metrics(preds, labels):\\n    print(labels)\\n    #preds, labels = eval_preds\\n    # 모델이 예측 로짓(logits)외에 다른 것을 리턴하는 경우.\\n    if isinstance(preds, tuple):\\n        preds = preds[0]\\n\\n    #print(f\"prediction: {preds.shape}, {preds}\") #torch.Size([8, 39]), batch size == 8\\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\\n\\n    print(decoded_preds)\\n\\n    #print(f\"labels : {labels.shape}, {type(labels)}\") #torch.Size([8, 39]), batch size == 8\\n    # -100은 건너뛴다.\\n    #labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\\n    labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\\n\\n    #print(labels)\\n    \\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\\n\\n    print(decoded_labels)\\n\\n    # 단순 후처리\\n    decoded_preds = [pred.strip() for pred in decoded_preds]\\n    decoded_labels = [[label.strip()] for label in decoded_labels]\\n    print(decoded_preds)\\n    print(decoded_labels)\\n## decoded_preds\\n#[\"L\\'éditeur d\\'entrée pour pour les champs de L L L L\", \\'Taille du cylindre Taille Taille \\n#Taille Taille Taille Taille Taille Taille Taille Taille Taille Taille\\', \\'Allemandvieen \\n#deryerenAllemagnehé) président le raisonss de la #ids.\\', \\'La fenêtre de marche La La La La La \\n#La La La La La La\\', \\'Ctrl; P Fichier Imprimer... C C C C C C C C\\', \\'Le mode Veurs de sources; \\n#V VPL.. Le Le Le\\', \\'Utilisation de & kkuickshow; Utilisation Utilisation Utilisation\\', \\n#\\'Types de champs Types Types Types Types Types Types Types Types Types Types Types Types \\n#Types Types\\']\\n## decoded_labels    \\n#[[\"L\\'éditeur d\\'entrées pour les champs Tableau\"], [\\'Taille de cylindre\\'], \\n#[\\'Jan Willem van de Meent (Adios), pour les icônes de & krusader;\\'], [\\'La fenêtre de progression\\'],\\n#[\\'Ctrl; P Fichier Imprimer...\\'], [\\'Le mode Éditeur de source et & VPL;.\\'],\\n#[\\'Utilisation de & kuickshow;\\'], [\\'Types de champs\\']]\\n    \\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\\n    return {\"bleu\": result[\"score\"]}\\n\\n\\nprint(batch[\"labels\"])\\n\\nscores = compute_metrics(predictions, batch[\"labels\"])\\nprint(scores)\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "#split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets = raw_datasets[\"train\"].select(range(10000)).train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # 타겟을 위한 토크나이저 셋업\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "print(tokenized_datasets)\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 189155\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 21018\n",
    "#    })\n",
    "#})\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "print(batch.keys()) #dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n",
    "#print(batch)\n",
    "\n",
    "#outputs = model(**batch) \n",
    "#print(outputs.logits.shape) #torch.Size([8, 39, 59514])\n",
    "#print(outputs.loss)\n",
    "\n",
    "#predictions = outputs.logits.argmax(-1) \n",
    "#print(f\"predictions shape: {predictions.shape}\") #torch.Size([8, 39])\n",
    "#print(f\"preds: {predictions}\")\n",
    "#print(f\"labels: {batch['labels']}\")\n",
    "\n",
    "#decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#print(decoded_preds)\n",
    "\n",
    "#print(tokenizer.decode(predictions[0].tolist()))\n",
    "#output = outputs.logits[0,:,:]\n",
    "#print(output.shape)\n",
    "#tokenizer.decode\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    print(labels)\n",
    "    #preds, labels = eval_preds\n",
    "    # 모델이 예측 로짓(logits)외에 다른 것을 리턴하는 경우.\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    #print(f\"prediction: {preds.shape}, {preds}\") #torch.Size([8, 39]), batch size == 8\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    print(decoded_preds)\n",
    "\n",
    "    #print(f\"labels : {labels.shape}, {type(labels)}\") #torch.Size([8, 39]), batch size == 8\n",
    "    # -100은 건너뛴다.\n",
    "    #labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    #print(labels)\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    print(decoded_labels)\n",
    "\n",
    "    # 단순 후처리\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    print(decoded_preds)\n",
    "    print(decoded_labels)\n",
    "## decoded_preds\n",
    "#[\"L'éditeur d'entrée pour pour les champs de L L L L\", 'Taille du cylindre Taille Taille \n",
    "#Taille Taille Taille Taille Taille Taille Taille Taille Taille Taille', 'Allemandvieen \n",
    "#deryerenAllemagnehé) président le raisonss de la #ids.', 'La fenêtre de marche La La La La La \n",
    "#La La La La La La', 'Ctrl; P Fichier Imprimer... C C C C C C C C', 'Le mode Veurs de sources; \n",
    "#V VPL.. Le Le Le', 'Utilisation de & kkuickshow; Utilisation Utilisation Utilisation', \n",
    "#'Types de champs Types Types Types Types Types Types Types Types Types Types Types Types \n",
    "#Types Types']\n",
    "## decoded_labels    \n",
    "#[[\"L'éditeur d'entrées pour les champs Tableau\"], ['Taille de cylindre'], \n",
    "#['Jan Willem van de Meent (Adios), pour les icônes de & krusader;'], ['La fenêtre de progression'],\n",
    "#['Ctrl; P Fichier Imprimer...'], ['Le mode Éditeur de source et & VPL;.'],\n",
    "#['Utilisation de & kuickshow;'], ['Types de champs']]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "\n",
    "print(batch[\"labels\"])\n",
    "\n",
    "scores = compute_metrics(predictions, batch[\"labels\"])\n",
    "print(scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a244d09-ae85-4008-b289-5bd2d95dae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_values, max_indices = torch.max(output, dim=1)\n",
    "#print(max_values)\n",
    "#print(max_indices.tolist())\n",
    "#print(tokenizer.convert_ids_to_tokens(max_indices.tolist()))\n",
    "#print(tokenizer.decode(max_indices.tolist()))\n",
    "#print(tokenizer.decode(max_indices.tolist(), skip_special_tokens=True))\n",
    "#print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c42b0cb5-e045-4575-8cfa-7e8ccda99dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # 모델이 예측 로짓(logits)외에 다른 것을 리턴하는 경우.\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # -100은 건너뛴다.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 단순 후처리\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d03e5c24-e32c-42ca-a027-cff774831666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cad01950-e726-4b6a-a4c6-8413f2d82f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ad850fe-e8fa-4740-8090-82c3db199bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-en-to-fr\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    #push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e396607-25da-4b5a-b868-69422078ef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 189155\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 21018\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "print(tokenized_datasets[\"train\"])\n",
    "print(tokenized_datasets[\"validation\"])\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a90e351f-bc6f-40a0-9efc-4574ce009e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5def8861-443e-40ee-90f3-b9c21ab13777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b1af3d3-75be-4a4d-a512-4da8fe32d29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='166' max='83' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [83/83 29:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6985208988189697,\n",
       " 'eval_bleu': 39.27124165416069,\n",
       " 'eval_runtime': 534.5806,\n",
       " 'eval_samples_per_second': 39.317,\n",
       " 'eval_steps_per_second': 0.155}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "161af7b0-dc6e-4e0d-8965-3f5068c9ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4434' max='4434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4434/4434 12:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.976100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.915100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4434, training_loss=1.0253028990074917, metrics={'train_runtime': 769.6242, 'train_samples_per_second': 737.327, 'train_steps_per_second': 5.761, 'total_flos': 1.6505690950139904e+16, 'train_loss': 1.0253028990074917, 'epoch': 3.0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5112f-c507-4d2f-8856-48903d2d23eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f56b59-dae6-41c0-b83d-82238ac79b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.push_to_hub(tags=\"tanslation\", commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62967fa2-ae7a-4321-8870-35fa0b110a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"hwang2006/marian-finetuned-kde4-en-to-fr\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb3601-7a10-417c-9149-13bf309d9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e40c5-84f8-4c7a-9f15-46a1980679b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator(\n",
    "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6dc4a8-2764-46a7-965a-120f8e45e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in raw_datasets[\"train\"][\"translation\"]:\n",
    "    if \" email\" in text[\"en\"]:\n",
    "        print(f\"{text}\\n\")\n",
    "        print(translator(text[\"en\"]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429650c0-bffc-424a-8a5a-a3407934a382",
   "metadata": {},
   "source": [
    "### \r\n",
    "맞춤형 학습 루프 (Custom Training Loop\n",
    ")\r\n",
    "이제 전체 학습 루프(full training loop)를 살펴보고 필요한 부분을 쉽게 커스터마이징할 수 있습니다. 7.2와 7.3에서 했던 것과 매우 유사할 것입니다.\r\n",
    "\r\n",
    "학습을 위한 모든 사항 준비하기\r\n",
    "아래 내용은 이미 몇번씩 공부했으므로 코드를 매우 빠르게 살펴보겠습니다. 먼저 데이터셋을 \"torch\" 형식으로 설정한 후 데이터셋에서 DataLoader를 빌드하여 PyTorch 텐서를 얻습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eec006-733a-4e8d-989d-ea06038d32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577241a-c6b2-41e2-ade2-ea60f4a79001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ceb297-a99a-47e6-91e6-5632a0500e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c0df8-80bc-4ab6-a0e0-afe5c395f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d7eec-9e7c-4eeb-bb2f-38962f188a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc4ced-03f2-4a89-b723-28f262761c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "#model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "#repo_name = get_full_repo_name(model_name)\n",
    "#repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4661ae4-ef61-4342-8bb0-52f2236416cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "#repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4dacd8-7c2d-4bea-9170-908ea85fa31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7ada2-3689-4d6d-a449-22023a6eb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # 학습\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # 평가\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # 예측과 레이블을 모으기 전에 함께 패딩 수행\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # 저장 및 업로드\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        #repo.push_to_hub(\n",
    "        #    commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        #)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a70d60-500e-44a5-907a-146083875fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 199664\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 10509\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6575/850021938.py:61: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='658' max='329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [329/329 21:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training: {'eval_loss': 1.7459030151367188, 'eval_bleu': 38.9906643720949, 'eval_runtime': 309.8045, 'eval_samples_per_second': 33.921, 'eval_steps_per_second': 1.062}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18720' max='18720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18720/18720 47:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.995100</td>\n",
       "      <td>0.924919</td>\n",
       "      <td>49.642413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.891300</td>\n",
       "      <td>0.869644</td>\n",
       "      <td>52.180058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.827100</td>\n",
       "      <td>0.854542</td>\n",
       "      <td>52.685193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='329' max='329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [329/329 04:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training: {'eval_loss': 0.8545421957969666, 'eval_bleu': 52.64871528841124, 'eval_runtime': 327.0346, 'eval_samples_per_second': 32.134, 'eval_steps_per_second': 1.006, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "#raw_datasets = raw_datasets[\"train\"].select(range(1000))\n",
    "#split_datasets = raw_datasets[\"train\"].select(range(10000)).train_test_split(train_size=0.90, seed=20)\n",
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.95, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n",
    "print(split_datasets)\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # 타겟을 위한 토크나이저 셋업\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "#print(tokenized_datasets)\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 189155\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 21018\n",
    "#    })\n",
    "#})\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    #print(preds.shape, labels.shape) # (500, 512) (500, 512)\n",
    "    # 모델이 예측 로짓(logits)외에 다른 것을 리턴하는 경우.\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # -100은 건너뛴다.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id) # replace -100 with pad_token\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 단순 후처리\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    \n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-en-to-fr\",\n",
    "    #evaluation_strategy=\"no\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    #per_device_train_batch_size=32,\n",
    "    #per_device_eval_batch_size=64,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"before training: {trainer.evaluate(max_length=max_target_length)}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(f\"after training: {trainer.evaluate(max_length=max_target_length)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018fa328-2d70-49c9-a53e-a812254deb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 199664\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10509\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49868b1e764f4755a0bad9860b399370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27613347103949eea041f3ed57e89c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, BLEU score: 51.39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7df1601804044dd993bb8f0aa9fa2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, BLEU score: 53.02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2a3fc579ee49e5ae925d0c31cff5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, BLEU score: 53.90\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "#split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets = raw_datasets[\"train\"].shuffle(seed=20).select(range(10000)).train_test_split(train_size=0.95, seed=20)\n",
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.95, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # 타겟을 위한 토크나이저 셋업\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "print(tokenized_datasets)\n",
    "#DatasetDict({\n",
    "#    train: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 189155\n",
    "#    })\n",
    "#    validation: Dataset({\n",
    "#        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#        num_rows: 21018\n",
    "#    })\n",
    "#})\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "#from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "#model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "#repo_name = get_full_repo_name(model_name)\n",
    "#repo_name\n",
    "\n",
    "output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "#repo = Repository(output_dir, clone_from=repo_name)\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    #print(f\"postprocess: {predictions.shape}, {labels.shape}\")\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # 학습\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # 평가\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "    #for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        #print(generated_tokens.shape, labels.shape)\n",
    "\n",
    "        # 예측과 레이블을 모으기 전에 함께 패딩 수행\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # 저장 및 업로드\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        #repo.push_to_hub(\n",
    "        #    commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        #)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc68360-fba2-4136-8cc1-21bd1a3219fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061676fb-a526-4864-8c97-ae8290986e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "from transformers import AdamW # try out with torch.optim.AdamW\n",
    "#from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "#split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets = raw_datasets[\"train\"].shuffle(seed=20).select(range(10000)).train_test_split(train_size=0.95, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def get_dataloaders(accelerator: Accelerator, model, batch_size: int = 32):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    max_input_length = 128\n",
    "    max_target_length = 128\n",
    "\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "        targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "        # 타겟을 위한 토크나이저 셋업\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    # Apply the method we just defined to all the examples in all the splits of the dataset\n",
    "    # starting with the main process first:\n",
    "    with accelerator.main_process_first():\n",
    "        tokenized_datasets = split_datasets.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=split_datasets[\"train\"].column_names,\n",
    "        )\n",
    "\n",
    "    #print(tokenized_datasets)\n",
    "    #DatasetDict({\n",
    "    #    train: Dataset({\n",
    "    #        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "    #        num_rows: 189155\n",
    "    #    })\n",
    "    #    validation: Dataset({\n",
    "    #        features: ['input_ids', 'attention_mask', 'labels'],\n",
    "    #        num_rows: 21018\n",
    "    #    })\n",
    "    #})\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    #tokenized_datasets.set_format(\"torch\")\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation\"], \n",
    "        collate_fn=data_collator, \n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, eval_dataloader, tokenizer\n",
    "\n",
    "\n",
    "#def training_function(config, args):\n",
    "def training_function():\n",
    "\n",
    "    batch_size = 8\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "    train_dataloader, eval_dataloader, tokenizer = get_dataloaders(accelerator, model, batch_size=batch_size)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    num_train_epochs = 3\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "\n",
    "    def postprocess(predictions, labels):\n",
    "        #print(f\"postprocess: {predictions.shape}, {labels.shape}\")\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "        # Replace -100 in the labels as we can't decode them\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "        return decoded_preds, decoded_labels\n",
    "\n",
    "    #metric = load_metric(\"sacrebleu\")\n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        # 학습\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # 평가\n",
    "        model.eval()\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "        #for batch in eval_dataloader:\n",
    "            with torch.no_grad():\n",
    "                generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                    batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=128,\n",
    "                )\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            #print(generated_tokens.shape, labels.shape)\n",
    "            #torch.Size([8, 15]) torch.Size([8, 17])\n",
    "            #torch.Size([8, 13]) torch.Size([8, 12])\n",
    "            #torch.Size([8, 32]) torch.Size([8, 28])\n",
    "            #torch.Size([8, 61]) torch.Size([8, 59])\n",
    "\n",
    "            # 예측과 레이블을 모으기 전에 함께 패딩 수행\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "            #print(f\"after padding: {generated_tokens.shape}, {labels.shape}\")\n",
    "            #after padding: torch.Size([8, 61]), torch.Size([8, 59])\n",
    "            #after padding: torch.Size([8, 61]), torch.Size([8, 59])\n",
    "            #after padding: torch.Size([8, 61]), torch.Size([8, 59])\n",
    "            #after padding: torch.Size([8, 61]), torch.Size([8, 59])\n",
    "            \n",
    "\n",
    "            predictions_gathered = accelerator.gather(generated_tokens)\n",
    "            labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "            #print(f\"after gathered: {predictions_gathered.shape}, {labels_gathered.shape}\")\n",
    "            #after gathered: torch.Size([32, 61]), torch.Size([32, 59])\n",
    "            #after gathered: torch.Size([32, 61]), torch.Size([32, 59])\n",
    "            #after gathered: torch.Size([32, 61]), torch.Size([32, 59])\n",
    "            #after gathered: torch.Size([32, 61]), torch.Size([32, 59])\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "            metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "        results = metric.compute()\n",
    "        #print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "        accelerator.print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "        # 저장 및 업로드\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            #repo.push_to_hub(\n",
    "            #    commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "            #)\n",
    "\n",
    "#from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "#model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "#repo_name = get_full_repo_name(model_name)\n",
    "#repo_name\n",
    "#repo = Repository(output_dir, clone_from=repo_name)\n",
    "#notebook_launcher(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb1fc8-f36d-440c-89da-a773b894a1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
