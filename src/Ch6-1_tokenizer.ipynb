{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278c1aa6-e500-4f41-b155-7b279adf293c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0cf6b7-0de3-4b78-9fe7-600236d75c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9087c89-97f2-46cc-bbd3-c548389fee4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Héllò hôw are ü?\n"
     ]
    }
   ],
   "source": [
    "print(\"Héllò hôw are ü?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b2e00a-88b3-4d5c-9bf9-95c1e6714e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Héllò hôw are ü?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e48cbd-9e06-4e58-bc29-2f249075f245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'H', '##é', '##ll', '##ò', 'h', '##ô', '##w', 'are', 'ü', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\"Héllò hôw are ü?\")\n",
    "print(tokens.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9d24b6-1627-4dc4-ad52-d0add022387e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'hello', 'how', 'are', 'u', '?', '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer(\"Héllò hôw are ü?\").tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4165665-6944-4079-b5ef-00408cd7cbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT Tokenizer\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "badbd1fa-60eb-49d6-a25f-4f62dd475105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'hello', ',', 'how', 'are', 'you', '?', '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, how are  you?\").tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fed8c6f-afc9-42de-89d7-b5a415d189a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT2 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5488a06-df6e-4068-b143-74858e69a177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Hello', ',', 'how', 'are', 'you', '?', '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, how are  you?\").tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c6fd046-d4f7-4338-9c90-6f2b52ffac89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('Ġhow', (6, 10)),\n",
       " ('Ġare', (10, 14)),\n",
       " ('Ġ', (14, 15)),\n",
       " ('Ġyou', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT2 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd29d842-49f8-40b0-9306-67cfffbdad6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'Ġhow', 'Ġare', 'Ġ', 'Ġyou', '?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, how are  you?\").tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "793926ce-330a-4ba2-82c4-ecc618ba8f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁Hello,', (0, 6)),\n",
       " ('▁how', (7, 10)),\n",
       " ('▁are', (11, 14)),\n",
       " ('▁you?', (16, 20))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#T5 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a2878e3-f3e4-4430-ba7a-34bccc039d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Hello', ',', '▁how', '▁are', '▁you', '?', '</s>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, how are  you?\").tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5086db19-b2df-4c11-9614-e5134e1ceb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "b'a\\xea\\xb0\\x80 \\xec\\xa7\\xa7'\n",
      "8\n",
      "a가 짧\n"
     ]
    }
   ],
   "source": [
    "s ='a가 짧'\n",
    "print(len(s))\n",
    "b = s.encode('utf-8')\n",
    "print(b)\n",
    "print(len(b))\n",
    "print(b.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2c57144-d955-41e7-8f32-49a08b988c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "b'\\xe3\\x84\\xb1'\n",
      "3\n",
      "ㄱ\n"
     ]
    }
   ],
   "source": [
    "s ='ㄱ'\n",
    "print(len(s))\n",
    "b = s.encode('utf-8')\n",
    "print(b)\n",
    "print(len(b))\n",
    "print(b.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eed1c5b9-f0e1-456f-abd3-b8d524160a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "b'\\xea\\xb0\\x80'\n",
      "3\n",
      "가\n"
     ]
    }
   ],
   "source": [
    "s ='가'\n",
    "print(len(s))\n",
    "b = s.encode('utf-8')\n",
    "print(b)\n",
    "print(len(b))\n",
    "print(b.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfd197-5014-4532-9f67-4c0b77c8aa6c",
   "metadata": {},
   "source": [
    "#### BPE 토큰화\n",
    "\n",
    "BPE(Byte-Pair Encoding)는 초기에 텍스트를 압축하는 알고리즘으로 개발된 후, GPT 모델을 사전 학습할 때 토큰화를 위해 OpenAI에서 사용되었습니다. GPT, GPT-2, RoBERTa, BART 및 DeBERTa를 포함한 많은 트랜스포머 모델에서 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "577e21bf-fe2d-4467-b363-9274a52edd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4632e696-e345-4245-a6f8-e72765eb3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "997de42f-fdbf-4d4b-a787-51135d4f65af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'Ġcourse': 1, '.': 4, 'Ġchapter': 1, 'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "711d925d-2764-40e2-8254-46b54bf0259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b039a0c5-d856-4519-a62c-96aa2d151600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9920c08a-4765-4d63-b029-53362b6244dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ['T', 'h', 'i', 's'], 'Ġis': ['Ġ', 'i', 's'], 'Ġthe': ['Ġ', 't', 'h', 'e'], 'ĠHugging': ['Ġ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'], 'ĠFace': ['Ġ', 'F', 'a', 'c', 'e'], 'Ġcourse': ['Ġ', 'c', 'o', 'u', 'r', 's', 'e'], '.': ['.'], 'Ġchapter': ['Ġ', 'c', 'h', 'a', 'p', 't', 'e', 'r'], 'Ġabout': ['Ġ', 'a', 'b', 'o', 'u', 't'], 'Ġtokenization': ['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'], 'Ġsection': ['Ġ', 's', 'e', 'c', 't', 'i', 'o', 'n'], 'Ġshows': ['Ġ', 's', 'h', 'o', 'w', 's'], 'Ġseveral': ['Ġ', 's', 'e', 'v', 'e', 'r', 'a', 'l'], 'Ġtokenizer': ['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'], 'Ġalgorithms': ['Ġ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'], 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'], ',': [','], 'Ġyou': ['Ġ', 'y', 'o', 'u'], 'Ġwill': ['Ġ', 'w', 'i', 'l', 'l'], 'Ġbe': ['Ġ', 'b', 'e'], 'Ġable': ['Ġ', 'a', 'b', 'l', 'e'], 'Ġto': ['Ġ', 't', 'o'], 'Ġunderstand': ['Ġ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'], 'Ġhow': ['Ġ', 'h', 'o', 'w'], 'Ġthey': ['Ġ', 't', 'h', 'e', 'y'], 'Ġare': ['Ġ', 'a', 'r', 'e'], 'Ġtrained': ['Ġ', 't', 'r', 'a', 'i', 'n', 'e', 'd'], 'Ġand': ['Ġ', 'a', 'n', 'd'], 'Ġgenerate': ['Ġ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'], 'Ġtokens': ['Ġ', 't', 'o', 'k', 'e', 'n', 's']}\n"
     ]
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04f7ad6d-cee6-4173-b143-1b8bdeb4b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i+1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9a0aae5-77cf-4f64-b30e-b86774fe4533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'): 3\n",
      "('h', 'i'): 3\n",
      "('i', 's'): 5\n",
      "('Ġ', 'i'): 2\n",
      "('Ġ', 't'): 7\n",
      "('t', 'h'): 3\n",
      "('h', 'e'): 2\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i > 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad58ab95-ca3d-49f7-87a2-a773a80ce9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('T', 'h'): 3, ('h', 'i'): 3, ('i', 's'): 5, ('Ġ', 'i'): 2, ('Ġ', 't'): 7, ('t', 'h'): 3, ('h', 'e'): 2, ('Ġ', 'H'): 1, ('H', 'u'): 1, ('u', 'g'): 1, ('g', 'g'): 1, ('g', 'i'): 1, ('i', 'n'): 2, ('n', 'g'): 1, ('Ġ', 'F'): 1, ('F', 'a'): 1, ('a', 'c'): 1, ('c', 'e'): 1, ('Ġ', 'c'): 2, ('c', 'o'): 1, ('o', 'u'): 3, ('u', 'r'): 1, ('r', 's'): 2, ('s', 'e'): 3, ('c', 'h'): 1, ('h', 'a'): 1, ('a', 'p'): 1, ('p', 't'): 1, ('t', 'e'): 2, ('e', 'r'): 5, ('Ġ', 'a'): 5, ('a', 'b'): 2, ('b', 'o'): 1, ('u', 't'): 1, ('t', 'o'): 4, ('o', 'k'): 3, ('k', 'e'): 3, ('e', 'n'): 4, ('n', 'i'): 2, ('i', 'z'): 2, ('z', 'a'): 1, ('a', 't'): 2, ('t', 'i'): 2, ('i', 'o'): 2, ('o', 'n'): 2, ('Ġ', 's'): 3, ('e', 'c'): 1, ('c', 't'): 1, ('s', 'h'): 1, ('h', 'o'): 2, ('o', 'w'): 2, ('w', 's'): 1, ('e', 'v'): 1, ('v', 'e'): 1, ('r', 'a'): 3, ('a', 'l'): 2, ('z', 'e'): 1, ('l', 'g'): 1, ('g', 'o'): 1, ('o', 'r'): 1, ('r', 'i'): 1, ('i', 't'): 1, ('h', 'm'): 1, ('m', 's'): 1, ('H', 'o'): 1, ('o', 'p'): 1, ('p', 'e'): 1, ('e', 'f'): 1, ('f', 'u'): 1, ('u', 'l'): 1, ('l', 'l'): 2, ('l', 'y'): 1, ('Ġ', 'y'): 1, ('y', 'o'): 1, ('Ġ', 'w'): 1, ('w', 'i'): 1, ('i', 'l'): 1, ('Ġ', 'b'): 1, ('b', 'e'): 1, ('b', 'l'): 1, ('l', 'e'): 1, ('Ġ', 'u'): 1, ('u', 'n'): 1, ('n', 'd'): 3, ('d', 'e'): 1, ('s', 't'): 1, ('t', 'a'): 1, ('a', 'n'): 2, ('Ġ', 'h'): 1, ('e', 'y'): 1, ('a', 'r'): 1, ('r', 'e'): 1, ('t', 'r'): 1, ('a', 'i'): 1, ('n', 'e'): 2, ('e', 'd'): 1, ('Ġ', 'g'): 1, ('g', 'e'): 1, ('n', 's'): 1})\n"
     ]
    }
   ],
   "source": [
    "print(pair_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "875bc97c-c559-410f-a2e0-e3ac7f8a6c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ġ', 't') 7\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d128ca65-71c8-4af7-9a79-224c718ee46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {(\"Ġ\", \"t\"): \"Ġt\"}\n",
    "vocab.append(\"Ġt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d3a2852-85a2-47b2-ba1e-bb67ee0d098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aef3f2f5-1b34-4c1a-b3fd-7981a0f28931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Ġ', 't'): 'Ġt'}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f5904d8-c14e-409e-9b19-ba416f3e38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "146a0b37-0eac-43b4-ad29-2e2cf0fd809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ['T', 'h', 'i', 's'], 'Ġis': ['Ġ', 'i', 's'], 'Ġthe': ['Ġt', 'h', 'e'], 'ĠHugging': ['Ġ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'], 'ĠFace': ['Ġ', 'F', 'a', 'c', 'e'], 'Ġcourse': ['Ġ', 'c', 'o', 'u', 'r', 's', 'e'], '.': ['.'], 'Ġchapter': ['Ġ', 'c', 'h', 'a', 'p', 't', 'e', 'r'], 'Ġabout': ['Ġ', 'a', 'b', 'o', 'u', 't'], 'Ġtokenization': ['Ġt', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'], 'Ġsection': ['Ġ', 's', 'e', 'c', 't', 'i', 'o', 'n'], 'Ġshows': ['Ġ', 's', 'h', 'o', 'w', 's'], 'Ġseveral': ['Ġ', 's', 'e', 'v', 'e', 'r', 'a', 'l'], 'Ġtokenizer': ['Ġt', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'], 'Ġalgorithms': ['Ġ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'], 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'], ',': [','], 'Ġyou': ['Ġ', 'y', 'o', 'u'], 'Ġwill': ['Ġ', 'w', 'i', 'l', 'l'], 'Ġbe': ['Ġ', 'b', 'e'], 'Ġable': ['Ġ', 'a', 'b', 'l', 'e'], 'Ġto': ['Ġt', 'o'], 'Ġunderstand': ['Ġ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'], 'Ġhow': ['Ġ', 'h', 'o', 'w'], 'Ġthey': ['Ġt', 'h', 'e', 'y'], 'Ġare': ['Ġ', 'a', 'r', 'e'], 'Ġtrained': ['Ġt', 'r', 'a', 'i', 'n', 'e', 'd'], 'Ġand': ['Ġ', 'a', 'n', 'd'], 'Ġgenerate': ['Ġ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'], 'Ġtokens': ['Ġt', 'o', 'k', 'e', 'n', 's']}\n",
      "['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']\n"
     ]
    }
   ],
   "source": [
    "splits = merge_pair(\"Ġ\", \"t\", splits)\n",
    "print(splits)\n",
    "print(splits[\"Ġtrained\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5127ed07-7e20-45a2-b42e-cbc6cde7cb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2219d6c-2886-428d-a913-7e495432081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7898b48c-c6a6-4602-baaa-e3aba9256de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdc0d335-a20c-4bcd-a742-c39a65f567c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'is', 'er', 'Ġa', 'Ġto', 'en', 'Th', 'This', 'ou', 'se', 'Ġtok', 'Ġtoken', 'nd', 'Ġis', 'Ġth', 'Ġthe', 'in', 'Ġc', 'Ġab', 'Ġtokeni']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47729eed-33ce-4083-a646-df60bc3662bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Ġ', 't'): 'Ġt', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok', ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe', ('i', 'n'): 'in', ('Ġ', 'c'): 'Ġc', ('Ġa', 'b'): 'Ġab', ('Ġtoken', 'i'): 'Ġtokeni'}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6caaccc6-faa8-41c3-9044-54317c08fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70ef407f-1fd3-4b56-9ebe-5f168fb24ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is not a token.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff6b649d-899c-4196-94e3-36ad4b40d97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('Ġis', (4, 7)),\n",
       " ('Ġnot', (7, 11)),\n",
       " ('Ġa', (11, 13)),\n",
       " ('Ġtoken', (13, 19)),\n",
       " ('.', (19, 20))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is not a token.\"\n",
    "pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "pre_tokenize_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24ab9c96-8fec-4fe5-ba72-0a0638911722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'Ġis', 'Ġnot', 'Ġa', 'Ġtoken', '.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "pre_tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f74a582-7335-4b97-9747-7b2590a32802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['T', 'h', 'i', 's'],\n",
       " ['Ġ', 'i', 's'],\n",
       " ['Ġ', 'n', 'o', 't'],\n",
       " ['Ġ', 'a'],\n",
       " ['Ġ', 't', 'o', 'k', 'e', 'n'],\n",
       " ['.']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab11bec4-89b0-4e7e-b7c5-12c3230ee50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "250644bd-3ed0-4aeb-9be0-2f16808a92d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This'], ['Ġis'], ['Ġ', 'n', 'o', 't'], ['Ġa'], ['Ġtoken'], ['.']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f97fc84-9e24-4334-a255-082975d67dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23e5e1dc-ed7a-4e18-bedc-0a50f6c3bdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'is']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Tis\") # T is not in the vacabulary ==> [UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f70ef8bd-313b-49fc-a22d-b1c228f49b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the Hugging Face course.', 'This chapter is about tokenization.', 'This section shows several tokenizer algorithms.', 'Hopefully, you will be able to understand how they are trained and generate tokens.']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fc0904e-c468-456a-8ce1-72b407ebc223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(corpus, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "179f0b3c-fdee-434c-bb0b-33b3e37afa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "709323ec-2cbd-4ab6-bf20-58a5e9cf3e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'U': 53, 'F': 38, 'ö': 179, '¤': 98, '.': 14, 'Þ': 155, '¹': 118, 'Ć': 195, 'J': 42, 'o': 79, 'Ð': 141, 'Ï': 140, 'r': 82, 'ĥ': 226, 'Ô': 145, 'ķ': 244, 'c': 67, 's': 83, 'ę': 214, 'Ĺ': 246, '^': 62, 'f': 70, 'Û': 152, 'í': 170, '7': 23, '|': 92, 'Ò': 143, 'ĕ': 210, 'ă': 192, 'Ã': 128, 'Ú': 151, 'N': 46, '«': 105, '¾': 123, '!': 1, 'P': 48, 'Á': 126, 'Ü': 153, 'µ': 114, 'á': 158, '3': 19, 'ä': 161, 'ğ': 220, 'Ą': 193, 'n': 78, 'k': 75, 'g': 71, '¦': 100, 'Ī': 231, 'Į': 235, 'Ċ': 199, '±': 110, 'ð': 173, '´': 113, 'ĩ': 230, '[': 59, '¼': 121, '<|endoftext|>': 0, '£': 97, '½': 122, 'ā': 190, 'O': 47, 'Õ': 146, 'ø': 181, '\\\\': 60, 'æ': 163, '&': 6, '³': 112, '\"': 2, '×': 148, 'Ń': 256, '4': 20, 'ċ': 200, 'ģ': 224, 'Ê': 135, ']': 61, '-': 13, 'Ĝ': 217, 'x': 88, 'ã': 160, '÷': 180, 'È': 133, 'ô': 177, 'Ē': 207, '6': 22, 'Ĥ': 225, 'ĳ': 240, 'G': 39, ',': 12, 'ě': 216, 'M': 45, '_': 63, 'ò': 175, 'ĸ': 245, 'd': 68, 'E': 37, 'Đ': 205, 'ı': 238, 'ß': 156, '2': 18, ';': 27, '`': 64, '9': 25, 'Ğ': 219, 'Í': 138, '0': 16, 'H': 40, '®': 107, '>': 30, 'ć': 196, 'č': 202, 'Ā': 189, 'S': 51, '¥': 99, 'ĵ': 242, 'L': 44, 'V': 54, 'ú': 183, '{': 91, 'ī': 232, '+': 11, 'Ę': 213, 'v': 86, 'Æ': 131, '°': 109, 'h': 72, 'É': 134, 'Ù': 150, 'Ħ': 227, 'ï': 172, 'ł': 255, '5': 21, 'l': 76, 'Ì': 137, 'º': 119, 'Ľ': 250, 'Ç': 132, 'W': 55, 'K': 43, 't': 84, 'Ö': 147, 'à': 157, 'ù': 182, 'ý': 186, '~': 94, 'ĭ': 234, 'b': 66, 'u': 85, '¿': 124, 'į': 236, 'ĉ': 198, 'Ë': 136, 'w': 87, 'Č': 201, 'p': 80, 'ì': 169, 'Â': 127, '²': 111, '¨': 102, 'Ó': 144, '·': 116, 'Ļ': 248, 'ü': 185, 'ê': 167, 'Ä': 129, 'Ĭ': 233, '#': 3, '1': 17, '©': 103, 'é': 166, '(': 8, 'Ě': 215, ')': 9, 'İ': 237, 'D': 36, 'R': 50, 'û': 184, 'Ĩ': 229, '¯': 108, 'đ': 206, '»': 120, 'i': 73, 'A': 33, 'Î': 139, '8': 24, 'Ý': 154, 'ĝ': 218, '*': 10, '§': 101, '$': 4, 'a': 65, 'Ķ': 243, 'ď': 204, 'Ĵ': 241, 'À': 125, 'm': 77, '¢': 96, 'ē': 208, '¸': 117, 'õ': 178, 'Ģ': 223, ':': 26, 'ç': 164, 'Ł': 254, 'þ': 187, '=': 29, 'T': 52, 'Y': 57, 'ĺ': 247, '¬': 106, 'Ė': 211, 'ħ': 228, 'B': 34, '@': 32, 'Ĉ': 197, 'Ñ': 142, 'ñ': 174, 'ó': 176, '%': 5, 'Q': 49, 'î': 171, 'ė': 212, 'ą': 194, 'z': 90, 'Ø': 149, '<': 28, 'ë': 168, 'Ĕ': 209, 'Ġ': 221, 'q': 81, 'ľ': 251, '?': 31, '¡': 95, 'ÿ': 188, 'I': 41, 'ŀ': 253, 'e': 69, 'C': 35, '}': 93, '¶': 115, 'Ă': 191, 'X': 56, 'å': 162, 'ġ': 222, 'Ď': 203, 'j': 74, 'ª': 104, \"'\": 7, 'Ĳ': 239, 'â': 159, 'Å': 130, 'y': 89, 'è': 165, '/': 15, 'Ŀ': 252, 'ļ': 249, 'Z': 58}\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d7e1a91-234f-4e1a-82aa-d26eea456457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 'Ġ',\n",
       " 'i',\n",
       " 's',\n",
       " 'Ġ',\n",
       " 'n',\n",
       " 'o',\n",
       " 't',\n",
       " 'Ġ',\n",
       " 'a',\n",
       " 'Ġ',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer(\"This is not a token.\").tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea2f51-8df1-4cb3-9945-b5bde493c27b",
   "metadata": {},
   "source": [
    "#### WordPiece 토큰화\n",
    "\n",
    "WordPiece는 Google이 BERT를 사전 학습하기 위해 개발한 토큰화 알고리즘입니다. 그 이후로 DitilBERT, MobileBERT, Funnel Transformers 및 MPNET과 같은 BERT 기반의 상당히 많은 Transformer 모델에서 재사용되었습니다. 학습 측면에서 BPE와 매우 유사하지만 실제 토큰화는 다르게 수행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41be99ab-1018-4fc7-b193-676a0c1bc9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ebd8d04e-0b51-40ec-8087-4a4655839d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16e8d037-301e-4e9f-b799-da5f03f9aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2009238b-a1ee-40c1-b9da-ea7c85be5c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', 'This', 'is', 'the', 'Hu', '##gging', 'Face', 'course', '.', '[SEP]'], ['[CLS]', 'This', 'chapter', 'is', 'about', 'token', '##ization', '.', '[SEP]'], ['[CLS]', 'This', 'section', 'shows', 'several', 'token', '##izer', 'algorithms', '.', '[SEP]'], ['[CLS]', 'Hopefully', ',', 'you', 'will', 'be', 'able', 'to', 'understand', 'how', 'they', 'are', 'trained', 'and', 'generate', 'token', '##s', '.', '[SEP]']]\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer(corpus).tokens(i) for i in range(len(corpus))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c3027b6-c334-46e3-be19-45593ad6435d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'This': 3,\n",
       "             'is': 2,\n",
       "             'the': 1,\n",
       "             'Hugging': 1,\n",
       "             'Face': 1,\n",
       "             'course': 1,\n",
       "             '.': 4,\n",
       "             'chapter': 1,\n",
       "             'about': 1,\n",
       "             'tokenization': 1,\n",
       "             'section': 1,\n",
       "             'shows': 1,\n",
       "             'several': 1,\n",
       "             'tokenizer': 1,\n",
       "             'algorithms': 1,\n",
       "             'Hopefully': 1,\n",
       "             ',': 1,\n",
       "             'you': 1,\n",
       "             'will': 1,\n",
       "             'be': 1,\n",
       "             'able': 1,\n",
       "             'to': 1,\n",
       "             'understand': 1,\n",
       "             'how': 1,\n",
       "             'they': 1,\n",
       "             'are': 1,\n",
       "             'trained': 1,\n",
       "             'and': 1,\n",
       "             'generate': 1,\n",
       "             'tokens': 1})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "248ed766-a5a4-4bc9-8765-8bd3539fc4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('This', (0, 4)),\n",
       "  ('is', (5, 7)),\n",
       "  ('the', (8, 11)),\n",
       "  ('Hugging', (12, 19)),\n",
       "  ('Face', (20, 24)),\n",
       "  ('course', (25, 31)),\n",
       "  ('.', (31, 32))],\n",
       " [('This', (0, 4)),\n",
       "  ('chapter', (5, 12)),\n",
       "  ('is', (13, 15)),\n",
       "  ('about', (16, 21)),\n",
       "  ('tokenization', (22, 34)),\n",
       "  ('.', (34, 35))],\n",
       " [('This', (0, 4)),\n",
       "  ('section', (5, 12)),\n",
       "  ('shows', (13, 18)),\n",
       "  ('several', (19, 26)),\n",
       "  ('tokenizer', (27, 36)),\n",
       "  ('algorithms', (37, 47)),\n",
       "  ('.', (47, 48))],\n",
       " [('Hopefully', (0, 9)),\n",
       "  (',', (9, 10)),\n",
       "  ('you', (11, 14)),\n",
       "  ('will', (15, 19)),\n",
       "  ('be', (20, 22)),\n",
       "  ('able', (23, 27)),\n",
       "  ('to', (28, 30)),\n",
       "  ('understand', (31, 41)),\n",
       "  ('how', (42, 45)),\n",
       "  ('they', (46, 50)),\n",
       "  ('are', (51, 54)),\n",
       "  ('trained', (55, 62)),\n",
       "  ('and', (63, 66)),\n",
       "  ('generate', (67, 75)),\n",
       "  ('tokens', (76, 82)),\n",
       "  ('.', (82, 83))]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1e29953-1789-4aee-bec2-f1c342a9d9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n",
    "\n",
    "print(alphabet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76f9080e-47c4-4c2f-80e4-f70a0495f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "877dda22-d591-498c-b35a-d9a712da7f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f396eaf6-c35c-43cf-91d4-11aea4d322f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ['T', '##h', '##i', '##s'], 'is': ['i', '##s'], 'the': ['t', '##h', '##e'], 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'], 'Face': ['F', '##a', '##c', '##e'], 'course': ['c', '##o', '##u', '##r', '##s', '##e'], '.': ['.'], 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'], 'about': ['a', '##b', '##o', '##u', '##t'], 'tokenization': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##t', '##i', '##o', '##n'], 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'], 'shows': ['s', '##h', '##o', '##w', '##s'], 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'], 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'], 'algorithms': ['a', '##l', '##g', '##o', '##r', '##i', '##t', '##h', '##m', '##s'], 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'], ',': [','], 'you': ['y', '##o', '##u'], 'will': ['w', '##i', '##l', '##l'], 'be': ['b', '##e'], 'able': ['a', '##b', '##l', '##e'], 'to': ['t', '##o'], 'understand': ['u', '##n', '##d', '##e', '##r', '##s', '##t', '##a', '##n', '##d'], 'how': ['h', '##o', '##w'], 'they': ['t', '##h', '##e', '##y'], 'are': ['a', '##r', '##e'], 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'], 'and': ['a', '##n', '##d'], 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'], 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}\n"
     ]
    }
   ],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6ff2784-14d8-464a-9462-6c944137a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29859143-4505-4e8e-a3bd-f99aee4d0b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', '##h'): 0.125\n",
      "('##h', '##i'): 0.03409090909090909\n",
      "('##i', '##s'): 0.02727272727272727\n",
      "('i', '##s'): 0.1\n",
      "('t', '##h'): 0.03571428571428571\n",
      "('##h', '##e'): 0.011904761904761904\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "634b1d19-122f-4089-a87d-0e3d143ea773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('T', '##h'): 0.125, ('##h', '##i'): 0.03409090909090909, ('##i', '##s'): 0.02727272727272727, ('i', '##s'): 0.1, ('t', '##h'): 0.03571428571428571, ('##h', '##e'): 0.011904761904761904, ('H', '##u'): 0.1, ('##u', '##g'): 0.05, ('##g', '##g'): 0.0625, ('##g', '##i'): 0.022727272727272728, ('##i', '##n'): 0.01652892561983471, ('##n', '##g'): 0.022727272727272728, ('F', '##a'): 0.14285714285714285, ('##a', '##c'): 0.07142857142857142, ('##c', '##e'): 0.023809523809523808, ('c', '##o'): 0.038461538461538464, ('##o', '##u'): 0.046153846153846156, ('##u', '##r'): 0.022222222222222223, ('##r', '##s'): 0.022222222222222223, ('##s', '##e'): 0.004761904761904762, ('c', '##h'): 0.0625, ('##h', '##a'): 0.017857142857142856, ('##a', '##p'): 0.07142857142857142, ('##p', '##t'): 0.07142857142857142, ('##t', '##e'): 0.013605442176870748, ('##e', '##r'): 0.026455026455026454, ('a', '##b'): 0.2, ('##b', '##o'): 0.038461538461538464, ('##u', '##t'): 0.02857142857142857, ('t', '##o'): 0.04395604395604396, ('##o', '##k'): 0.07692307692307693, ('##k', '##e'): 0.047619047619047616, ('##e', '##n'): 0.017316017316017316, ('##n', '##i'): 0.01652892561983471, ('##i', '##z'): 0.09090909090909091, ('##z', '##a'): 0.07142857142857142, ('##a', '##t'): 0.04081632653061224, ('##t', '##i'): 0.025974025974025976, ('##i', '##o'): 0.013986013986013986, ('##o', '##n'): 0.013986013986013986, ('s', '##e'): 0.031746031746031744, ('##e', '##c'): 0.023809523809523808, ('##c', '##t'): 0.07142857142857142, ('s', '##h'): 0.041666666666666664, ('##h', '##o'): 0.009615384615384616, ('##o', '##w'): 0.07692307692307693, ('##w', '##s'): 0.05, ('##e', '##v'): 0.047619047619047616, ('##v', '##e'): 0.047619047619047616, ('##r', '##a'): 0.047619047619047616, ('##a', '##l'): 0.02040816326530612, ('##z', '##e'): 0.023809523809523808, ('a', '##l'): 0.02857142857142857, ('##l', '##g'): 0.03571428571428571, ('##g', '##o'): 0.019230769230769232, ('##o', '##r'): 0.008547008547008548, ('##r', '##i'): 0.010101010101010102, ('##i', '##t'): 0.012987012987012988, ('##t', '##h'): 0.017857142857142856, ('##h', '##m'): 0.125, ('##m', '##s'): 0.1, ('H', '##o'): 0.038461538461538464, ('##o', '##p'): 0.038461538461538464, ('##p', '##e'): 0.023809523809523808, ('##e', '##f'): 0.047619047619047616, ('##f', '##u'): 0.2, ('##u', '##l'): 0.02857142857142857, ('##l', '##l'): 0.04081632653061224, ('##l', '##y'): 0.07142857142857142, ('y', '##o'): 0.07692307692307693, ('w', '##i'): 0.09090909090909091, ('##i', '##l'): 0.012987012987012988, ('b', '##e'): 0.047619047619047616, ('##b', '##l'): 0.07142857142857142, ('##l', '##e'): 0.006802721088435374, ('u', '##n'): 0.09090909090909091, ('##n', '##d'): 0.06818181818181818, ('##d', '##e'): 0.011904761904761904, ('##s', '##t'): 0.014285714285714285, ('##t', '##a'): 0.02040816326530612, ('##a', '##n'): 0.012987012987012988, ('h', '##o'): 0.07692307692307693, ('##e', '##y'): 0.023809523809523808, ('a', '##r'): 0.022222222222222223, ('##r', '##e'): 0.005291005291005291, ('t', '##r'): 0.015873015873015872, ('##a', '##i'): 0.012987012987012988, ('##n', '##e'): 0.008658008658008658, ('##e', '##d'): 0.011904761904761904, ('a', '##n'): 0.01818181818181818, ('g', '##e'): 0.047619047619047616, ('##n', '##s'): 0.00909090909090909}\n"
     ]
    }
   ],
   "source": [
    "print(pair_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1b0cf30-b533-4b07-9c87-e9b3fe61a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '##b') 0.2\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "262fdd15-0ab1-46fa-beaf-681669aac635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab']\n"
     ]
    }
   ],
   "source": [
    "vocab.append(\"ab\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc30a1c5-2254-4120-83da-a8a46d6e15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dc9ffd9f-eace-4a7d-b877-a6034908eeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ['T', '##h', '##i', '##s'], 'is': ['i', '##s'], 'the': ['t', '##h', '##e'], 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'], 'Face': ['F', '##a', '##c', '##e'], 'course': ['c', '##o', '##u', '##r', '##s', '##e'], '.': ['.'], 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'], 'about': ['ab', '##o', '##u', '##t'], 'tokenization': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##t', '##i', '##o', '##n'], 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'], 'shows': ['s', '##h', '##o', '##w', '##s'], 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'], 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'], 'algorithms': ['a', '##l', '##g', '##o', '##r', '##i', '##t', '##h', '##m', '##s'], 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'], ',': [','], 'you': ['y', '##o', '##u'], 'will': ['w', '##i', '##l', '##l'], 'be': ['b', '##e'], 'able': ['ab', '##l', '##e'], 'to': ['t', '##o'], 'understand': ['u', '##n', '##d', '##e', '##r', '##s', '##t', '##a', '##n', '##d'], 'how': ['h', '##o', '##w'], 'they': ['t', '##h', '##e', '##y'], 'are': ['a', '##r', '##e'], 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'], 'and': ['a', '##n', '##d'], 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'], 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}\n",
      "['ab', '##o', '##u', '##t']\n"
     ]
    }
   ],
   "source": [
    "splits = merge_pair(\"a\", \"##b\", splits)\n",
    "print(splits)\n",
    "print(splits[\"about\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81f450a6-8d8c-47c6-b519-034813a0b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e36818ba-5fba-4253-8c99-89f13d8deef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', '##hm', '##thm', 'Hu', 'Hug', 'Hugg', 'ch', 'cha', 'chap', 'chapt', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut', '##ta']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30427c00-9327-4068-883a-5c4f20619fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ['Th', '##i', '##s'], 'is': ['is'], 'the': ['th', '##e'], 'Hugging': ['Hugg', '##i', '##n', '##g'], 'Face': ['Fac', '##e'], 'course': ['c', '##o', '##u', '##r', '##s', '##e'], '.': ['.'], 'chapter': ['chapt', '##e', '##r'], 'about': ['ab', '##o', '##ut'], 'tokenization': ['t', '##o', '##k', '##e', '##n', '##i', '##zat', '##i', '##o', '##n'], 'section': ['s', '##e', '##ct', '##i', '##o', '##n'], 'shows': ['sh', '##o', '##w', '##s'], 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'], 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'], 'algorithms': ['a', '##l', '##g', '##o', '##r', '##i', '##thms'], 'Hopefully': ['H', '##o', '##p', '##e', '##fully'], ',': [','], 'you': ['y', '##o', '##u'], 'will': ['w', '##i', '##l', '##l'], 'be': ['b', '##e'], 'able': ['ab', '##l', '##e'], 'to': ['t', '##o'], 'understand': ['u', '##n', '##d', '##e', '##r', '##s', '##ta', '##n', '##d'], 'how': ['h', '##o', '##w'], 'they': ['th', '##e', '##y'], 'are': ['a', '##r', '##e'], 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'], 'and': ['a', '##n', '##d'], 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'], 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}\n"
     ]
    }
   ],
   "source": [
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5734d63b-4b75-4fde-86de-c7351572249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "92337189-1baf-4612-bc4f-74df4f52d373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hugg', '##i', '##n', '##g']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hugging\"))\n",
    "print(encode_word(\"HOgging\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1a1a635b-f599-4643-88ae-55441e43da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "566f88c9-df71-41e2-8a44-44e6b1d17cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s', '##e', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"This is the Hugging Face course!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b1165-bf5c-4400-9b79-9a10fc0eded2",
   "metadata": {},
   "source": [
    "#### Unigram 토큰화\n",
    "\n",
    "Unigram 알고리즘은 AlBERT, T5, mBART, Big Bird 및 XLNet과 같은 모델에서 사용되는 토큰화 알고리즘인 SentencePiece에서 자주 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e4ad67b3-26b5-468b-a859-d35e94f3889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "435991f6-93ab-496c-9625-8c9634ab8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dfa59d54-e6bc-4536-b8a8-ca2f5f54dd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'▁This': 3,\n",
       "             '▁is': 2,\n",
       "             '▁the': 1,\n",
       "             '▁Hugging': 1,\n",
       "             '▁Face': 1,\n",
       "             '▁course.': 1,\n",
       "             '▁chapter': 1,\n",
       "             '▁about': 1,\n",
       "             '▁tokenization.': 1,\n",
       "             '▁section': 1,\n",
       "             '▁shows': 1,\n",
       "             '▁several': 1,\n",
       "             '▁tokenizer': 1,\n",
       "             '▁algorithms.': 1,\n",
       "             '▁Hopefully,': 1,\n",
       "             '▁you': 1,\n",
       "             '▁will': 1,\n",
       "             '▁be': 1,\n",
       "             '▁able': 1,\n",
       "             '▁to': 1,\n",
       "             '▁understand': 1,\n",
       "             '▁how': 1,\n",
       "             '▁they': 1,\n",
       "             '▁are': 1,\n",
       "             '▁trained': 1,\n",
       "             '▁and': 1,\n",
       "             '▁generate': 1,\n",
       "             '▁tokens.': 1})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8e53f077-425c-445d-8fbc-36e146ffa735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁t', 7),\n",
       " ('is', 5),\n",
       " ('er', 5),\n",
       " ('▁a', 5),\n",
       " ('▁to', 4),\n",
       " ('to', 4),\n",
       " ('en', 4),\n",
       " ('▁T', 3),\n",
       " ('▁Th', 3),\n",
       " ('▁Thi', 3)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        # 길이가 적어도 2 이상인 subword들을 추가함.\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq\n",
    "\n",
    "# Subword들을 빈도 역순으로 정렬\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_subwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "025b2589-cb32-457b-9868-b020de965bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁t', 7), ('is', 5), ('er', 5), ('▁a', 5), ('▁to', 4), ('to', 4), ('en', 4), ('▁T', 3), ('▁Th', 3), ('▁Thi', 3), ('▁This', 3), ('Th', 3), ('Thi', 3), ('This', 3), ('hi', 3), ('his', 3), ('th', 3), ('ou', 3), ('se', 3), ('▁tok', 3), ('▁toke', 3), ('▁token', 3), ('tok', 3), ('toke', 3), ('token', 3), ('ok', 3), ('oke', 3), ('oken', 3), ('ke', 3), ('ken', 3), ('▁s', 3), ('ra', 3), ('nd', 3), ('▁i', 2), ('▁is', 2), ('▁th', 2), ('▁the', 2), ('the', 2), ('he', 2), ('▁H', 2), ('in', 2), ('▁c', 2), ('rs', 2), ('te', 2), ('▁ab', 2), ('ab', 2), ('▁tokeni', 2), ('▁tokeniz', 2), ('tokeni', 2), ('tokeniz', 2), ('okeni', 2), ('okeniz', 2), ('keni', 2), ('keniz', 2), ('eni', 2), ('eniz', 2), ('ni', 2), ('niz', 2), ('iz', 2), ('at', 2), ('ti', 2), ('tio', 2), ('tion', 2), ('io', 2), ('ion', 2), ('on', 2), ('▁se', 2), ('ho', 2), ('how', 2), ('ow', 2), ('era', 2), ('al', 2), ('s.', 2), ('ll', 2), ('an', 2), ('and', 2), ('ne', 2), ('▁Hu', 1), ('▁Hug', 1), ('▁Hugg', 1), ('▁Huggi', 1), ('▁Huggin', 1), ('▁Hugging', 1), ('Hu', 1), ('Hug', 1), ('Hugg', 1), ('Huggi', 1), ('Huggin', 1), ('Hugging', 1), ('ug', 1), ('ugg', 1), ('uggi', 1), ('uggin', 1), ('ugging', 1), ('gg', 1), ('ggi', 1), ('ggin', 1), ('gging', 1), ('gi', 1), ('gin', 1), ('ging', 1), ('ing', 1), ('ng', 1), ('▁F', 1), ('▁Fa', 1), ('▁Fac', 1), ('▁Face', 1), ('Fa', 1), ('Fac', 1), ('Face', 1), ('ac', 1), ('ace', 1), ('ce', 1), ('▁co', 1), ('▁cou', 1), ('▁cour', 1), ('▁cours', 1), ('▁course', 1), ('▁course.', 1), ('co', 1), ('cou', 1), ('cour', 1), ('cours', 1), ('course', 1), ('course.', 1), ('our', 1), ('ours', 1), ('ourse', 1), ('ourse.', 1), ('ur', 1), ('urs', 1), ('urse', 1), ('urse.', 1), ('rse', 1), ('rse.', 1), ('se.', 1), ('e.', 1), ('▁ch', 1), ('▁cha', 1), ('▁chap', 1), ('▁chapt', 1), ('▁chapte', 1), ('▁chapter', 1), ('ch', 1), ('cha', 1), ('chap', 1), ('chapt', 1), ('chapte', 1), ('chapter', 1), ('ha', 1), ('hap', 1), ('hapt', 1), ('hapte', 1), ('hapter', 1), ('ap', 1), ('apt', 1), ('apte', 1), ('apter', 1), ('pt', 1), ('pte', 1), ('pter', 1), ('ter', 1), ('▁abo', 1), ('▁abou', 1), ('▁about', 1), ('abo', 1), ('abou', 1), ('about', 1), ('bo', 1), ('bou', 1), ('bout', 1), ('out', 1), ('ut', 1), ('▁tokeniza', 1), ('▁tokenizat', 1), ('▁tokenizati', 1), ('▁tokenizatio', 1), ('▁tokenization', 1), ('▁tokenization.', 1), ('tokeniza', 1), ('tokenizat', 1), ('tokenizati', 1), ('tokenizatio', 1), ('tokenization', 1), ('tokenization.', 1), ('okeniza', 1), ('okenizat', 1), ('okenizati', 1), ('okenizatio', 1), ('okenization', 1), ('okenization.', 1), ('keniza', 1), ('kenizat', 1), ('kenizati', 1), ('kenizatio', 1), ('kenization', 1), ('kenization.', 1), ('eniza', 1), ('enizat', 1), ('enizati', 1), ('enizatio', 1), ('enization', 1), ('enization.', 1), ('niza', 1), ('nizat', 1), ('nizati', 1), ('nizatio', 1), ('nization', 1), ('nization.', 1), ('iza', 1), ('izat', 1), ('izati', 1), ('izatio', 1), ('ization', 1), ('ization.', 1), ('za', 1), ('zat', 1), ('zati', 1), ('zatio', 1), ('zation', 1), ('zation.', 1), ('ati', 1), ('atio', 1), ('ation', 1), ('ation.', 1), ('tion.', 1), ('ion.', 1), ('on.', 1), ('n.', 1), ('▁sec', 1), ('▁sect', 1), ('▁secti', 1), ('▁sectio', 1), ('▁section', 1), ('sec', 1), ('sect', 1), ('secti', 1), ('sectio', 1), ('section', 1), ('ec', 1), ('ect', 1), ('ecti', 1), ('ectio', 1), ('ection', 1), ('ct', 1), ('cti', 1), ('ctio', 1), ('ction', 1), ('▁sh', 1), ('▁sho', 1), ('▁show', 1), ('▁shows', 1), ('sh', 1), ('sho', 1), ('show', 1), ('shows', 1), ('hows', 1), ('ows', 1), ('ws', 1), ('▁sev', 1), ('▁seve', 1), ('▁sever', 1), ('▁severa', 1), ('▁several', 1), ('sev', 1), ('seve', 1), ('sever', 1), ('severa', 1), ('several', 1), ('ev', 1), ('eve', 1), ('ever', 1), ('evera', 1), ('everal', 1), ('ve', 1), ('ver', 1), ('vera', 1), ('veral', 1), ('eral', 1), ('ral', 1), ('▁tokenize', 1), ('▁tokenizer', 1), ('tokenize', 1), ('tokenizer', 1), ('okenize', 1), ('okenizer', 1), ('kenize', 1), ('kenizer', 1), ('enize', 1), ('enizer', 1), ('nize', 1), ('nizer', 1), ('ize', 1), ('izer', 1), ('ze', 1), ('zer', 1), ('▁al', 1), ('▁alg', 1), ('▁algo', 1), ('▁algor', 1), ('▁algori', 1), ('▁algorit', 1), ('▁algorith', 1), ('▁algorithm', 1), ('▁algorithms', 1), ('▁algorithms.', 1), ('alg', 1), ('algo', 1), ('algor', 1), ('algori', 1), ('algorit', 1), ('algorith', 1), ('algorithm', 1), ('algorithms', 1), ('algorithms.', 1), ('lg', 1), ('lgo', 1), ('lgor', 1), ('lgori', 1), ('lgorit', 1), ('lgorith', 1), ('lgorithm', 1), ('lgorithms', 1), ('lgorithms.', 1), ('go', 1), ('gor', 1), ('gori', 1), ('gorit', 1), ('gorith', 1), ('gorithm', 1), ('gorithms', 1), ('gorithms.', 1), ('or', 1), ('ori', 1), ('orit', 1), ('orith', 1), ('orithm', 1), ('orithms', 1), ('orithms.', 1), ('ri', 1), ('rit', 1), ('rith', 1), ('rithm', 1), ('rithms', 1), ('rithms.', 1), ('it', 1), ('ith', 1), ('ithm', 1), ('ithms', 1), ('ithms.', 1), ('thm', 1), ('thms', 1), ('thms.', 1), ('hm', 1), ('hms', 1), ('hms.', 1), ('ms', 1), ('ms.', 1), ('▁Ho', 1), ('▁Hop', 1), ('▁Hope', 1), ('▁Hopef', 1), ('▁Hopefu', 1), ('▁Hopeful', 1), ('▁Hopefull', 1), ('▁Hopefully', 1), ('▁Hopefully,', 1), ('Ho', 1), ('Hop', 1), ('Hope', 1), ('Hopef', 1), ('Hopefu', 1), ('Hopeful', 1), ('Hopefull', 1), ('Hopefully', 1), ('Hopefully,', 1), ('op', 1), ('ope', 1), ('opef', 1), ('opefu', 1), ('opeful', 1), ('opefull', 1), ('opefully', 1), ('opefully,', 1), ('pe', 1), ('pef', 1), ('pefu', 1), ('peful', 1), ('pefull', 1), ('pefully', 1), ('pefully,', 1), ('ef', 1), ('efu', 1), ('eful', 1), ('efull', 1), ('efully', 1), ('efully,', 1), ('fu', 1), ('ful', 1), ('full', 1), ('fully', 1), ('fully,', 1), ('ul', 1), ('ull', 1), ('ully', 1), ('ully,', 1), ('lly', 1), ('lly,', 1), ('ly', 1), ('ly,', 1), ('y,', 1), ('▁y', 1), ('▁yo', 1), ('▁you', 1), ('yo', 1), ('you', 1), ('▁w', 1), ('▁wi', 1), ('▁wil', 1), ('▁will', 1), ('wi', 1), ('wil', 1), ('will', 1), ('il', 1), ('ill', 1), ('▁b', 1), ('▁be', 1), ('be', 1), ('▁abl', 1), ('▁able', 1), ('abl', 1), ('able', 1), ('bl', 1), ('ble', 1), ('le', 1), ('▁u', 1), ('▁un', 1), ('▁und', 1), ('▁unde', 1), ('▁under', 1), ('▁unders', 1), ('▁underst', 1), ('▁understa', 1), ('▁understan', 1), ('▁understand', 1), ('un', 1), ('und', 1), ('unde', 1), ('under', 1), ('unders', 1), ('underst', 1), ('understa', 1), ('understan', 1), ('understand', 1), ('nde', 1), ('nder', 1), ('nders', 1), ('nderst', 1), ('ndersta', 1), ('nderstan', 1), ('nderstand', 1), ('de', 1), ('der', 1), ('ders', 1), ('derst', 1), ('dersta', 1), ('derstan', 1), ('derstand', 1), ('ers', 1), ('erst', 1), ('ersta', 1), ('erstan', 1), ('erstand', 1), ('rst', 1), ('rsta', 1), ('rstan', 1), ('rstand', 1), ('st', 1), ('sta', 1), ('stan', 1), ('stand', 1), ('ta', 1), ('tan', 1), ('tand', 1), ('▁h', 1), ('▁ho', 1), ('▁how', 1), ('▁they', 1), ('they', 1), ('hey', 1), ('ey', 1), ('▁ar', 1), ('▁are', 1), ('ar', 1), ('are', 1), ('re', 1), ('▁tr', 1), ('▁tra', 1), ('▁trai', 1), ('▁train', 1), ('▁traine', 1), ('▁trained', 1), ('tr', 1), ('tra', 1), ('trai', 1), ('train', 1), ('traine', 1), ('trained', 1), ('rai', 1), ('rain', 1), ('raine', 1), ('rained', 1), ('ai', 1), ('ain', 1), ('aine', 1), ('ained', 1), ('ine', 1), ('ined', 1), ('ned', 1), ('ed', 1), ('▁an', 1), ('▁and', 1), ('▁g', 1), ('▁ge', 1), ('▁gen', 1), ('▁gene', 1), ('▁gener', 1), ('▁genera', 1), ('▁generat', 1), ('▁generate', 1), ('ge', 1), ('gen', 1), ('gene', 1), ('gener', 1), ('genera', 1), ('generat', 1), ('generate', 1), ('ene', 1), ('ener', 1), ('enera', 1), ('enerat', 1), ('enerate', 1), ('ner', 1), ('nera', 1), ('nerat', 1), ('nerate', 1), ('erat', 1), ('erate', 1), ('rat', 1), ('rate', 1), ('ate', 1), ('▁tokens', 1), ('▁tokens.', 1), ('tokens', 1), ('tokens.', 1), ('okens', 1), ('okens.', 1), ('kens', 1), ('kens.', 1), ('ens', 1), ('ens.', 1), ('ns', 1), ('ns.', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f67fd496-5e94-4faf-ac51-e42f43bf440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token, freq in token_freqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aba48e27-f977-439e-a69a-328fc0f0edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'▁': 31, 'T': 3, 'h': 9, 'i': 13, 's': 13, 't': 14, 'e': 21, 'H': 2, 'u': 6, 'g': 5, 'n': 11, 'F': 1, 'a': 12, 'c': 4, 'o': 13, 'r': 9, '.': 4, 'p': 2, 'b': 3, 'k': 3, 'z': 2, 'w': 3, 'v': 1, 'l': 7, 'm': 1, 'f': 1, 'y': 3, ',': 1, 'd': 4, '▁t': 7, 'is': 5, 'er': 5, '▁a': 5, '▁to': 4, 'to': 4, 'en': 4, '▁T': 3, '▁Th': 3, '▁Thi': 3, '▁This': 3, 'Th': 3, 'Thi': 3, 'This': 3, 'hi': 3, 'his': 3, 'th': 3, 'ou': 3, 'se': 3, '▁tok': 3, '▁toke': 3, '▁token': 3, 'tok': 3, 'toke': 3, 'token': 3, 'ok': 3, 'oke': 3, 'oken': 3, 'ke': 3, 'ken': 3, '▁s': 3, 'ra': 3, 'nd': 3, '▁i': 2, '▁is': 2, '▁th': 2, '▁the': 2, 'the': 2, 'he': 2, '▁H': 2, 'in': 2, '▁c': 2, 'rs': 2, 'te': 2, '▁ab': 2, 'ab': 2, '▁tokeni': 2, '▁tokeniz': 2, 'tokeni': 2, 'tokeniz': 2, 'okeni': 2, 'okeniz': 2, 'keni': 2, 'keniz': 2, 'eni': 2, 'eniz': 2, 'ni': 2, 'niz': 2, 'iz': 2, 'at': 2, 'ti': 2, 'tio': 2, 'tion': 2, 'io': 2, 'ion': 2, 'on': 2, '▁se': 2, 'ho': 2, 'how': 2, 'ow': 2, 'era': 2, 'al': 2, 's.': 2, 'll': 2, 'an': 2, 'and': 2, 'ne': 2, '▁Hu': 1, '▁Hug': 1, '▁Hugg': 1, '▁Huggi': 1, '▁Huggin': 1, '▁Hugging': 1, 'Hu': 1, 'Hug': 1, 'Hugg': 1, 'Huggi': 1, 'Huggin': 1, 'Hugging': 1, 'ug': 1, 'ugg': 1, 'uggi': 1, 'uggin': 1, 'ugging': 1, 'gg': 1, 'ggi': 1, 'ggin': 1, 'gging': 1, 'gi': 1, 'gin': 1, 'ging': 1, 'ing': 1, 'ng': 1, '▁F': 1, '▁Fa': 1, '▁Fac': 1, '▁Face': 1, 'Fa': 1, 'Fac': 1, 'Face': 1, 'ac': 1, 'ace': 1, 'ce': 1, '▁co': 1, '▁cou': 1, '▁cour': 1, '▁cours': 1, '▁course': 1, '▁course.': 1, 'co': 1, 'cou': 1, 'cour': 1, 'cours': 1, 'course': 1, 'course.': 1, 'our': 1, 'ours': 1, 'ourse': 1, 'ourse.': 1, 'ur': 1, 'urs': 1, 'urse': 1, 'urse.': 1, 'rse': 1, 'rse.': 1, 'se.': 1, 'e.': 1, '▁ch': 1, '▁cha': 1, '▁chap': 1, '▁chapt': 1, '▁chapte': 1, '▁chapter': 1, 'ch': 1, 'cha': 1, 'chap': 1, 'chapt': 1, 'chapte': 1, 'chapter': 1, 'ha': 1, 'hap': 1, 'hapt': 1, 'hapte': 1, 'hapter': 1, 'ap': 1, 'apt': 1, 'apte': 1, 'apter': 1, 'pt': 1, 'pte': 1, 'pter': 1, 'ter': 1, '▁abo': 1, '▁abou': 1, '▁about': 1, 'abo': 1, 'abou': 1, 'about': 1, 'bo': 1, 'bou': 1, 'bout': 1, 'out': 1, 'ut': 1, '▁tokeniza': 1, '▁tokenizat': 1, '▁tokenizati': 1, '▁tokenizatio': 1, '▁tokenization': 1, '▁tokenization.': 1, 'tokeniza': 1, 'tokenizat': 1, 'tokenizati': 1, 'tokenizatio': 1, 'tokenization': 1, 'tokenization.': 1, 'okeniza': 1, 'okenizat': 1, 'okenizati': 1, 'okenizatio': 1, 'okenization': 1, 'okenization.': 1, 'keniza': 1, 'kenizat': 1, 'kenizati': 1, 'kenizatio': 1, 'kenization': 1, 'kenization.': 1, 'eniza': 1, 'enizat': 1, 'enizati': 1, 'enizatio': 1, 'enization': 1, 'enization.': 1, 'niza': 1, 'nizat': 1, 'nizati': 1, 'nizatio': 1, 'nization': 1, 'nization.': 1, 'iza': 1, 'izat': 1, 'izati': 1, 'izatio': 1, 'ization': 1, 'ization.': 1, 'za': 1, 'zat': 1, 'zati': 1, 'zatio': 1, 'zation': 1, 'zation.': 1, 'ati': 1, 'atio': 1, 'ation': 1, 'ation.': 1, 'tion.': 1, 'ion.': 1, 'on.': 1, 'n.': 1, '▁sec': 1, '▁sect': 1, '▁secti': 1, '▁sectio': 1, '▁section': 1, 'sec': 1, 'sect': 1, 'secti': 1, 'sectio': 1, 'section': 1, 'ec': 1, 'ect': 1, 'ecti': 1, 'ectio': 1, 'ection': 1, 'ct': 1, 'cti': 1, 'ctio': 1, 'ction': 1, '▁sh': 1, '▁sho': 1, '▁show': 1, '▁shows': 1, 'sh': 1, 'sho': 1, 'show': 1, 'shows': 1, 'hows': 1, 'ows': 1, 'ws': 1, '▁sev': 1, '▁seve': 1, '▁sever': 1, '▁severa': 1, '▁several': 1, 'sev': 1, 'seve': 1, 'sever': 1, 'severa': 1, 'several': 1, 'ev': 1, 'eve': 1}\n"
     ]
    }
   ],
   "source": [
    "print(token_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eb4c621b-9a76-4c6f-9333-e149f7916319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "21298f07-2744-4738-a8c3-cf2cda592422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d53ec5ae-fde5-45a4-b40d-fda01ca391c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'▁': 2.9562534625802033, 'T': 5.29162837839724, 'h': 4.19301608972913, 'i': 3.8252913096038133, 's': 3.8252913096038133, 't': 3.751183337450091, 'e': 3.345718229341927, 'H': 5.697093486505405, 'u': 4.598481197837295, 'g': 4.7808027546312495, 'n': 3.9923453942669793, 'F': 6.39024066706535, 'a': 3.9053340172773496, 'c': 5.003946305945459, 'o': 3.8252913096038133, 'r': 4.19301608972913, '.': 5.003946305945459, 'p': 5.697093486505405, 'b': 5.29162837839724, 'k': 5.29162837839724, 'z': 5.697093486505405, 'w': 5.29162837839724, 'v': 6.39024066706535, 'l': 4.4443305180100365, 'm': 6.39024066706535, 'f': 6.39024066706535, 'y': 5.29162837839724, ',': 6.39024066706535, 'd': 5.003946305945459, '▁t': 4.4443305180100365, 'is': 4.7808027546312495, 'er': 4.7808027546312495, '▁a': 4.7808027546312495, '▁to': 5.003946305945459, 'to': 5.003946305945459, 'en': 5.003946305945459, '▁T': 5.29162837839724, '▁Th': 5.29162837839724, '▁Thi': 5.29162837839724, '▁This': 5.29162837839724, 'Th': 5.29162837839724, 'Thi': 5.29162837839724, 'This': 5.29162837839724, 'hi': 5.29162837839724, 'his': 5.29162837839724, 'th': 5.29162837839724, 'ou': 5.29162837839724, 'se': 5.29162837839724, '▁tok': 5.29162837839724, '▁toke': 5.29162837839724, '▁token': 5.29162837839724, 'tok': 5.29162837839724, 'toke': 5.29162837839724, 'token': 5.29162837839724, 'ok': 5.29162837839724, 'oke': 5.29162837839724, 'oken': 5.29162837839724, 'ke': 5.29162837839724, 'ken': 5.29162837839724, '▁s': 5.29162837839724, 'ra': 5.29162837839724, 'nd': 5.29162837839724, '▁i': 5.697093486505405, '▁is': 5.697093486505405, '▁th': 5.697093486505405, '▁the': 5.697093486505405, 'the': 5.697093486505405, 'he': 5.697093486505405, '▁H': 5.697093486505405, 'in': 5.697093486505405, '▁c': 5.697093486505405, 'rs': 5.697093486505405, 'te': 5.697093486505405, '▁ab': 5.697093486505405, 'ab': 5.697093486505405, '▁tokeni': 5.697093486505405, '▁tokeniz': 5.697093486505405, 'tokeni': 5.697093486505405, 'tokeniz': 5.697093486505405, 'okeni': 5.697093486505405, 'okeniz': 5.697093486505405, 'keni': 5.697093486505405, 'keniz': 5.697093486505405, 'eni': 5.697093486505405, 'eniz': 5.697093486505405, 'ni': 5.697093486505405, 'niz': 5.697093486505405, 'iz': 5.697093486505405, 'at': 5.697093486505405, 'ti': 5.697093486505405, 'tio': 5.697093486505405, 'tion': 5.697093486505405, 'io': 5.697093486505405, 'ion': 5.697093486505405, 'on': 5.697093486505405, '▁se': 5.697093486505405, 'ho': 5.697093486505405, 'how': 5.697093486505405, 'ow': 5.697093486505405, 'era': 5.697093486505405, 'al': 5.697093486505405, 's.': 5.697093486505405, 'll': 5.697093486505405, 'an': 5.697093486505405, 'and': 5.697093486505405, 'ne': 5.697093486505405, '▁Hu': 6.39024066706535, '▁Hug': 6.39024066706535, '▁Hugg': 6.39024066706535, '▁Huggi': 6.39024066706535, '▁Huggin': 6.39024066706535, '▁Hugging': 6.39024066706535, 'Hu': 6.39024066706535, 'Hug': 6.39024066706535, 'Hugg': 6.39024066706535, 'Huggi': 6.39024066706535, 'Huggin': 6.39024066706535, 'Hugging': 6.39024066706535, 'ug': 6.39024066706535, 'ugg': 6.39024066706535, 'uggi': 6.39024066706535, 'uggin': 6.39024066706535, 'ugging': 6.39024066706535, 'gg': 6.39024066706535, 'ggi': 6.39024066706535, 'ggin': 6.39024066706535, 'gging': 6.39024066706535, 'gi': 6.39024066706535, 'gin': 6.39024066706535, 'ging': 6.39024066706535, 'ing': 6.39024066706535, 'ng': 6.39024066706535, '▁F': 6.39024066706535, '▁Fa': 6.39024066706535, '▁Fac': 6.39024066706535, '▁Face': 6.39024066706535, 'Fa': 6.39024066706535, 'Fac': 6.39024066706535, 'Face': 6.39024066706535, 'ac': 6.39024066706535, 'ace': 6.39024066706535, 'ce': 6.39024066706535, '▁co': 6.39024066706535, '▁cou': 6.39024066706535, '▁cour': 6.39024066706535, '▁cours': 6.39024066706535, '▁course': 6.39024066706535, '▁course.': 6.39024066706535, 'co': 6.39024066706535, 'cou': 6.39024066706535, 'cour': 6.39024066706535, 'cours': 6.39024066706535, 'course': 6.39024066706535, 'course.': 6.39024066706535, 'our': 6.39024066706535, 'ours': 6.39024066706535, 'ourse': 6.39024066706535, 'ourse.': 6.39024066706535, 'ur': 6.39024066706535, 'urs': 6.39024066706535, 'urse': 6.39024066706535, 'urse.': 6.39024066706535, 'rse': 6.39024066706535, 'rse.': 6.39024066706535, 'se.': 6.39024066706535, 'e.': 6.39024066706535, '▁ch': 6.39024066706535, '▁cha': 6.39024066706535, '▁chap': 6.39024066706535, '▁chapt': 6.39024066706535, '▁chapte': 6.39024066706535, '▁chapter': 6.39024066706535, 'ch': 6.39024066706535, 'cha': 6.39024066706535, 'chap': 6.39024066706535, 'chapt': 6.39024066706535, 'chapte': 6.39024066706535, 'chapter': 6.39024066706535, 'ha': 6.39024066706535, 'hap': 6.39024066706535, 'hapt': 6.39024066706535, 'hapte': 6.39024066706535, 'hapter': 6.39024066706535, 'ap': 6.39024066706535, 'apt': 6.39024066706535, 'apte': 6.39024066706535, 'apter': 6.39024066706535, 'pt': 6.39024066706535, 'pte': 6.39024066706535, 'pter': 6.39024066706535, 'ter': 6.39024066706535, '▁abo': 6.39024066706535, '▁abou': 6.39024066706535, '▁about': 6.39024066706535, 'abo': 6.39024066706535, 'abou': 6.39024066706535, 'about': 6.39024066706535, 'bo': 6.39024066706535, 'bou': 6.39024066706535, 'bout': 6.39024066706535, 'out': 6.39024066706535, 'ut': 6.39024066706535, '▁tokeniza': 6.39024066706535, '▁tokenizat': 6.39024066706535, '▁tokenizati': 6.39024066706535, '▁tokenizatio': 6.39024066706535, '▁tokenization': 6.39024066706535, '▁tokenization.': 6.39024066706535, 'tokeniza': 6.39024066706535, 'tokenizat': 6.39024066706535, 'tokenizati': 6.39024066706535, 'tokenizatio': 6.39024066706535, 'tokenization': 6.39024066706535, 'tokenization.': 6.39024066706535, 'okeniza': 6.39024066706535, 'okenizat': 6.39024066706535, 'okenizati': 6.39024066706535, 'okenizatio': 6.39024066706535, 'okenization': 6.39024066706535, 'okenization.': 6.39024066706535, 'keniza': 6.39024066706535, 'kenizat': 6.39024066706535, 'kenizati': 6.39024066706535, 'kenizatio': 6.39024066706535, 'kenization': 6.39024066706535, 'kenization.': 6.39024066706535, 'eniza': 6.39024066706535, 'enizat': 6.39024066706535, 'enizati': 6.39024066706535, 'enizatio': 6.39024066706535, 'enization': 6.39024066706535, 'enization.': 6.39024066706535, 'niza': 6.39024066706535, 'nizat': 6.39024066706535, 'nizati': 6.39024066706535, 'nizatio': 6.39024066706535, 'nization': 6.39024066706535, 'nization.': 6.39024066706535, 'iza': 6.39024066706535, 'izat': 6.39024066706535, 'izati': 6.39024066706535, 'izatio': 6.39024066706535, 'ization': 6.39024066706535, 'ization.': 6.39024066706535, 'za': 6.39024066706535, 'zat': 6.39024066706535, 'zati': 6.39024066706535, 'zatio': 6.39024066706535, 'zation': 6.39024066706535, 'zation.': 6.39024066706535, 'ati': 6.39024066706535, 'atio': 6.39024066706535, 'ation': 6.39024066706535, 'ation.': 6.39024066706535, 'tion.': 6.39024066706535, 'ion.': 6.39024066706535, 'on.': 6.39024066706535, 'n.': 6.39024066706535, '▁sec': 6.39024066706535, '▁sect': 6.39024066706535, '▁secti': 6.39024066706535, '▁sectio': 6.39024066706535, '▁section': 6.39024066706535, 'sec': 6.39024066706535, 'sect': 6.39024066706535, 'secti': 6.39024066706535, 'sectio': 6.39024066706535, 'section': 6.39024066706535, 'ec': 6.39024066706535, 'ect': 6.39024066706535, 'ecti': 6.39024066706535, 'ectio': 6.39024066706535, 'ection': 6.39024066706535, 'ct': 6.39024066706535, 'cti': 6.39024066706535, 'ctio': 6.39024066706535, 'ction': 6.39024066706535, '▁sh': 6.39024066706535, '▁sho': 6.39024066706535, '▁show': 6.39024066706535, '▁shows': 6.39024066706535, 'sh': 6.39024066706535, 'sho': 6.39024066706535, 'show': 6.39024066706535, 'shows': 6.39024066706535, 'hows': 6.39024066706535, 'ows': 6.39024066706535, 'ws': 6.39024066706535, '▁sev': 6.39024066706535, '▁seve': 6.39024066706535, '▁sever': 6.39024066706535, '▁severa': 6.39024066706535, '▁several': 6.39024066706535, 'sev': 6.39024066706535, 'seve': 6.39024066706535, 'sever': 6.39024066706535, 'severa': 6.39024066706535, 'several': 6.39024066706535, 'ev': 6.39024066706535, 'eve': 6.39024066706535}\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bc99f5db-1d0c-4129-a112-299fe4625a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    for start_idx in range(len(word)):\n",
    "        # This should be properly filled by the previous steps of the loop\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                # If we have found a better segmentation ending at end_idx, we update\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # We did not find a tokenization of the word -> unknown\n",
    "        return [\"<unk>\"], None\n",
    "\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1c7f0480-c90e-4bd9-9cd8-bbac0e564e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.54264024176184)\n",
      "(['This'], 6.29162837839724)\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hopefully\", model))\n",
    "print(encode_word(\"This\", model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "08178123-8ff8-4432-a4f6-9027f05cb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7b36ed58-e4b1-4c25-b2c7-3d403b8b6657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413.362600202517"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "61baa2b3-62cb-4a4c-91a9-d64e52814107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # We always keep tokens of length 1\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5359552-5451-4e66-9101-2c593a0f6e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.383135099029346\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "scores = compute_scores(model)\n",
    "print(scores[\"ll\"])\n",
    "print(scores[\"his\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ef77f983-4827-49d8-96ee-cbf841a08258",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # Remove percent_to_remove tokens with the lowest scores.\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "703e4610-587c-439c-bc35-278cf683311a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁This', '▁is', '▁the', '▁Hugging', '▁Face', '▁course.']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "tokenize(\"This is the Hugging Face course.\", model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde01926-abce-4104-9644-44ceb824070f",
   "metadata": {},
   "source": [
    "### 블록 단위로 토크나이저 빌딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ec2a5b70-d83f-4e68-96d9-ab0e67e55032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb647c45-9c8e-4572-82db-7ba1dc32cf3c",
   "metadata": {},
   "source": [
    "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(dataset)):\n",
    "        f.write(dataset[i][\"text\"] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc10aaa-1c0d-41bd-bfec-52d3ab501e71",
   "metadata": {},
   "source": [
    "#### WordPiece 토크나이저를 처음부터 빌딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eab67de0-2886-4b9c-b083-88850d576fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "829465af-38c7-4a17-beb5-15f4a1e855e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x24fe350>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "36db92f2-b58a-4c69-83d8-c7b97ce7f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8517b37d-4137-4cd8-a815-267fa4b8b38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d17ac7fd-1d1e-44d6-958f-b2662e3d0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1dc4799e-9628-4ea9-b907-9edeb3d86bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f3a71032-1d94-4c5a-9580-7b0337af5627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2c36ce02-9c2b-4efd-b608-732ace7144b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.Tokenizer object at 0x24fe350>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ea55d9e1-91e0-45b8-b6a4-9ec659c23420",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5aeb1d3b-6011-4a85-9173-cb650e9f4117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5c723b05-a1e6-46b6-9639-130f31fb84f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "51936da2-0c82-4bb3-99b7-5b648eb74d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2ba53bbe-baf0-4e79-90bd-0354b955c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9120fda9-7aab-4fc7-934f-d5fa91b2d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b7cc99ee-56b8-4db3-908b-04cac0c14dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eaee7762-3042-4558-8552-12e6d7e70747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a950e8e3-b9ab-44d3-b7c9-1f0dd3879675",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "90cf5bf4-0815-4fb9-8522-00286336f614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2817,\n",
       " 11,\n",
       " 61,\n",
       " 3409,\n",
       " 1317,\n",
       " 24117,\n",
       " 18701,\n",
       " 6411,\n",
       " 2180,\n",
       " 3,\n",
       " 1167,\n",
       " 43,\n",
       " 3952,\n",
       " 1143,\n",
       " 9250,\n",
       " 18,\n",
       " 3]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fdb3bacd-3bbf-48f6-b625-c6edbd2c683f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let ' s test this tokenizer... on a pair of sentences.\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e17357d1-2c41-4317-af9f-58f60b1054ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b2e3ee58-f19b-4302-bd6b-b406e4fc8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "85d4d21e-34d4-4fbc-aa1b-64d541d3a58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x9cc1150>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fcd42a3e-554a-430e-88ce-60d285960649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bf3bef0c-258a-439d-8648-bda3c6316c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 2817, 11, 61, 3409, 2031, 1435, 17, 24117, 18701, 6411, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'let', \"'\", 's', 'test', 'my', 'pre', '-', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = wrapped_tokenizer(\"Let's test my pre-tokenizer.\")\n",
    "print(inputs)\n",
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4c194786-9697-405c-831d-f1ed2d27383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "wrapped_tokenizer2 = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "18facd18-6a7d-458a-9267-44e9a3e553af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 2817, 11, 61, 3409, 2031, 1435, 17, 24117, 18701, 6411, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'let', \"'\", 's', 'test', 'my', 'pre', '-', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs2 = wrapped_tokenizer2(\"Let's test my pre-tokenizer.\")\n",
    "print(inputs2)\n",
    "print(inputs2.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbdda7e-67c7-4c86-8dd4-0fdf29f62151",
   "metadata": {},
   "source": [
    "#### BPE 토크나이저를 처음부터 빌딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "34b9e80d-8d2a-4125-a075-5c1956b6b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "168dfb40-af77-41d3-be6b-016587a16652",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0783837e-ed94-415f-bc43-e8ac469e2de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ġtest', (5, 10)),\n",
       " ('Ġpre', (10, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28))]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bbe3f493-6555-46ab-8ca6-51418d900c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9d190cf5-16b3-4340-8154-03fd28f0c0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.model = models.BPE()\n",
    "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5d3a752b-5ed4-47f6-aa3a-880f7e84a606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'et', \"'\", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4167c5b4-bbd8-4ff5-9e2e-59cf5afd7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9470a326-e7b8-49da-bbcf-0906b9b6de67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Let's test this tokenizer.\"\n",
    "encoding = tokenizer.encode(sentence)\n",
    "start, end = encoding.offsets[4]\n",
    "sentence[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "80beaa84-072d-4c77-858d-93c9b8ec9d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1afa9cef-2eef-4c22-b947-ae83e436ad89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's test this tokenizer.\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "682df7b3-42d2-4de6-a49c-5267ad828818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9209eff6-b74f-4132-a1aa-7f616c5f2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf4a2a-f481-44fe-8b1a-e5fdc1494e00",
   "metadata": {},
   "source": [
    "#### Unigram 토크나이저를 처음부터 빌딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5619a3dd-9e72-46a2-b4a1-946e05382c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "abc1b4ed-d1ee-425b-a6a4-1eb2e0728206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Regex\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f10c949c-737e-4915-bb7f-3be83c2589aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d3e61816-69b2-41e2-84db-26a34de9ceeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"▁Let's\", (0, 5)),\n",
       " ('▁test', (5, 10)),\n",
       " ('▁the', (10, 14)),\n",
       " ('▁pre-tokenizer!', (14, 29))]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2080b41a-6ef7-4d7e-9069-7f342b262c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
    ")\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "97568929-4eb9-4085-9f4e-57a876b1bace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.model = models.Unigram()\n",
    "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "413d80ea-8575-4c18-a474-93e33f8699a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "449cc2c5-9462-4fe3-aeda-549651139755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
    "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "289f3d23-bb5f-41dd-9574-9d3b262a9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "db3267c1-1e93-4dbd-b03f-7ef2e8e0af03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair', '▁of', '▁sentence', 's', '!', '<sep>', '<cls>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4d3ba9dc-d2b0-4bb4-9b1f-cf727abef9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '<cls>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "044c2b4b-fb2b-47d4-af90-165db1570121",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b57c47f9-dbca-4711-848b-f061d55f282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "802752af-75af-484e-9e11-96e190de4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18861c4-9f2d-4eaf-a660-5180e7098c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
