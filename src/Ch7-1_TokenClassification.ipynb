{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7797064-f9ee-44b3-8674-0039b9ff22d8",
   "metadata": {},
   "source": [
    "### 1. 토큰 분류 (Token Classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a103d043-5082-4446-a8c6-ea88a8cc68c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# CoNLL-2003 URL이 변경되었음.\n",
    "#raw_datasets = load_dataset(\"conll2003\", revision=\"master\")\n",
    "raw_datasets = load_dataset(\"conll2003\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77cf398-e348-49c5-9567-bb0b5fc800d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c12a6ea-44c8-4615-9f1f-33b4472078e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "834db3e7-79c5-40d8-93f4-0a577490a11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d42f7f-d902-4c2e-85bf-3dd6a18be853",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f36d22a-6ddb-477a-8138-239e2441103c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
      "Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None)\n",
      "Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"].features[\"tokens\"])\n",
    "print(raw_datasets[\"train\"].features[\"pos_tags\"])\n",
    "print(raw_datasets[\"train\"].features[\"chunk_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1787720c-dc48-41c9-a518-07f92bd9ee67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf35e66c-15bf-4490-aa4f-85d21841a536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][0][\"tokens\"])\n",
    "print(raw_datasets[\"train\"][0][\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a93a819d-c845-470a-b96d-d3b4693ef031",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU    rejects German call to boycott British lamb . \n",
      "B-ORG O       B-MISC O    O  O       B-MISC  O    O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af8dabc0-87c3-48b1-8ebd-6a0e353c8f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '4', 'tokens': ['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.'], 'pos_tags': [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7], 'chunk_tags': [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0], 'ner_tags': [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdae0d0e-cbdb-4458-af53-6d52851adf80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer . \n",
      "B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][4][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][4][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeb27daa-6428-4cc0-b36c-44c771c14344",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU  rejects German call to boycott British lamb . \n",
      "NNP VBZ     JJ     NN   TO VB      JJ      NN   . \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"pos_tags\"]\n",
    "label_names = raw_datasets[\"train\"].features[\"pos_tags\"].feature.names\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd5b4003-0d6b-4f38-aa23-c1285d097b81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU   rejects German call to   boycott British lamb . \n",
      "B-NP B-VP    B-NP   I-NP B-VP I-VP    B-NP    I-NP O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"chunk_tags\"]\n",
    "label_names = raw_datasets[\"train\"].features[\"chunk_tags\"].feature.names\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "281769f3-4613-4b77-b216-16cd3e227508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7596bdde-2023-4512-83ee-b2237db147f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c44fa5b4-1ee8-47d0-b273-8ee63b78e366",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67a3a613-3067-44df-a9d8-086567964fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17e0c2b0-0d8d-4591-b4fa-754110530ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'EU', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7270, 102], [101, 22961, 102], [101, 1528, 102], [101, 1840, 102], [101, 1106, 102], [101, 21423, 102], [101, 1418, 102], [101, 2495, 12913, 102], [101, 119, 102]], 'token_type_ids': [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"])\n",
    "print(inputs.tokens())\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e62b97-92ab-42ba-b248-8c06cf559298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27b05814-3610-4acb-8d09-52354dd7610f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 146, 1821, 170, 2377, 119, 1192, 1132, 170, 2377, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"I am a student.\", \"You are a student.\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22ee3eed-d137-4ff8-8944-64f1ee3076e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 146, 1821, 170, 2377, 119, 1192, 1132, 170, 2377, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"I am a student.\", \"You are a student.\", is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43d3dd30-fcad-4c9a-9374-d62173b1f01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 146, 1821, 170, 2377, 119, 102, 1192, 1132, 170, 2377, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"I am a student.\", \"You are a student.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d409c48f-fba0-4740-9722-7f7470896739",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 146, 1821, 170, 2377, 119, 102], [101, 1192, 1132, 170, 2377, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"I am a student.\", \"You are a student.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c56fadf6-2c18-4927-beaf-119ce9a8bd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 146, 1821, 170, 2377, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"I am a student.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2abe8ff0-6070-4e08-acb6-e1542889ed7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 146, 1821, 170, 2377, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"I am a student.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33f76966-4979-44d5-b41d-6ac1037428da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 146, 1821, 170, 2377, 119, 102], [101, 1192, 1132, 170, 2377, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n",
      "['[CLS]', 'I', 'am', 'a', 'student', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, None]\n",
      "['[CLS]', 'You', 'are', 'a', 'student', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, None]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\"I am a student.\", \"You are a student.\"])\n",
    "print(inputs)\n",
    "print(inputs.tokens())\n",
    "print(inputs.word_ids())\n",
    "print(inputs.tokens(1))\n",
    "print(inputs.word_ids(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9713c91-46d5-405b-b56e-b8aa10770ac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "705aa592-ae22-4953-baf5-855285c09008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04cc32dc-e38a-4783-a4dd-fda337a56279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a400cb6-a367-4be3-bb29-b78b589a753c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122109bd-9516-4c57-8fde-077b2aeb043e",
   "metadata": {},
   "source": [
    "약간의 부가 작업으로 토큰과 일치되도록 레이블 목록을 확장할 수 있습니다. 적용할 첫번째 규칙은 특수 토큰의 레이블의 ID가 -100이라는 것입니다. 기본적으로 -100은 우리가 사용할 손실 함수(cross entropy)에서 무시되는 인덱스이기 때문입니다. 추가적으로, 위의 토큰 식별자 리스트에서 두번째 7에 해당하는 토큰의 레이블은 첫번째 7에 해당하는 토큰의 레이블과 동일한 값을 가지게 됩니다. 물론 첫번째 7에 해당하는 토큰은 B-로 시작하고 두번째는 I-로 시작하는 레이블로 지정됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2bfe980-a365-4a00-916a-62cae8cb270e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # 새로운 단어의 시작 토큰.\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # 특수 토큰.\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # 이전 토큰과 동일한 단어에 소속된 토큰.\n",
    "            label = labels[word_id]\n",
    "            # 만약 레이블이 B-XXX이면 이를 I-XXX로 변경.\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30178160-584b-42c7-9cc5-6fa159ac88fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def new_align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "            #current_word = word_id\n",
    "        elif current_word != word_id:\n",
    "            label = labels[word_id]\n",
    "            new_labels.append(label)\n",
    "            current_word = word_id\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2880e5a2-611a-471f-857e-305cf337623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]\n",
      "[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]\n",
      "[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "# ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', \n",
    "# 'B-MISC', 'I-MISC']\n",
    "inputs = tokenizer(raw_datasets[\"train\"][4][\"tokens\"], is_split_into_words=True)\n",
    "word_ids = inputs.word_ids()\n",
    "labels = raw_datasets[\"train\"][4][\"ner_tags\"]\n",
    "print(word_ids)\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))\n",
    "print(new_align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a8ad72-4ddc-4669-936e-db9ad36d1f64",
   "metadata": {},
   "source": [
    "보시다시피, 함수는 시작과 끝에 두 개의 특수 토큰에 대해 -100을 추가하고 **두 개의 토큰으로 분할된 토큰에 대해 0을 추가**했습니다.\n",
    "\n",
    "✏️ Your turn! 일부 연구자는 단어당 하나의 레이블만 지정하고 해당 단어의 다른 하위 토큰에 -100을 할당하는 것을 선호합니다. 이는 손실(loss)에 크게 기여하는 많은 하위 토큰으로 분할되는 긴 단어를 회피하기 위함입니다. 이 규칙에 따라 레이블을 입력 ID와 정렬할 수 있도록 이전 함수를 변경해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69bdc770-0f13-44c1-991f-84945f6ad424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']\n",
      "['[CLS]', 'Germany', \"'\", 's', 'representative', 'to', 'the', 'European', 'Union', \"'\", 's', 'veterinary', 'committee', 'Werner', 'Z', '##wing', '##mann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][4][\"tokens\"])\n",
    "inputs = tokenizer(raw_datasets[\"train\"][4][\"tokens\"], is_split_into_words=True)\n",
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3eb9f850-6ec2-4ac8-b0b7-2d6c9709f7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Germany', \"'\", 's', 'representative', 'to', 'the', 'European', 'Union', \"'\", 's', 'veterinary', 'committee', 'Werner', 'Z', '##wing', '##mann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']\n",
      "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]\n",
      "[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]\n",
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
      "[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]\n",
      "[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][4][\"tokens\"], is_split_into_words=True)\n",
    "print(inputs.tokens())\n",
    "word_ids = inputs.word_ids()\n",
    "print(word_ids)\n",
    "labels = raw_datasets[\"train\"][4][\"ner_tags\"]\n",
    "label_names = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(labels)\n",
    "print(label_names)\n",
    "print(align_labels_with_tokens(labels, word_ids))\n",
    "print(new_align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f786c83-56b7-4209-bc96-84cc1cb7cf44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82240e91-26c0-482b-8ceb-c0a1e10c7ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8aa00604-ab9e-43f7-8e8d-bbc32c250e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98767fb4-aad4-4135-9593-bc00e05d909d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Germany',\n",
       " \"'s\",\n",
       " 'representative',\n",
       " 'to',\n",
       " 'the',\n",
       " 'European',\n",
       " 'Union',\n",
       " \"'s\",\n",
       " 'veterinary',\n",
       " 'committee',\n",
       " 'Werner',\n",
       " 'Zwingmann',\n",
       " 'said',\n",
       " 'on',\n",
       " 'Wednesday',\n",
       " 'consumers',\n",
       " 'should',\n",
       " 'buy',\n",
       " 'sheepmeat',\n",
       " 'from',\n",
       " 'countries',\n",
       " 'other',\n",
       " 'than',\n",
       " 'Britain',\n",
       " 'until',\n",
       " 'the',\n",
       " 'scientific',\n",
       " 'advice',\n",
       " 'was',\n",
       " 'clearer',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][4][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f677fd99-6079-4490-a48d-1fc752be82ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][4][\"tokens\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e84e3d25-bc3e-49b1-891c-785bd0fab003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Germany', \"'\", 's', 'representative', 'to', 'the', 'European', 'Union', \"'\", 's', 'veterinary', 'committee', 'Werner', 'Z', '##wing', '##mann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']\n",
      "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]\n",
      "[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.tokens())\n",
    "print(inputs.word_ids())\n",
    "print(raw_datasets[\"train\"][4][\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1505cecd-454c-4ebb-8d4a-a4349c6249ee",
   "metadata": {},
   "source": [
    "전체 데이터셋을 전처리하기 위해, 모든 입력을 토큰화하고 모든 레이블에 align_labels_with_tokens()를 적용해야 합니다. \"빠른(fast)\" 토크나이저의 장점을 활용하려면 다량의 텍스트를 동시에 토큰화하는 것이 가장 좋습니다. 따라서 예제 리스트를 처리하는 함수를 작성하고 batched=True 옵션을 지정하여 Dataset.map() 메서드를 사용합니다. 이전 예제와 다른 점은 한번에 다중 텍스트가 포함된 배치(batch)를 처리해야 하므로, word_ids() 함수의 매개변수로 현재 배치(batch)에 대한 인덱스를 매개변수로 넘겨야 한다는 것입니다. 따라서 다음과 같이 새로운 함수를 작성합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3dd2c862-51e1-4ebb-8775-3a2be04432a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2]]\n",
      "[[1, 2], [2, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "l.append([1,2])\n",
    "print(l)\n",
    "l.append([2,3,4])\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd8632d2-dd1d-4bd1-8739-ad167571658f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# examples는 단일 텍스트(문장)가 아니라 다중 텍스트임.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    # print(tokenized_inputs[\"input_ids\"]) # 1000개씩 처리\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    #print(len(all_labels)) # 1000개 1000개 1000개 ... 1000개 41개 총 14041개\n",
    "    #[[3, 0, 7, 0, 0, 0, 7, 0, 0], [1, 2], [5, 0], [0, 3, 4, 0, 0, 0, 0, \n",
    "    #0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "    #0],...]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels): #1000개씩 처리\n",
    "        word_ids = tokenized_inputs.word_ids(i)    # 배치(batch) 인덱스 지정\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87393bae-1760-4697-89ae-24abf5b368c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff7eccd53bc4a5f9420c5fa3e22d236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3d48ef7-8da6-4d69-80d6-ee8771de7a18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cb6334b-7061-4c1d-9cbc-8448139679af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041\n",
      "{'input_ids': [[101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], [101, 1943, 14428, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100], [-100, 1, 2, -100]]}\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets[\"train\"][\"input_ids\"])) # or, print(len(tokenized_datasets[\"train\"])\n",
    "print(tokenized_datasets[\"train\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "482f8788-575e-4674-b7ec-cc6ae0fe948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041\n",
      "{'input_ids': [[101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], [101, 1943, 14428, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100], [-100, 1, 2, -100]]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# examples는 단일 텍스트(문장)가 아니라 다중 텍스트임.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    # print(tokenized_inputs[\"input_ids\"]) # 1000개씩 처리\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    #print(len(all_labels)) # 1000개 1000개 1000개 ... 1000개 41개 총 14041개\n",
    "    #[[3, 0, 7, 0, 0, 0, 7, 0, 0], [1, 2], [5, 0], [0, 3, 4, 0, 0, 0, 0, \n",
    "    #0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "    #0],...]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels): #1000개씩 처리\n",
    "        word_ids = tokenized_inputs.word_ids(i)    # 배치(batch) 인덱스 지정\n",
    "        new_labels.append(new_align_labels_with_tokens(labels, word_ids)) # the new_align_labels_with_tokens() function is called\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    #return tokenized_inputs\n",
    "    return {\n",
    "        \"input_ids\" : tokenized_inputs[\"input_ids\"],\n",
    "        \"token_type_ids\" : tokenized_inputs[\"token_type_ids\"],\n",
    "        \"attention_mask\" : tokenized_inputs[\"attention_mask\"],\n",
    "        \"labels\" : new_labels\n",
    "    }\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "print(len(tokenized_datasets[\"train\"][\"input_ids\"])) # or, print(len(tokenized_datasets[\"train\"])\n",
    "print(tokenized_datasets[\"train\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a629fb-fda1-4c7e-a7d2-eeba0d47916d",
   "metadata": {},
   "source": [
    "### Trainer API를 이용하여 모델 미세조정\n",
    "Trainer를 활용하는 실제 코드는 이전과 동일합니다. 유일한 변경 사항은 데이터가 배치(batch)로 조합되는 방식과 메트릭(metrics) 계산 기능입니다.\n",
    "\n",
    "#### 데이터 콜레이션 (Data collation)\n",
    "앞서서 살펴봤던, **DataCollatorWithPadding은 입력(input IDs, attention mask 및 token type IDs)에 대해서만 패딩(padding)을 수행**하기 때문에 여기서는 사용할 수 없습니다. 여기서는 레이블도 입력과 똑같은 방식으로 패딩(padding)이 되어야 동일한 크기를 유지하고 -100을 패딩값으로 사용하여 해당 예측이 손실 계산에서 무시되도록 할 수 있습니다. 또한 datasets을 tensor로 바꾸어 줌. \n",
    "\n",
    "이는 모두 DataCollatorForTokenClassification에 의해 수행됩니다. DataCollatorWithPadding과 마찬가지로 입력을 전처리하는데 사용되는 tokenizer를 사용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "558771d8-83c9-4331-8349-db0e9f858954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f428892f-8419-4d37-bc14-7e18fb7678ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]}\n",
      "{'input_ids': [[101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], [101, 1943, 14428, 102], [101, 26660, 13329, 12649, 15928, 1820, 118, 4775, 118, 1659, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100], [-100, 1, 2, -100], [-100, 5, 6, 6, 6, 0, 0, 0, 0, 0, -100]]}\n",
      "tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
      "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n",
      "{'input_ids': tensor([[  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495, 12913,\n",
      "           119,   102],\n",
      "        [  101,  1943, 14428,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
      "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][0])\n",
    "print(tokenized_datasets[\"train\"][:3])\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "print(batch[\"labels\"])\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fcf01fc-6e9d-4382-a5fd-4cede2fb224b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "        return tokenizer.pad(\n",
    "            examples,\n",
    "            padding=\"longest\",\n",
    "            max_length=max_length,\n",
    "            #pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    ")\n",
    "    \n",
    "def collate_fn_1(examples):\n",
    "        return tokenizer(\n",
    "            examples,\n",
    "            padding=\"longest\",\n",
    "            max_length=max_length,\n",
    "            #pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19fd5562-f114-48e5-8ec6-bead32e3fe40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 26660, 13329, 12649, 15928, 1820, 118, 4775, 118, 1659, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 5, 6, 6, 6, 0, 0, 0, 0, 0, -100]}\n",
      "{'input_ids': tensor([  101, 26660, 13329, 12649, 15928,  1820,   118,  4775,   118,  1659,\n",
      "          102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([-100,    5,    6,    6,    6,    0,    0,    0,    0,    0, -100])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][2])\n",
    "batch = collate_fn(tokenized_datasets[\"train\"][2])\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4754afb6-f029-45ce-8170-c1e8e1ba8ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '2', 'tokens': ['BRUSSELS', '1996-08-22'], 'pos_tags': [22, 11], 'chunk_tags': [11, 12], 'ner_tags': [5, 0]}\n",
      "{'input_ids': tensor([[  101, 26660, 13329, 12649, 15928,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][2])\n",
    "batch = collate_fn_1(raw_datasets[\"train\"][2][\"tokens\"][0])\n",
    "#batch = collate_fn_1(raw_datasets[\"train\"][:2])\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0007cff6-95a6-4e71-9f81-1b5676a76143",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seqeval in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from seqeval) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home01/qualis/.local/lib/python3.10/site-packages (from seqeval) (1.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home01/qualis/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# 일반적인 경우...\n",
    "#!pip install seqeval\n",
    "\n",
    "# 아나콘다 가상환경을 사용하는 경우...\n",
    "# https://stackoverflow.com/questions/59997065/pip-python-normal-site-packages-is-not-writeable/65290638\n",
    "#!python3 -m pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a7e6f16c-ae6c-466c-85ce-6e90a9a8bdd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from datasets import load_metric\n",
    "#metric = load_metric(\"seqeval\")\n",
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6bc29efc-cf60-4dda-bd5f-3e230e4c8f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONDA_SHLVL=1\n",
      "CONDA_PROMPT_MODIFIER=(transformer) \n",
      "CONDA_EXE=/scratch/qualis/miniconda3/bin/conda\n",
      "_CE_CONDA=\n",
      "CONDA_PREFIX=/scratch/qualis/miniconda3/envs/transformer\n",
      "CONDA_PYTHON_EXE=/scratch/qualis/miniconda3/bin/python\n",
      "CONDA_DEFAULT_ENV=transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#!env | grep CONDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e2a96b40-2c57-4e9e-82e5-6e4c30a09a32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "label_names = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label= raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "print(labels)\n",
    "labels = [label_names[i] for i in labels]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed7229a5-3284-41ea-88b2-0c496a46f546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ORG', 'O', '0', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 2},\n",
       " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.6666666666666666,\n",
       " 'overall_f1': 0.8,\n",
       " 'overall_accuracy': 0.8888888888888888}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"0\"\n",
    "print(predictions)\n",
    "print(labels)\n",
    "metric.compute(predictions=[predictions], references=[labels])\n",
    "#metric.compute(predictions=predictions, references=labels) #error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d979797-b36a-429c-bbc5-0f871df09033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    print(logits, labels)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # 무시된 인덱스(특수 토큰들)를 제거하고 레이블로 변환\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8117d6-fc98-49ea-86c3-dbabe00dfa77",
   "metadata": {},
   "source": [
    "#### 모델 정의하기\n",
    "토큰 분류 문제에 대해 작업 중이므로 AutoModelForTokenClassification 클래스를 사용합니다. 이 모델을 정의할 때 기억해야 할 주요 사항은 가지고 있는 레이블 수에 대한 정보를 전달하는 것입니다. 이를 수행하는 가장 쉬운 방법은 num_labels 인수를 사용하여 해당 값을 전달하는 것이지만, 이 섹션의 시작 부분에서처럼 작동하는 멋진 추론 위젯(inference widget)을 원한다면, 올바른 레이블 대응 정보(label correspondences)를 설정하는 것이 좋습니다.\n",
    "\n",
    "ID에서 레이블로 또는 그 반대로의 매핑을 포함하는 두 개의 딕셔너리, 즉 id2label 및 label2id를 활용하여 이 대응 정보를 설정할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f193d01e-79b5-4aeb-bdf0-ff6dd6202149",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(label_names[0]))\n",
    "print(label_names)\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "#id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e66ffea2-6acf-4f4a-bfd5-d8e9b1c3d76e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-PER': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-ORG': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-MISC': 7,\n",
       " 'I-MISC': 8}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {v:k for k, v in id2label.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a740fb71-f371-4cb1-97cd-02f71d495662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    #id2label=id2label,\n",
    "    #label2id=label2id,\n",
    ")\n",
    "print(model)\n",
    "print(model.config)\n",
    "print(model.config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "815d5386-58b2-4341-a1c5-992889433ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PER\",\n",
      "    \"2\": \"I-PER\",\n",
      "    \"3\": \"B-ORG\",\n",
      "    \"4\": \"I-ORG\",\n",
      "    \"5\": \"B-LOC\",\n",
      "    \"6\": \"I-LOC\",\n",
      "    \"7\": \"B-MISC\",\n",
      "    \"8\": \"I-MISC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 5,\n",
      "    \"B-MISC\": 7,\n",
      "    \"B-ORG\": 3,\n",
      "    \"B-PER\": 1,\n",
      "    \"I-LOC\": 6,\n",
      "    \"I-MISC\": 8,\n",
      "    \"I-ORG\": 4,\n",
      "    \"I-PER\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "print(model)\n",
    "print(model.config)\n",
    "print(model.config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b41ddfcd-cb3c-4729-b6a9-0957232e766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> BERT number of parameters: 107.726601M\n"
     ]
    }
   ],
   "source": [
    "print(f\">>> BERT number of parameters: {model.num_parameters() / 1_000_000}M\") # bert-base-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5206aa2e-d480-496b-b6b6-f22562a1416e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee68983195a44ecbcfb0060f456ff07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f1530c55-0caf-4558-994f-adcb33be6753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    #save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    #push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6d6b9a2e-69c6-444e-bf73-0f7324a79355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    #print(logits.shape, labels.shape) # (3250, 151, 9) (3250, 151) validation datasets size\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    #print(predictions.shape) # (3250, 151)\n",
    "\n",
    "    # 무시된 인덱스(특수 토큰들)를 제거하고 레이블로 변환\n",
    "    #print(predictions[:2])\n",
    "    #print(labels[:2])\n",
    "    #print([[label_names[l] for l in label if l != -100] for label in labels[:5]])\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    #print(true_labels[:2])\n",
    "    #[['O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', \n",
    "    #  'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', \n",
    "    #  'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', \n",
    "    #  'O', 'O', 'O']]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    #print(true_predictions[:2])\n",
    "    #[['O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', \n",
    "    #  'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'\n",
    "    #  'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', \n",
    "    #  'O', 'O', 'O']]\n",
    "    \n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a8fa34a8-43c7-4428-b2b4-fac879468cb4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 03:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.074400</td>\n",
       "      <td>0.060147</td>\n",
       "      <td>0.915380</td>\n",
       "      <td>0.937563</td>\n",
       "      <td>0.926339</td>\n",
       "      <td>0.983340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>0.935291</td>\n",
       "      <td>0.948670</td>\n",
       "      <td>0.941933</td>\n",
       "      <td>0.985607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.064511</td>\n",
       "      <td>0.935031</td>\n",
       "      <td>0.951868</td>\n",
       "      <td>0.943374</td>\n",
       "      <td>0.986136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.06575343449548418, metrics={'train_runtime': 188.9464, 'train_samples_per_second': 222.936, 'train_steps_per_second': 27.881, 'total_flos': 920580703084350.0, 'train_loss': 0.06575343449548418, 'epoch': 3.0})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2947a2-f3ae-4474-b126-b721a762e354",
   "metadata": {},
   "source": [
    "학습이 진행되는 동안 모델이 저장될 때마다(여기서는 모든 에포크마다) 백그라운드에서 허브에 업로드됩니다. 이렇게 하면 필요한 경우 다른 서버에서 학습을 재개할 수 있습니다.\n",
    "\n",
    "학습이 완료되면 push_to_hub() 메서드를 사용하여 모델의 최신 버전을 업로드했는지 확인합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c141ab26-0745-499f-a507-75c1b271d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4ebd8-d83b-41dc-aefc-66862acd0ef4",
   "metadata": {},
   "source": [
    "또한 Trainer는 모든 평가 결과가 포함된 모델 카드(model card)의 초안을 작성하여 업로드합니다. 이 단계에서 Model Hub의 추론 위젯을 사용하여 모델을 테스트하고 친구와 공유할 수 있습니다. 토큰 분류 작업에서 모델을 성공적으로 미세 조정했습니다. 축하합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6ebaff98-d709-4117-b840-085da744168a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf13b9f3b61345bd9a1c3a09306308e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493ee322ff6146c5a4d065a02ad4e9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e4c973197a4c28977c55b1d97a7863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1317' max='1317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1317/1317 01:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.068491</td>\n",
       "      <td>0.876543</td>\n",
       "      <td>0.920061</td>\n",
       "      <td>0.897775</td>\n",
       "      <td>0.980397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.187600</td>\n",
       "      <td>0.061013</td>\n",
       "      <td>0.912972</td>\n",
       "      <td>0.935712</td>\n",
       "      <td>0.924202</td>\n",
       "      <td>0.983664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.055713</td>\n",
       "      <td>0.923813</td>\n",
       "      <td>0.942780</td>\n",
       "      <td>0.933200</td>\n",
       "      <td>0.985047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/108 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3.0,\n",
      " 'eval_accuracy': 0.9712106648177621,\n",
      " 'eval_f1': 0.8926808142052836,\n",
      " 'eval_loss': 0.14742746949195862,\n",
      " 'eval_precision': 0.8738341529591318,\n",
      " 'eval_recall': 0.9123583569405099,\n",
      " 'eval_runtime': 3.4335,\n",
      " 'eval_samples_per_second': 1005.67,\n",
      " 'eval_steps_per_second': 31.454}\n",
      "CPU times: user 1min 27s, sys: 31.8 s, total: 1min 59s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# CoNLL-2003 URL이 변경되었음.\n",
    "#raw_datasets = load_dataset(\"conll2003\", revision=\"master\")\n",
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\"\"\"\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "        num_rows: 14041\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "        num_rows: 3250\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "        num_rows: 3453\n",
    "    })\n",
    "})\n",
    "\"\"\"\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # 새로운 단어의 시작 토큰.\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # 특수 토큰.\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # 이전 토큰과 동일한 단어에 소속된 토큰.\n",
    "            label = labels[word_id]\n",
    "            # 만약 레이블이 B-XXX이면 이를 I-XXX로 변경.\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "# examples는 단일 텍스트(문장)가 아니라 다중 텍스트임.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)    # 배치(batch) 인덱스 지정\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\"\"\"\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
    "        num_rows: 14041\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
    "        num_rows: 3250\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
    "        num_rows: 3453\n",
    "    })\n",
    "})\n",
    "\"\"\"\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "label_names = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "#print(label_names)\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "\"\"\"\n",
    "{'0': 'O', '1': 'B-PER', '2': 'I-PER', '3': 'B-ORG', '4': 'I-ORG', \n",
    "'5': 'B-LOC', '6': 'I-LOC', '7': 'B-MISC', '8': 'I-MISC'}\n",
    "\"\"\"\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {k: v for k, v in id2label.items()}\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    #print(logits.shape, labels.shape) # (3250, 151, 9) (3250, 151)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # 무시된 인덱스(특수 토큰들)를 제거하고 레이블로 변환\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    # print(len(true_predictions), len(true_labels)) (3250, 3250)\n",
    "    #print(true_predictions)\n",
    "    #print(labels)\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size = 32, # default: 8\n",
    "    per_device_eval_batch_size = 32, # default: 8\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#save\n",
    "#trainer.save_model()\n",
    "\n",
    "#eval\n",
    "metrics = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "pprint(metrics)\n",
    "\n",
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()\n",
    "#trainer.push_to_hub(commit_message=\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5356875-e4ef-4182-91e2-b8e99ed7041a",
   "metadata": {},
   "source": [
    "모든 프로세스가 이 지점에 도달할때까지 기다리도록 지시합니다(accelerator.wait_for_everyone()). 이는 저장하기 전에 모든 프로세스에서 동일한 모델이 있는지 확인하기 위한 것입니다. 그런 다음 우리가 정의한 기본 모델인 unwrapped_model을 가져옵니다. accelerator.prepare() 메서드는 모델을 분산 학습(distributed training)에서 작동하도록 변경하므로 모델에 더이상 save_pretrained() 메서드가 없습니다. accelerator.unwrap_model() 메서드는 모델을 다시 save_pretrained()가 존재하는 일반 모델 객체로 돌려놓습니다. 마지막으로 save_pretrained()를 호출하지만 그 메서드에 torch.save() 대신 accelerate.save()를 사용하도록 지시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2c4ed4f1-e85a-44c5-b8f3-3c92fba6726b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65682601531d41758b6241c5a99914e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2960eb5e194d96a75a1832102be255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ca333f43d3486a916ded148c95d9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qualis2006/bert-finetuned-ner-accelerate\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5258c778dc764f768d9edcf8b190028d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1317 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.9173678895994615, 'recall': 0.8743984600577478, 'f1': 0.8953679369250985, 'accuracy': 0.9801612998175075}\n",
      "[2024-03-22 19:20:07,880] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "epoch 1: {'precision': 0.9382362840794345, 'recall': 0.9158863150977493, 'f1': 0.9269265940643445, 'accuracy': 0.9843704008948019}\n",
      "epoch 2: {'precision': 0.9419387411645911, 'recall': 0.9198027937551356, 'f1': 0.9307391702003825, 'accuracy': 0.9850473891799612}\n",
      "CPU times: user 1min 21s, sys: 31.8 s, total: 1min 53s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification, get_scheduler\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from pprint import pprint\n",
    "\n",
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# CoNLL-2003 URL이 변경되었음.\n",
    "#raw_datasets = load_dataset(\"conll2003\", revision=\"master\")\n",
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\"\"\"\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "        num_rows: 14041\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "        num_rows: 3250\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "        num_rows: 3453\n",
    "    })\n",
    "})\n",
    "\"\"\"\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # 새로운 단어의 시작 토큰.\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # 특수 토큰.\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # 이전 토큰과 동일한 단어에 소속된 토큰.\n",
    "            label = labels[word_id]\n",
    "            # 만약 레이블이 B-XXX이면 이를 I-XXX로 변경.\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "# examples는 단일 텍스트(문장)가 아니라 다중 텍스트임.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)    # 배치(batch) 인덱스 지정\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\"\"\"\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
    "        num_rows: 14041\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
    "        num_rows: 3250\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
    "        num_rows: 3453\n",
    "    })\n",
    "})\n",
    "\"\"\"\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "label_names = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "#print(label_names)\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "\"\"\"\n",
    "{'0': 'O', '1': 'B-PER', '2': 'I-PER', '3': 'B-ORG', '4': 'I-ORG', \n",
    "'5': 'B-LOC', '6': 'I-LOC', '7': 'B-MISC', '8': 'I-MISC'}\n",
    "\"\"\"\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {k: v for k, v in id2label.items()}\n",
    "\n",
    "#from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=32,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "print(repo_name)\n",
    "\n",
    "output_dir = \"bert-finetuned-ner-accelerate\"\n",
    "#repo = Repository(output_dir, clone_from=repo_name) # git lfs 설치 에러러\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # 학습 (Training)\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # 평가 (Evaluation)\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        #print(labels.shape)\n",
    "\n",
    "        # 취합 대상인 예측(predictions)과 레이블(labels)을 패딩하기 위해서 필요함..\n",
    "        #predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=0)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        #print(f\"***{labels.shape}\")\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    #accelerator.print(f\"epoch {epoch}:\", results)\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "       },\n",
    "    )\n",
    "\n",
    "    # 저장 및 업로드\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        #repo.push_to_hub(\n",
    "        #    commit_message = f\"Training in progress epoch {epoch}\", blocking=False,\n",
    "        #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129ff987-4f51-4f0c-81fe-6c0df4a8649d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<timed exec>:191: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "<timed exec>:191: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "<timed exec>:191: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "<timed exec>:191: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/transformer/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d73ca1cb9404c9fa745f7b9dfb92d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f543313ee7fe4647b0fd6ccd683f0ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be240bfc672a448ca9ddb476dfa87b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e461303b3fc4c899f7a711db025c551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.8080327868852459, 'recall': 0.7341376228775692, 'f1': 0.7693148119244575, 'accuracy': 0.9576704299367655}\n",
      "epoch 1: {'precision': 0.899344262295082, 'recall': 0.8403799019607843, 'f1': 0.8688628444726005, 'accuracy': 0.9757745502844108}\n",
      "epoch 2: {'precision': 0.908360655737705, 'recall': 0.8648353363508663, 'f1': 0.8860638042696092, 'accuracy': 0.9785464730170647}\n",
      "CPU times: user 458 ms, sys: 213 ms, total: 672 ms\n",
      "Wall time: 57.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification, get_scheduler\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from pprint import pprint\n",
    "\n",
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# CoNLL-2003 URL이 변경되었음.\n",
    "#raw_datasets = load_dataset(\"conll2003\", revision=\"master\")\n",
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "\"\"\"\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "        num_rows: 14041\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "        num_rows: 3250\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "        num_rows: 3453\n",
    "    })\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "\n",
    "def get_dataloaders(accelerator: Accelerator, batch_size: int = 8):\n",
    "    # CoNLL-2003 URL이 변경되었음.\n",
    "    #raw_datasets = load_dataset(\"conll2003\", revision=\"master\")\n",
    "    #raw_datasets = load_dataset(\"conll2003\")\n",
    "    \"\"\"\n",
    "    DatasetDict({\n",
    "        train: Dataset({\n",
    "            features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "            num_rows: 14041\n",
    "        })\n",
    "        validation: Dataset({\n",
    "            features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "            num_rows: 3250\n",
    "        })\n",
    "        test: Dataset({\n",
    "            features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "            num_rows: 3453\n",
    "        })\n",
    "    })\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    def align_labels_with_tokens(labels, word_ids):\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id != current_word:\n",
    "                # 새로운 단어의 시작 토큰.\n",
    "                current_word = word_id\n",
    "                label = -100 if word_id is None else labels[word_id]\n",
    "                new_labels.append(label)\n",
    "            elif word_id is None:\n",
    "                # 특수 토큰.\n",
    "                new_labels.append(-100)\n",
    "            else:\n",
    "                # 이전 토큰과 동일한 단어에 소속된 토큰.\n",
    "                label = labels[word_id]\n",
    "                # 만약 레이블이 B-XXX이면 이를 I-XXX로 변경.\n",
    "                if label % 2 == 1:\n",
    "                    label += 1\n",
    "                new_labels.append(label)\n",
    "\n",
    "        return new_labels\n",
    "\n",
    "    # examples는 단일 텍스트(문장)가 아니라 다중 텍스트임.\n",
    "    def tokenize_and_align_labels(examples):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "        )\n",
    "        all_labels = examples[\"ner_tags\"]\n",
    "        new_labels = []\n",
    "        for i, labels in enumerate(all_labels):\n",
    "            word_ids = tokenized_inputs.word_ids(i)    # 배치(batch) 인덱스 지정\n",
    "            new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = new_labels\n",
    "        return tokenized_inputs\n",
    "    \n",
    "    # Apply the method we just defined to all the examples in all the splits of the dataset\n",
    "    # starting with the main process first:\n",
    "    with accelerator.main_process_first():\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=True,\n",
    "            remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        )\n",
    "    \"\"\"\n",
    "    DatasetDict({\n",
    "        train: Dataset({\n",
    "            features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
    "            num_rows: 14041\n",
    "        })\n",
    "        validation: Dataset({\n",
    "            features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
    "            num_rows: 3250\n",
    "        })\n",
    "        test: Dataset({\n",
    "            features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
    "            num_rows: 3453\n",
    "        })\n",
    "    })\n",
    "    \"\"\"\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation\"],\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return train_dataloader, eval_dataloader, tokenizer\n",
    "\n",
    "def training_function():\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    train_dataloader, eval_dataloader, tokenizer = get_dataloaders(accelerator, batch_size=32)\n",
    "    \n",
    "    label_names = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "    #print(label_names)\n",
    "    id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "    \"\"\"\n",
    "    {'0': 'O', '1': 'B-PER', '2': 'I-PER', '3': 'B-ORG', '4': 'I-ORG', \n",
    "    '5': 'B-LOC', '6': 'I-LOC', '7': 'B-MISC', '8': 'I-MISC'}\n",
    "    \"\"\"\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "    \n",
    "    id2label = {i: label for i, label in enumerate(label_names)}\n",
    "    label2id = {k: v for k, v in id2label.items()}\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    num_train_epochs = 3\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    #model_name = \"bert-finetuned-ner-accelerate\"\n",
    "    #repo_name = get_full_repo_name(model_name)\n",
    "    #print(repo_name)\n",
    "    output_dir = \"bert-finetuned-ner-accelerate\"\n",
    "    #repo = Repository(output_dir, clone_from=repo_name) # git lfs 설치 에러 발생\n",
    "\n",
    "    def postprocess(predictions, labels):\n",
    "        predictions = predictions.detach().cpu().clone().numpy()\n",
    "        labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "        # Remove ignored index (special tokens) and convert to labels\n",
    "        true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "        true_predictions = [\n",
    "            [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        return true_labels, true_predictions\n",
    "\n",
    "    metric = load_metric(\"seqeval\")\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "    # 학습 (Training)\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # 평가 (Evaluation)\n",
    "        model.eval()\n",
    "        for batch in eval_dataloader:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            labels = batch[\"labels\"]\n",
    "            #print(labels.shape) # batch size 32, number of processes 4\n",
    "            #torch.Size([32, 52]) torch.Size([32, 49]) torch.Size([32, 18]) torch.Size([32, 51])\n",
    "\n",
    "            # 취합 대상인 예측(predictions)과 레이블(labels)을 패딩하기 위해서 필요함..\n",
    "            predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "            #predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=0)\n",
    "            labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "            #print(f\"*{labels.shape}\")\n",
    "            #*torch.Size([32, 52]) *torch.Size([32, 52]) *torch.Size([32, 52]) *torch.Size([32, 52])\n",
    "            \n",
    "\n",
    "            predictions_gathered = accelerator.gather(predictions)\n",
    "            labels_gathered = accelerator.gather(labels)\n",
    "            #print(f\"**{labels_gathered.shape}\")\n",
    "            #**torch.Size([128, 52]) **torch.Size([128, 52]) **torch.Size([128, 52]) **torch.Size([128, 52])\n",
    "\n",
    "            true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "            #true_predictions, true_labels = postprocess(predictions, labels)\n",
    "            metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "        results = metric.compute()\n",
    "        # Use accelerator.print to print only on the main process.\n",
    "        #accelerator.print(f\"epoch {epoch}:\", results)\n",
    "        #print(\n",
    "        accelerator.print(\n",
    "            f\"epoch {epoch}:\",\n",
    "            {\n",
    "                key: results[f\"overall_{key}\"]\n",
    "                for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "           },\n",
    "        )\n",
    "\n",
    "        # 저장 및 업로드\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            #repo.push_to_hub(\n",
    "            #    commit_message = f\"Training in progress epoch {epoch}\", blocking=False,\n",
    "            #)\n",
    "\n",
    "#model_name = \"bert-finetuned-ner-accelerate\"\n",
    "#repo_name = get_full_repo_name(model_name)\n",
    "#print(repo_name)\n",
    "#repo = Repository(output_dir, clone_from=repo_name) # git lfs 설치 에러러\n",
    "\n",
    "#notebook_launcher(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=4)\n",
    "        \n",
    "#def main():\n",
    "#    parser = argparse.ArgumentParser(description=\"Simple example of training script.\")\n",
    "#    parser.add_argument(\n",
    "#        \"--mixed_precision\",\n",
    "#        type=str,\n",
    "#        default=None,\n",
    "#        choices=[\"no\", \"fp16\", \"bf16\", \"fp8\"],\n",
    "#        help=\"Whether to use mixed precision. Choose\"\n",
    "#        \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n",
    "#        \"and an Nvidia Ampere GPU.\",\n",
    "#    )\n",
    "#    parser.add_argument(\"--cpu\", action=\"store_true\", help=\"If passed, will train on the CPU.\")\n",
    "#    args = parser.parse_args()\n",
    "#    config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 16}\n",
    "#    training_function(config, args)\n",
    "#\n",
    "#\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a128a6f4-b3f5-43ec-8bac-f879a244fda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.9961349,\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.996436,\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9951277,\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99728584,\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.9441438,\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.98225856,\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9827559,\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9920243,\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 아래 내용을 본인의 checkpoint로 변경하시오.\n",
    "model_checkpoint = \"spasis/bert-finetuned-ner\"\n",
    "#token_classifier = pipeline(\n",
    "#    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    "#)\n",
    "\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint\n",
    ")\n",
    "\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e0170-3341-4eca-b786-6e13ca8e0b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
